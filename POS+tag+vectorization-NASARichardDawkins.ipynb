{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21749"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data\n",
    "df=pd.read_csv('C:\\Kate\\Python\\Authorship Attribution\\data\\AllTweets.csv')\n",
    "author1='NASA'\n",
    "author2='RichardDawkins'\n",
    "df_kk=df.loc[(df['author'] == author1)]\n",
    "df_hc=df.loc[(df['author'] == author2)]\n",
    "df=df_kk.append(df_hc,ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "#2000 random sample rows for KK\n",
    "rows = random.sample(df_kk.index, 2000)\n",
    "df_kk = df_kk.ix[rows]\n",
    "#2000 random sample rows for HC\n",
    "rows = random.sample(df_hc.index, 2000)\n",
    "df_hc = df_hc.ix[rows]\n",
    "#join back together\n",
    "df=df_kk.append(df_hc,ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3993"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data pre-processing\n",
    "df.drop(df[df.retweet==True].index, inplace=True)\n",
    "df['num_of_words'] = df[\"text\"].str.split().apply(len)\n",
    "df.drop(df[df.num_of_words<4].index, inplace=True)\n",
    "df[\"text\"].replace(r\"http\\S+\", \"URL\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"@\\S+\", \"REF\", regex=True ,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/.-](\\d{1,2})[/.-](\\d{2,4})+\", \"DATE\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"\\d+\", \"NUM\", regex=True,inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "pos_code_map={'CC':'A','CD':'B','DT':'C','EX':'D','FW':'E','IN':'F','JJ':'G','JJR':'H','JJS':'I','LS':'J','MD':'K','NN':'L','NNS':'M',\n",
    "'NNP':'N','NNPS':'O','PDT':'P','POS':'Q','PRP':'R','PRP$':'S','RB':'T','RBR':'U','RBS':'V','RP':'W','SYM':'X','TO':'Y','UH':'Z',\n",
    "'VB':'1','VBD':'2','VBG':'3','VBN':'4','VBP':'5','VBZ':'6','WDT':'7','WP':'8','WP$':'9','WRB':'@'}\n",
    "code_pos_map={v: k for k, v in pos_code_map.iteritems()}\n",
    "#Python 3 inv_map = {v: k for k, v in my_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert(tag):\n",
    "    try:\n",
    "        code=pos_code_map[tag]\n",
    "    except:\n",
    "        code='?'\n",
    "    return code\n",
    "def inv_convert(code):\n",
    "    try:\n",
    "        tag=code_pos_map[code]\n",
    "    except:\n",
    "        tag='?'\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "def pos_tags(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed=tokenizer.tokenize(text)\n",
    "    return \"\".join(convert(tag) for (word, tag) in nltk.pos_tag(text_processed))\n",
    "def text_pos_inv_convert(text):\n",
    "    return \"-\".join(inv_convert(c.upper()) for c in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['text_pos']=df.apply(lambda x: pos_tags(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>text_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NASA</td>\n",
       "      <td>What's up with Earth's changing ice sheets? Ch...</td>\n",
       "      <td>82WFN53LM1WCGGLNNNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NASA</td>\n",
       "      <td>The #stsNUM crew will check out the flight &amp; r...</td>\n",
       "      <td>CLLK1WCLLLMA1FLYNFNNNNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NASA</td>\n",
       "      <td>RS-NUM ENGINE TEST UNDERWAY NOW: Watch live on...</td>\n",
       "      <td>NNNNNNN5FNLN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NASA</td>\n",
       "      <td>A massive cluster of galaxies located about NU...</td>\n",
       "      <td>CGLFM4FNNBGMTNN6MF3N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NASA</td>\n",
       "      <td>I'm getting real-time search results at TweetG...</td>\n",
       "      <td>R53GLLMFNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author                                               text  \\\n",
       "0   NASA  What's up with Earth's changing ice sheets? Ch...   \n",
       "1   NASA  The #stsNUM crew will check out the flight & r...   \n",
       "2   NASA  RS-NUM ENGINE TEST UNDERWAY NOW: Watch live on...   \n",
       "3   NASA  A massive cluster of galaxies located about NU...   \n",
       "4   NASA  I'm getting real-time search results at TweetG...   \n",
       "\n",
       "                  text_pos  \n",
       "0      82WFN53LM1WCGGLNNNN  \n",
       "1  CLLK1WCLLLMA1FLYNFNNNNN  \n",
       "2             NNNNNNN5FNLN  \n",
       "3     CGLFM4FNNBGMTNN6MF3N  \n",
       "4               R53GLLMFNN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix[:,['author','text','text_pos']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "for a in df.author.unique():\n",
    "    v = CountVectorizer(analyzer='char',ngram_range=(7, 7))\n",
    "    ngrams = v.fit_transform(df[df['author'] == a]['text_pos'])\n",
    "    df_t=pd.DataFrame(\n",
    "    {'Feature': v.get_feature_names(),\n",
    "     'Count': list(ngrams.sum(axis=0).flat),\n",
    "     'Author': a\n",
    "    })\n",
    "    #\n",
    "    df_features=df_features.append(df_t,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features['Feature_POS']=df_features.apply(lambda x: text_pos_inv_convert(x['Feature']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Count</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40379</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>33</td>\n",
       "      <td>nnnnnng</td>\n",
       "      <td>NNP-NNP-NNP-NNP-NNP-NNP-JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34022</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>18</td>\n",
       "      <td>lfnnnng</td>\n",
       "      <td>NN-IN-NNP-NNP-NNP-NNP-JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39651</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>14</td>\n",
       "      <td>nnfnngl</td>\n",
       "      <td>NNP-NNP-IN-NNP-NNP-JJ-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38311</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>10</td>\n",
       "      <td>nglgnll</td>\n",
       "      <td>NNP-JJ-NN-JJ-NNP-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39718</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>9</td>\n",
       "      <td>nnglgnl</td>\n",
       "      <td>NNP-NNP-JJ-NN-JJ-NNP-NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Author  Count  Feature                 Feature_POS\n",
       "40379  RichardDawkins     33  nnnnnng  NNP-NNP-NNP-NNP-NNP-NNP-JJ\n",
       "34022  RichardDawkins     18  lfnnnng    NN-IN-NNP-NNP-NNP-NNP-JJ\n",
       "39651  RichardDawkins     14  nnfnngl    NNP-NNP-IN-NNP-NNP-JJ-NN\n",
       "38311  RichardDawkins     10  nglgnll      NNP-JJ-NN-JJ-NNP-NN-NN\n",
       "39718  RichardDawkins      9  nnglgnl     NNP-NNP-JJ-NN-JJ-NNP-NN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[~df_features.Feature.isin(df_features[df_features['Author'] != author2].Feature)].sort_values('Count', ascending=False).ix[:,['Author','Count','Feature','Feature_POS']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Count</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>NASA</td>\n",
       "      <td>17</td>\n",
       "      <td>lfcnnnf</td>\n",
       "      <td>NN-IN-DT-NNP-NNP-NNP-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5391</th>\n",
       "      <td>NASA</td>\n",
       "      <td>9</td>\n",
       "      <td>cnnnnnl</td>\n",
       "      <td>DT-NNP-NNP-NNP-NNP-NNP-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2734</th>\n",
       "      <td>NASA</td>\n",
       "      <td>9</td>\n",
       "      <td>64y1fnn</td>\n",
       "      <td>VBZ-VBN-TO-VB-IN-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6986</th>\n",
       "      <td>NASA</td>\n",
       "      <td>9</td>\n",
       "      <td>fnlalll</td>\n",
       "      <td>IN-NNP-NN-CC-NN-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>NASA</td>\n",
       "      <td>7</td>\n",
       "      <td>4y1fnnn</td>\n",
       "      <td>VBN-TO-VB-IN-NNP-NNP-NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Author  Count  Feature                Feature_POS\n",
       "10199   NASA     17  lfcnnnf    NN-IN-DT-NNP-NNP-NNP-IN\n",
       "5391    NASA      9  cnnnnnl  DT-NNP-NNP-NNP-NNP-NNP-NN\n",
       "2734    NASA      9  64y1fnn   VBZ-VBN-TO-VB-IN-NNP-NNP\n",
       "6986    NASA      9  fnlalll      IN-NNP-NN-CC-NN-NN-NN\n",
       "2048    NASA      7  4y1fnnn   VBN-TO-VB-IN-NNP-NNP-NNP"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[~df_features.Feature.isin(df_features[df_features['Author'] != author1].Feature)].sort_values('Count', ascending=False).ix[:,['Author','Count','Feature','Feature_POS']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "twt_train, twt_test, author_train, author_test = train_test_split(df.ix[:,['text','text_pos']], df['author'], test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def text_process(text):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Tokenizes and removes punctuation\n",
    "    3. Stems\n",
    "    4. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenizing\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed=tokenizer.tokenize(text)\n",
    "    \n",
    "    \n",
    "    # steming\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    \n",
    "    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
    "    \n",
    "\n",
    "    return text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ScoreSummaryByModelParams = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextAndTextCodedExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the text & text_pos from a tweet in a single pass.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, tweets):\n",
    "        features=tweets.ix[:,['text_pos','text']].to_records(index=False)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ModelParamsEvaluation (f_union,model,params,comment):\n",
    "    pipeline = Pipeline([\n",
    "    # Extract the text & text_coded\n",
    "    ('textandtextcoded', TextAndTextCodedExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from text and text_coded\n",
    "    ('union', f_union, ),\n",
    "\n",
    "    # Use a  classifier on the combined features\n",
    "    ('clf', model),\n",
    "    ])\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=params, verbose=1, cv=10)\n",
    "    grid_search.fit(twt_train, author_train)\n",
    "    #best score\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        ScoreSummaryByModelParams.append([comment,grid_search.best_score_,\"\\t%s: %r\" % (param_name, best_parameters[param_name])])    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "              # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char')),\n",
    "            ])),               \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    9.5s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   38.6s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:  1.4min\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  2.6min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  4.1min\n",
      "[Parallel(n_jobs=1)]: Done 1440 out of 1440 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.959\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tunion__char__tfidf__max_df: 0.5\n",
      "\tunion__char__tfidf__max_features: None\n",
      "\tunion__char__tfidf__ngram_range: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "p = {\n",
    "    'union__char__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__char__tfidf__ngram_range': ((2, 2), (3, 3)), \n",
    "    'union__char__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "            ('selector', ItemSelector(key='text')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='word')),\n",
    "            ])),              \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 720 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    7.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   30.4s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:  1.1min\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  2.0min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  3.2min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  4.6min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:  6.3min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed:  8.2min\n",
      "[Parallel(n_jobs=1)]: Done 4049 tasks       | elapsed: 10.4min\n",
      "[Parallel(n_jobs=1)]: Done 4999 tasks       | elapsed: 12.8min\n",
      "[Parallel(n_jobs=1)]: Done 6049 tasks       | elapsed: 15.5min\n",
      "[Parallel(n_jobs=1)]: Done 7199 tasks       | elapsed: 18.5min\n",
      "[Parallel(n_jobs=1)]: Done 7200 out of 7200 | elapsed: 18.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.965\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tunion__word__tfidf__max_df: 0.75\n",
      "\tunion__word__tfidf__max_features: None\n",
      "\tunion__word__tfidf__ngram_range: (1, 1)\n",
      "\tunion__word__tfidf__stop_words: None\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__word__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__word__tfidf__ngram_range': ((1, 1),(2, 2), (3, 3),(4,4),(5,5)), \n",
    "    'union__word__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'union__word__tfidf__stop_words': (None, 'english'),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('text', Pipeline([\n",
    "            ('selector', ItemSelector(key='text')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process)),\n",
    "            ])),              \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 360 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   25.8s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:  1.7min\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:  3.9min\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  7.0min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed: 10.9min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed: 15.8min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed: 21.4min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed: 28.0min\n",
      "[Parallel(n_jobs=1)]: Done 3600 out of 3600 | elapsed: 31.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.969\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tunion__text__tfidf__max_df: 0.75\n",
      "\tunion__text__tfidf__max_features: 5000\n",
      "\tunion__text__tfidf__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__text__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__text__tfidf__ngram_range': ((1, 1),(2, 2), (3, 3),(4,4),(5,5)), \n",
    "    'union__text__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, stemmed words, no stop words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling pos tag features  from the text_pos\n",
    "            ('text_pos', Pipeline([\n",
    "            ('selector', ItemSelector(key='text_pos')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='char')),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 360 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    5.4s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   21.1s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:   47.4s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  1.4min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  2.2min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  3.2min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:  4.3min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed:  5.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.731\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1\n",
      "\tunion__text_pos__tfidf__max_df: 0.5\n",
      "\tunion__text_pos__tfidf__max_features: None\n",
      "\tunion__text_pos__tfidf__ngram_range: (3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 3600 out of 3600 | elapsed:  6.4min finished\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__text_pos__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__text_pos__tfidf__ngram_range': ((3, 3), (4, 4),(5,5),(6,6),(7,7)), \n",
    "    'union__text_pos__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>BestScore</th>\n",
       "      <th>BestParameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.969102</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.969102</td>\n",
       "      <td>\\tunion__text__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.969102</td>\n",
       "      <td>\\tunion__text__tfidf__max_features: 5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.969102</td>\n",
       "      <td>\\tunion__text__tfidf__max_df: 0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.965344</td>\n",
       "      <td>\\tunion__word__tfidf__stop_words: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.965344</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.965344</td>\n",
       "      <td>\\tunion__word__tfidf__max_df: 0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.965344</td>\n",
       "      <td>\\tunion__word__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.965344</td>\n",
       "      <td>\\tunion__word__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.958664</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.958664</td>\n",
       "      <td>\\tunion__char__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.958664</td>\n",
       "      <td>\\tunion__char__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.958664</td>\n",
       "      <td>\\tunion__char__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.730689</td>\n",
       "      <td>\\tclf__alpha: 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.730689</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.730689</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.730689</td>\n",
       "      <td>\\tunion__text_pos__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Method  BestScore  \\\n",
       "9   Bernoulli Naive Bayes, stemmed words, no stop ...   0.969102   \n",
       "12  Bernoulli Naive Bayes, stemmed words, no stop ...   0.969102   \n",
       "11  Bernoulli Naive Bayes, stemmed words, no stop ...   0.969102   \n",
       "10  Bernoulli Naive Bayes, stemmed words, no stop ...   0.969102   \n",
       "8                         Bernoulli Naive Bayes, word   0.965344   \n",
       "4                         Bernoulli Naive Bayes, word   0.965344   \n",
       "5                         Bernoulli Naive Bayes, word   0.965344   \n",
       "6                         Bernoulli Naive Bayes, word   0.965344   \n",
       "7                         Bernoulli Naive Bayes, word   0.965344   \n",
       "0                         Bernoulli Naive Bayes, char   0.958664   \n",
       "1                         Bernoulli Naive Bayes, char   0.958664   \n",
       "3                         Bernoulli Naive Bayes, char   0.958664   \n",
       "2                         Bernoulli Naive Bayes, char   0.958664   \n",
       "13                    Bernoulli Naive Bayes, POS tags   0.730689   \n",
       "14                    Bernoulli Naive Bayes, POS tags   0.730689   \n",
       "15                    Bernoulli Naive Bayes, POS tags   0.730689   \n",
       "16                    Bernoulli Naive Bayes, POS tags   0.730689   \n",
       "\n",
       "                                    BestParameter  \n",
       "9                               \\tclf__alpha: 0.1  \n",
       "12      \\tunion__text__tfidf__ngram_range: (1, 1)  \n",
       "11       \\tunion__text__tfidf__max_features: 5000  \n",
       "10             \\tunion__text__tfidf__max_df: 0.75  \n",
       "8          \\tunion__word__tfidf__stop_words: None  \n",
       "4                               \\tclf__alpha: 0.1  \n",
       "5              \\tunion__word__tfidf__max_df: 0.75  \n",
       "6        \\tunion__word__tfidf__max_features: None  \n",
       "7       \\tunion__word__tfidf__ngram_range: (1, 1)  \n",
       "0                               \\tclf__alpha: 0.1  \n",
       "1               \\tunion__char__tfidf__max_df: 0.5  \n",
       "3       \\tunion__char__tfidf__ngram_range: (3, 3)  \n",
       "2        \\tunion__char__tfidf__max_features: None  \n",
       "13                                \\tclf__alpha: 1  \n",
       "14          \\tunion__text_pos__tfidf__max_df: 0.5  \n",
       "15   \\tunion__text_pos__tfidf__max_features: None  \n",
       "16  \\tunion__text_pos__tfidf__ngram_range: (3, 3)  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\n",
    "df_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByModelParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   15.4s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   18.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.969\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f2_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling stememd word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   32.2s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   39.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.971\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.01\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f2_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),    \n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   36.0s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   44.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.970\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.01\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   34.5s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   42.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.966\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.01\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + stemmed word + POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   17.7s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   21.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.966\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f4_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "             # Pipeline for pulling word features after word_processing from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   38.7s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   47.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.970\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.01\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f4_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + stemmed word + POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>BestScore</th>\n",
       "      <th>BestParameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bernoulli Naive Bayes, char + stemmed word</td>\n",
       "      <td>0.971190</td>\n",
       "      <td>\\tclf__alpha: 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + stemmed w...</td>\n",
       "      <td>0.970355</td>\n",
       "      <td>\\tclf__alpha: 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + stemmed word</td>\n",
       "      <td>0.970355</td>\n",
       "      <td>\\tclf__alpha: 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.969102</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word</td>\n",
       "      <td>0.969102</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.969102</td>\n",
       "      <td>\\tunion__text__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.969102</td>\n",
       "      <td>\\tunion__text__tfidf__max_df: 0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.969102</td>\n",
       "      <td>\\tunion__text__tfidf__max_features: 5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bernoulli Naive Bayes, char + stemmed word + P...</td>\n",
       "      <td>0.965762</td>\n",
       "      <td>\\tclf__alpha: 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + POS tags</td>\n",
       "      <td>0.965762</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.965344</td>\n",
       "      <td>\\tunion__word__tfidf__stop_words: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.965344</td>\n",
       "      <td>\\tunion__word__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.965344</td>\n",
       "      <td>\\tunion__word__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.965344</td>\n",
       "      <td>\\tunion__word__tfidf__max_df: 0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.965344</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.958664</td>\n",
       "      <td>\\tunion__char__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.958664</td>\n",
       "      <td>\\tunion__char__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.958664</td>\n",
       "      <td>\\tunion__char__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.958664</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.730689</td>\n",
       "      <td>\\tclf__alpha: 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.730689</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.730689</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.730689</td>\n",
       "      <td>\\tunion__text_pos__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Method  BestScore  \\\n",
       "18         Bernoulli Naive Bayes, char + stemmed word   0.971190   \n",
       "22  Bernoulli Naive Bayes, char + word + stemmed w...   0.970355   \n",
       "19  Bernoulli Naive Bayes, char + word + stemmed word   0.970355   \n",
       "9   Bernoulli Naive Bayes, stemmed words, no stop ...   0.969102   \n",
       "17                 Bernoulli Naive Bayes, char + word   0.969102   \n",
       "12  Bernoulli Naive Bayes, stemmed words, no stop ...   0.969102   \n",
       "10  Bernoulli Naive Bayes, stemmed words, no stop ...   0.969102   \n",
       "11  Bernoulli Naive Bayes, stemmed words, no stop ...   0.969102   \n",
       "20  Bernoulli Naive Bayes, char + stemmed word + P...   0.965762   \n",
       "21      Bernoulli Naive Bayes, char + word + POS tags   0.965762   \n",
       "8                         Bernoulli Naive Bayes, word   0.965344   \n",
       "7                         Bernoulli Naive Bayes, word   0.965344   \n",
       "6                         Bernoulli Naive Bayes, word   0.965344   \n",
       "5                         Bernoulli Naive Bayes, word   0.965344   \n",
       "4                         Bernoulli Naive Bayes, word   0.965344   \n",
       "1                         Bernoulli Naive Bayes, char   0.958664   \n",
       "3                         Bernoulli Naive Bayes, char   0.958664   \n",
       "2                         Bernoulli Naive Bayes, char   0.958664   \n",
       "0                         Bernoulli Naive Bayes, char   0.958664   \n",
       "13                    Bernoulli Naive Bayes, POS tags   0.730689   \n",
       "14                    Bernoulli Naive Bayes, POS tags   0.730689   \n",
       "15                    Bernoulli Naive Bayes, POS tags   0.730689   \n",
       "16                    Bernoulli Naive Bayes, POS tags   0.730689   \n",
       "\n",
       "                                    BestParameter  \n",
       "18                             \\tclf__alpha: 0.01  \n",
       "22                             \\tclf__alpha: 0.01  \n",
       "19                             \\tclf__alpha: 0.01  \n",
       "9                               \\tclf__alpha: 0.1  \n",
       "17                            \\tclf__alpha: 0.001  \n",
       "12      \\tunion__text__tfidf__ngram_range: (1, 1)  \n",
       "10             \\tunion__text__tfidf__max_df: 0.75  \n",
       "11       \\tunion__text__tfidf__max_features: 5000  \n",
       "20                             \\tclf__alpha: 0.01  \n",
       "21                            \\tclf__alpha: 0.001  \n",
       "8          \\tunion__word__tfidf__stop_words: None  \n",
       "7       \\tunion__word__tfidf__ngram_range: (1, 1)  \n",
       "6        \\tunion__word__tfidf__max_features: None  \n",
       "5              \\tunion__word__tfidf__max_df: 0.75  \n",
       "4                               \\tclf__alpha: 0.1  \n",
       "1               \\tunion__char__tfidf__max_df: 0.5  \n",
       "3       \\tunion__char__tfidf__ngram_range: (3, 3)  \n",
       "2        \\tunion__char__tfidf__max_features: None  \n",
       "0                               \\tclf__alpha: 0.1  \n",
       "13                                \\tclf__alpha: 1  \n",
       "14          \\tunion__text_pos__tfidf__max_df: 0.5  \n",
       "15   \\tunion__text_pos__tfidf__max_features: None  \n",
       "16  \\tunion__text_pos__tfidf__ngram_range: (3, 3)  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\n",
    "df_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByModelParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc,precision_score, accuracy_score, recall_score, f1_score\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ScoreSummaryByVector = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PredictionEvaluation(author_test_b,author_predictions_b,comment):\n",
    "    Precision=precision_score(author_test_b,author_predictions_b)\n",
    "    print ('Precision: %0.3f' % (Precision))\n",
    "    Accuracy=accuracy_score(author_test_b,author_predictions_b)\n",
    "    print ('Accuracy: %0.3f' % (Accuracy))\n",
    "    Recall=recall_score(author_test_b,author_predictions_b)\n",
    "    print ('Recall: %0.3f' % (Recall))\n",
    "    F1=f1_score(author_test_b,author_predictions_b)\n",
    "    print ('F1: %0.3f' % (F1))\n",
    "    print ('Confussion matrix:')\n",
    "    print (confusion_matrix(author_test_b,author_predictions_b))\n",
    "    ROC_AUC=roc_auc_score(author_test_b,author_predictions_b)\n",
    "    print ('ROC-AUC: %0.3f' % (ROC_AUC))\n",
    "    ScoreSummaryByVector.append([Precision,Accuracy,Recall,F1,ROC_AUC,comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ModelRun (f_union,model):\n",
    "    pipeline = Pipeline([\n",
    "    # Extract the text & text_coded\n",
    "    ('textandtextcoded', TextAndTextCodedExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from text and text_coded\n",
    "    ('union', f_union, ),\n",
    "\n",
    "    # Use a  classifier on the combined features\n",
    "    ('clf', model),\n",
    "    ])\n",
    "    pipeline.fit(twt_train, author_train)\n",
    "    author_predicted = pipeline.predict(twt_test)\n",
    "    \n",
    "    feature_names=list()\n",
    "    for p in (pipeline.get_params()['union'].transformer_list):\n",
    "        fn=(p[0],pipeline.get_params()['union'].get_params()[p[0]].get_params()['tfidf'].get_feature_names())\n",
    "        feature_names.append(fn)\n",
    "    df_fn=pd.DataFrame()\n",
    "    for fn in feature_names:\n",
    "        df_fn= df_fn.append(pd.DataFrame(\n",
    "        {'FeatureType': fn[0],\n",
    "         'Feature': fn[1]\n",
    "        }),\n",
    "        ignore_index=True)    \n",
    "    \n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    lb = LabelBinarizer()\n",
    "    author_test_b = lb.fit_transform(author_test.values)\n",
    "    author_predicted_b  = lb.fit_transform(author_predicted)\n",
    "    return (df_fn,pipeline.get_params()['clf'],author_predicted,author_predicted_b, author_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_informative_feature_for_binary_classification(feature_names, classifier):\n",
    "    class_labels = classifier.classes_\n",
    "\n",
    "    topnvalues_class0 = sorted(zip(classifier.coef_[0], feature_names['Feature'].values, feature_names['FeatureType'].values))\n",
    "    topnvalues_class1 = sorted(zip(classifier.coef_[0], feature_names['Feature'].values, feature_names['FeatureType'].values), reverse=True)\n",
    "\n",
    "    topn_df_class0=pd.DataFrame(topnvalues_class0, columns=['Coef','Feature','FeatureType'])\n",
    "    topn_df_class0['Author']=class_labels[0]\n",
    "    \n",
    "    topn_df_class1=pd.DataFrame(topnvalues_class1, columns=['Coef','Feature','FeatureType'])\n",
    "    topn_df_class1['Author']=class_labels[1]    \n",
    "    \n",
    "    topn_df=topn_df_class0.append(topn_df_class1)\n",
    "    \n",
    "        \n",
    "    return topn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "            # Pipeline for pulling stememd word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.75,max_features=5000)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f2_union,BernoulliNB(alpha=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.981\n",
      "Accuracy: 0.979\n",
      "Recall: 0.976\n",
      "F1: 0.978\n",
      "Confussion matrix:\n",
      "[[800  15]\n",
      " [ 19 764]]\n",
      "ROC-AUC: 0.979\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.75,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f3_union,BernoulliNB(alpha=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.979\n",
      "Accuracy: 0.974\n",
      "Recall: 0.968\n",
      "F1: 0.974\n",
      "Confussion matrix:\n",
      "[[794  16]\n",
      " [ 25 763]]\n",
      "ROC-AUC: 0.974\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+stemmed word+POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f4_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.75,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "             # Pipeline for pulling word features after word_processing from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f4_union,BernoulliNB(alpha=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.982\n",
      "Accuracy: 0.973\n",
      "Recall: 0.963\n",
      "F1: 0.973\n",
      "Confussion matrix:\n",
      "[[790  14]\n",
      " [ 29 765]]\n",
      "ROC-AUC: 0.973\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+word+stemmed word+POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.980745</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.975734</td>\n",
       "      <td>0.978233</td>\n",
       "      <td>0.978665</td>\n",
       "      <td>char+stemmed word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.979461</td>\n",
       "      <td>0.974343</td>\n",
       "      <td>0.968274</td>\n",
       "      <td>0.973835</td>\n",
       "      <td>0.974261</td>\n",
       "      <td>char+stemmed word+POS tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.982028</td>\n",
       "      <td>0.973091</td>\n",
       "      <td>0.963476</td>\n",
       "      <td>0.972664</td>\n",
       "      <td>0.973032</td>\n",
       "      <td>char+word+stemmed word+POS tag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Accuracy    Recall        F1   ROC-AUC  \\\n",
       "0   0.980745  0.978723  0.975734  0.978233  0.978665   \n",
       "1   0.979461  0.974343  0.968274  0.973835  0.974261   \n",
       "2   0.982028  0.973091  0.963476  0.972664  0.973032   \n",
       "\n",
       "                           Vector  \n",
       "0               char+stemmed word  \n",
       "1       char+stemmed word+POS tag  \n",
       "2  char+word+stemmed word+POS tag  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByVector=DataFrame(ScoreSummaryByVector,columns=['Precision','Accuracy','Recall','F1','ROC-AUC','Vector'])\n",
    "df_ScoreSummaryByVector.sort_values(['F1'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TopFeatures_df=most_informative_feature_for_binary_classification(feature_names, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>CoefChar</th>\n",
       "      <th>Char</th>\n",
       "      <th>CoefWord</th>\n",
       "      <th>Word</th>\n",
       "      <th>CoefText</th>\n",
       "      <th>Text</th>\n",
       "      <th>CoefTextPOS</th>\n",
       "      <th>TextPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>-0.786156</td>\n",
       "      <td>he</td>\n",
       "      <td>-0.701405</td>\n",
       "      <td>url</td>\n",
       "      <td>-0.845652</td>\n",
       "      <td>the</td>\n",
       "      <td>-1.111849</td>\n",
       "      <td>NNP-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>-0.898761</td>\n",
       "      <td>ing</td>\n",
       "      <td>-0.845652</td>\n",
       "      <td>the</td>\n",
       "      <td>-1.055943</td>\n",
       "      <td>to</td>\n",
       "      <td>-1.382994</td>\n",
       "      <td>IN-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>-0.923301</td>\n",
       "      <td>to</td>\n",
       "      <td>-1.055943</td>\n",
       "      <td>to</td>\n",
       "      <td>-1.163141</td>\n",
       "      <td>of</td>\n",
       "      <td>-1.799984</td>\n",
       "      <td>NNP-IN-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>-1.025673</td>\n",
       "      <td>re</td>\n",
       "      <td>-1.163141</td>\n",
       "      <td>of</td>\n",
       "      <td>-1.251107</td>\n",
       "      <td>a</td>\n",
       "      <td>-1.845791</td>\n",
       "      <td>DT-JJ-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>-1.030271</td>\n",
       "      <td>to</td>\n",
       "      <td>-1.402861</td>\n",
       "      <td>is</td>\n",
       "      <td>-1.402861</td>\n",
       "      <td>is</td>\n",
       "      <td>-1.866843</td>\n",
       "      <td>NN-IN-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>-1.039530</td>\n",
       "      <td>ng</td>\n",
       "      <td>-1.597966</td>\n",
       "      <td>in</td>\n",
       "      <td>-1.585845</td>\n",
       "      <td>i</td>\n",
       "      <td>-1.944226</td>\n",
       "      <td>NNP-JJ-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>-1.089599</td>\n",
       "      <td>of</td>\n",
       "      <td>-1.626833</td>\n",
       "      <td>by</td>\n",
       "      <td>-1.589869</td>\n",
       "      <td>s</td>\n",
       "      <td>-1.949989</td>\n",
       "      <td>NNP-NNP-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>-1.096961</td>\n",
       "      <td>on</td>\n",
       "      <td>-1.678344</td>\n",
       "      <td>ref</td>\n",
       "      <td>-1.597966</td>\n",
       "      <td>in</td>\n",
       "      <td>-1.991290</td>\n",
       "      <td>NNP-NNP-JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>-1.163141</td>\n",
       "      <td>of</td>\n",
       "      <td>-1.687194</td>\n",
       "      <td>it</td>\n",
       "      <td>-1.626833</td>\n",
       "      <td>by</td>\n",
       "      <td>-1.991290</td>\n",
       "      <td>DT-NN-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>-1.173723</td>\n",
       "      <td>is</td>\n",
       "      <td>-1.840597</td>\n",
       "      <td>for</td>\n",
       "      <td>-1.647975</td>\n",
       "      <td>it</td>\n",
       "      <td>-2.009527</td>\n",
       "      <td>IN-DT-NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Author  CoefChar Char  CoefWord Word  CoefText Text  CoefTextPOS  \\\n",
       "0  RichardDawkins -0.786156  he  -0.701405  url -0.845652  the    -1.111849   \n",
       "1  RichardDawkins -0.898761  ing -0.845652  the -1.055943   to    -1.382994   \n",
       "2  RichardDawkins -0.923301   to -1.055943   to -1.163141   of    -1.799984   \n",
       "3  RichardDawkins -1.025673   re -1.163141   of -1.251107    a    -1.845791   \n",
       "4  RichardDawkins -1.030271  to  -1.402861   is -1.402861   is    -1.866843   \n",
       "5  RichardDawkins -1.039530  ng  -1.597966   in -1.585845    i    -1.944226   \n",
       "6  RichardDawkins -1.089599   of -1.626833   by -1.589869    s    -1.949989   \n",
       "7  RichardDawkins -1.096961  on  -1.678344  ref -1.597966   in    -1.991290   \n",
       "8  RichardDawkins -1.163141  of  -1.687194   it -1.626833   by    -1.991290   \n",
       "9  RichardDawkins -1.173723  is  -1.840597  for -1.647975   it    -2.009527   \n",
       "\n",
       "       TextPOS  \n",
       "0  NNP-NNP-NNP  \n",
       "1   IN-NNP-NNP  \n",
       "2   NNP-IN-NNP  \n",
       "3     DT-JJ-NN  \n",
       "4    NN-IN-NNP  \n",
       "5    NNP-JJ-NN  \n",
       "6   NNP-NNP-IN  \n",
       "7   NNP-NNP-JJ  \n",
       "8     DT-NN-IN  \n",
       "9     IN-DT-NN  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='char')),['Author','Coef','Feature']].head(10)\n",
    "df1.rename(columns={'Coef':'CoefChar','Feature':'Char'}, inplace=True)\n",
    "df1.reset_index(inplace=True)\n",
    "df2=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='word')),['Coef','Feature']].head(10)\n",
    "df2.rename(columns={'Coef':'CoefWord','Feature':'Word'}, inplace=True)\n",
    "df2.reset_index(inplace=True)\n",
    "df3=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='text')),['Coef','Feature']].head(10)\n",
    "df3.rename(columns={'Coef':'CoefText','Feature':'Text'}, inplace=True)\n",
    "df3.reset_index(inplace=True)\n",
    "df4=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='text_pos')),['Coef','Feature']].head(10)\n",
    "df4.rename(columns={'Coef':'CoefTextPOS','Feature':'TextPOS'}, inplace=True)\n",
    "df4['TextPOS']=df4.apply(lambda x: text_pos_inv_convert(x['TextPOS']), axis=1)\n",
    "df4.reset_index(inplace=True)\n",
    "df_kk_top_features = pd.concat([df1,df2,df3,df4],axis=1)\n",
    "df_kk_top_features.drop('index', axis=1, inplace=True)\n",
    "df_kk_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>CoefChar</th>\n",
       "      <th>Char</th>\n",
       "      <th>CoefWord</th>\n",
       "      <th>Word</th>\n",
       "      <th>CoefText</th>\n",
       "      <th>Text</th>\n",
       "      <th>CoefTextPOS</th>\n",
       "      <th>TextPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NASA</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>#d</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>aasnum</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>aasnum</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>VB-VBG-RP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NASA</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>#e</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>aasnumpic</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>aasnump</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>VB-WP-DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NASA</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>#g</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>abcs</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>abel</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>VB-WRB-DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NASA</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>#j</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>abell</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>abinumghzd</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>VB-WRB-JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NASA</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>#m</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>abinumghzd</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>aboard</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>VB-WRB-NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NASA</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>#v</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>aboard</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>abscicon</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>VB-WRB-PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NASA</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>#y</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>abscicon</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>abund</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>VB-CC-DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NASA</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>a.</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>abundant</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>academia</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>VB-CC-EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NASA</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>aq</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>academia</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>access</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>VB-CC-TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NASA</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>cy</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>accepting</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>accredit</td>\n",
       "      <td>-11.708509</td>\n",
       "      <td>VB-CD-NNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Author   CoefChar Char   CoefWord        Word   CoefText        Text  \\\n",
       "0   NASA -11.708509   #d -11.708509      aasnum -11.708509      aasnum   \n",
       "1   NASA -11.708509   #e -11.708509   aasnumpic -11.708509     aasnump   \n",
       "2   NASA -11.708509   #g -11.708509        abcs -11.708509        abel   \n",
       "3   NASA -11.708509   #j -11.708509       abell -11.708509  abinumghzd   \n",
       "4   NASA -11.708509   #m -11.708509  abinumghzd -11.708509      aboard   \n",
       "5   NASA -11.708509   #v -11.708509      aboard -11.708509    abscicon   \n",
       "6   NASA -11.708509   #y -11.708509    abscicon -11.708509       abund   \n",
       "7   NASA -11.708509   a. -11.708509    abundant -11.708509    academia   \n",
       "8   NASA -11.708509   aq -11.708509    academia -11.708509      access   \n",
       "9   NASA -11.708509   cy -11.708509   accepting -11.708509    accredit   \n",
       "\n",
       "   CoefTextPOS     TextPOS  \n",
       "0   -11.708509   VB-VBG-RP  \n",
       "1   -11.708509    VB-WP-DT  \n",
       "2   -11.708509   VB-WRB-DT  \n",
       "3   -11.708509   VB-WRB-JJ  \n",
       "4   -11.708509  VB-WRB-NNS  \n",
       "5   -11.708509  VB-WRB-PRP  \n",
       "6   -11.708509    VB-CC-DT  \n",
       "7   -11.708509    VB-CC-EX  \n",
       "8   -11.708509    VB-CC-TO  \n",
       "9   -11.708509   VB-CD-NNS  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='char')),['Author','Coef','Feature']].head(10)\n",
    "df1.rename(columns={'Coef':'CoefChar','Feature':'Char'}, inplace=True)\n",
    "df1.reset_index(inplace=True)\n",
    "df2=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='word')),['Coef','Feature']].head(10)\n",
    "df2.rename(columns={'Coef':'CoefWord','Feature':'Word'}, inplace=True)\n",
    "df2.reset_index(inplace=True)\n",
    "df3=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='text')),['Coef','Feature']].head(10)\n",
    "df3.rename(columns={'Coef':'CoefText','Feature':'Text'}, inplace=True)\n",
    "df3.reset_index(inplace=True)\n",
    "df4=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='text_pos')),['Coef','Feature']].head(10)\n",
    "df4.rename(columns={'Coef':'CoefTextPOS','Feature':'TextPOS'}, inplace=True)\n",
    "df4['TextPOS']=df4.apply(lambda x: text_pos_inv_convert(x['TextPOS']), axis=1)\n",
    "df4.reset_index(inplace=True)\n",
    "df_kk_top_features = pd.concat([df1,df2,df3,df4],axis=1)\n",
    "df_kk_top_features.drop('index', axis=1, inplace=True)\n",
    "df_kk_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>.REF We know you've been 'Across the Universe'...</td>\n",
       "      <td>NASA</td>\n",
       "      <td>RichardDawkins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NASA logos: The Meatball v. The Worm. Which do...</td>\n",
       "      <td>NASA</td>\n",
       "      <td>RichardDawkins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>REF Glad you like it! Actually, we have severa...</td>\n",
       "      <td>NASA</td>\n",
       "      <td>RichardDawkins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Comments from REF about Saturday's tragic shoo...</td>\n",
       "      <td>NASA</td>\n",
       "      <td>RichardDawkins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>RT REF You have to love REF they do have a sen...</td>\n",
       "      <td>NASA</td>\n",
       "      <td>RichardDawkins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Good news! REF received a green beacon from th...</td>\n",
       "      <td>NASA</td>\n",
       "      <td>RichardDawkins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>REF Both places are fascinating to study and h...</td>\n",
       "      <td>NASA</td>\n",
       "      <td>RichardDawkins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>People didn't always think airplanes could car...</td>\n",
       "      <td>NASA</td>\n",
       "      <td>RichardDawkins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>My on-stage conversation with the very interes...</td>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>NASA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>Using binoculars to project an image on the wa...</td>\n",
       "      <td>RichardDawkins</td>\n",
       "      <td>NASA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text          author  \\\n",
       "36   .REF We know you've been 'Across the Universe'...            NASA   \n",
       "43   NASA logos: The Meatball v. The Worm. Which do...            NASA   \n",
       "140  REF Glad you like it! Actually, we have severa...            NASA   \n",
       "155  Comments from REF about Saturday's tragic shoo...            NASA   \n",
       "233  RT REF You have to love REF they do have a sen...            NASA   \n",
       "243  Good news! REF received a green beacon from th...            NASA   \n",
       "253  REF Both places are fascinating to study and h...            NASA   \n",
       "268  People didn't always think airplanes could car...            NASA   \n",
       "349  My on-stage conversation with the very interes...  RichardDawkins   \n",
       "390  Using binoculars to project an image on the wa...  RichardDawkins   \n",
       "\n",
       "          predicted  \n",
       "36   RichardDawkins  \n",
       "43   RichardDawkins  \n",
       "140  RichardDawkins  \n",
       "155  RichardDawkins  \n",
       "233  RichardDawkins  \n",
       "243  RichardDawkins  \n",
       "253  RichardDawkins  \n",
       "268  RichardDawkins  \n",
       "349            NASA  \n",
       "390            NASA  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_predicted=pd.DataFrame(author_predicted,columns=['predicted'])\n",
    "df_wrong_result = pd.concat([twt_test.reset_index(),author_test.reset_index(),author_predicted], axis=1)\n",
    "df_wrong_result.drop('index', axis=1, inplace=True)\n",
    "df_wrong_result.drop('text_pos', axis=1, inplace=True)\n",
    "df_wrong_result=df_wrong_result[df_wrong_result['author']<>df_wrong_result['predicted']]\n",
    "df_wrong_result.head(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
