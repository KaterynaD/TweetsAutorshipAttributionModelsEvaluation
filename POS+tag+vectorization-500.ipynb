{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14044"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data\n",
    "df=pd.read_csv('C:\\Kate\\Python\\Authorship Attribution\\data\\AllTweets.csv')\n",
    "author1='KimKardashian'\n",
    "author2='HillaryClinton'\n",
    "df_kk=df.loc[(df['author'] == author1)]\n",
    "df_hc=df.loc[(df['author'] == author2)]\n",
    "df=df_kk.append(df_hc,ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "#2000 random sample rows for KK\n",
    "rows = random.sample(df_kk.index, 500)\n",
    "df_kk = df_kk.ix[rows]\n",
    "#2000 random sample rows for HC\n",
    "rows = random.sample(df_hc.index, 500)\n",
    "df_hc = df_hc.ix[rows]\n",
    "#join back together\n",
    "df=df_kk.append(df_hc,ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "950"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data pre-processing\n",
    "df.drop(df[df.retweet==True].index, inplace=True)\n",
    "df['num_of_words'] = df[\"text\"].str.split().apply(len)\n",
    "df.drop(df[df.num_of_words<4].index, inplace=True)\n",
    "df[\"text\"].replace(r\"http\\S+\", \"URL\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"@\\S+\", \"REF\", regex=True ,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/.-](\\d{1,2})[/.-](\\d{2,4})+\", \"DATE\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"\\d+\", \"NUM\", regex=True,inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "pos_code_map={'CC':'A','CD':'B','DT':'C','EX':'D','FW':'E','IN':'F','JJ':'G','JJR':'H','JJS':'I','LS':'J','MD':'K','NN':'L','NNS':'M',\n",
    "'NNP':'N','NNPS':'O','PDT':'P','POS':'Q','PRP':'R','PRP$':'S','RB':'T','RBR':'U','RBS':'V','RP':'W','SYM':'X','TO':'Y','UH':'Z',\n",
    "'VB':'1','VBD':'2','VBG':'3','VBN':'4','VBP':'5','VBZ':'6','WDT':'7','WP':'8','WP$':'9','WRB':'@'}\n",
    "code_pos_map={v: k for k, v in pos_code_map.iteritems()}\n",
    "#Python 3 inv_map = {v: k for k, v in my_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert(tag):\n",
    "    try:\n",
    "        code=pos_code_map[tag]\n",
    "    except:\n",
    "        code='?'\n",
    "    return code\n",
    "def inv_convert(code):\n",
    "    try:\n",
    "        tag=code_pos_map[code]\n",
    "    except:\n",
    "        tag='?'\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "def pos_tags(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed=tokenizer.tokenize(text)\n",
    "    return \"\".join(convert(tag) for (word, tag) in nltk.pos_tag(text_processed))\n",
    "def text_pos_inv_convert(text):\n",
    "    return \"-\".join(inv_convert(c.upper()) for c in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['text_pos']=df.apply(lambda x: pos_tags(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>text_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>Happy birthday Khloball!!! URL  #HappyBirthday...</td>\n",
       "      <td>GLNNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>I'm soooo sleepy, too lazy to get up and wash ...</td>\n",
       "      <td>R5GLTGY1WA1S1WTA1SMAR5Y1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>My message to REF on her NUMth birthday! URL  ...</td>\n",
       "      <td>SLY1FRNLNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>East coast Keeping Up With the Kardashians: Ab...</td>\n",
       "      <td>NL3WFCMFNMTFNR53FR5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>Such a tremendous honor to be on the cover of ...</td>\n",
       "      <td>GCGLY1FCLFNRT2CK11SNK1GLLL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                                               text  \\\n",
       "0  KimKardashian  Happy birthday Khloball!!! URL  #HappyBirthday...   \n",
       "1  KimKardashian  I'm soooo sleepy, too lazy to get up and wash ...   \n",
       "2  KimKardashian  My message to REF on her NUMth birthday! URL  ...   \n",
       "4  KimKardashian  East coast Keeping Up With the Kardashians: Ab...   \n",
       "5  KimKardashian  Such a tremendous honor to be on the cover of ...   \n",
       "\n",
       "                     text_pos  \n",
       "0                       GLNNN  \n",
       "1    R5GLTGY1WA1S1WTA1SMAR5Y1  \n",
       "2                  SLY1FRNLNN  \n",
       "4         NL3WFCMFNMTFNR53FR5  \n",
       "5  GCGLY1FCLFNRT2CK11SNK1GLLL  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix[:,['author','text','text_pos']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "for a in df.author.unique():\n",
    "    v = CountVectorizer(analyzer='char',ngram_range=(7, 7))\n",
    "    ngrams = v.fit_transform(df[df['author'] == a]['text_pos'])\n",
    "    df_t=pd.DataFrame(\n",
    "    {'Feature': v.get_feature_names(),\n",
    "     'Count': list(ngrams.sum(axis=0).flat),\n",
    "     'Author': a\n",
    "    })\n",
    "    #\n",
    "    df_features=df_features.append(df_t,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features['Feature_POS']=df_features.apply(lambda x: text_pos_inv_convert(x['Feature']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Count</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4839</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>3</td>\n",
       "      <td>4fclfnn</td>\n",
       "      <td>VBN-IN-DT-NN-IN-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9264</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>3</td>\n",
       "      <td>nnnllll</td>\n",
       "      <td>NNP-NNP-NNP-NN-NN-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9236</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>3</td>\n",
       "      <td>nnn2cgl</td>\n",
       "      <td>NNP-NNP-NNP-VBD-DT-JJ-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7962</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>3</td>\n",
       "      <td>lk1cglf</td>\n",
       "      <td>NN-MD-VB-DT-JJ-NN-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9560</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>3</td>\n",
       "      <td>r5cl8k1</td>\n",
       "      <td>PRP-VBP-DT-NN-WP-MD-VB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author  Count  Feature               Feature_POS\n",
       "4839  HillaryClinton      3  4fclfnn   VBN-IN-DT-NN-IN-NNP-NNP\n",
       "9264  HillaryClinton      3  nnnllll   NNP-NNP-NNP-NN-NN-NN-NN\n",
       "9236  HillaryClinton      3  nnn2cgl  NNP-NNP-NNP-VBD-DT-JJ-NN\n",
       "7962  HillaryClinton      3  lk1cglf      NN-MD-VB-DT-JJ-NN-IN\n",
       "9560  HillaryClinton      3  r5cl8k1    PRP-VBP-DT-NN-WP-MD-VB"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[~df_features.Feature.isin(df_features[df_features['Author'] != author2].Feature)].sort_values('Count', ascending=False).ix[:,['Author','Count','Feature','Feature_POS']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Count</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>4</td>\n",
       "      <td>lfnnnnn</td>\n",
       "      <td>NN-IN-NNP-NNP-NNP-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>3</td>\n",
       "      <td>gnnr5r5</td>\n",
       "      <td>JJ-NNP-NNP-PRP-VBP-PRP-VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>3</td>\n",
       "      <td>fco6fnn</td>\n",
       "      <td>IN-DT-NNPS-VBZ-IN-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3975</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>3</td>\n",
       "      <td>wfco6fn</td>\n",
       "      <td>RP-IN-DT-NNPS-VBZ-IN-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>3</td>\n",
       "      <td>nnr5r5g</td>\n",
       "      <td>NNP-NNP-PRP-VBP-PRP-VBP-JJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Author  Count  Feature                 Feature_POS\n",
       "2025  KimKardashian      4  lfnnnnn   NN-IN-NNP-NNP-NNP-NNP-NNP\n",
       "1726  KimKardashian      3  gnnr5r5  JJ-NNP-NNP-PRP-VBP-PRP-VBP\n",
       "1265  KimKardashian      3  fco6fnn   IN-DT-NNPS-VBZ-IN-NNP-NNP\n",
       "3975  KimKardashian      3  wfco6fn    RP-IN-DT-NNPS-VBZ-IN-NNP\n",
       "3164  KimKardashian      3  nnr5r5g  NNP-NNP-PRP-VBP-PRP-VBP-JJ"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[~df_features.Feature.isin(df_features[df_features['Author'] != author1].Feature)].sort_values('Count', ascending=False).ix[:,['Author','Count','Feature','Feature_POS']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "twt_train, twt_test, author_train, author_test = train_test_split(df.ix[:,['text','text_pos']], df['author'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def text_process(text):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Tokenizes and removes punctuation\n",
    "    3. Stems\n",
    "    4. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenizing\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed=tokenizer.tokenize(text)\n",
    "    \n",
    "    \n",
    "    # steming\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    \n",
    "    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
    "    \n",
    "\n",
    "    return text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ScoreSummaryByModelParams = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextAndTextCodedExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the text & text_pos from a tweet in a single pass.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, tweets):\n",
    "        features=tweets.ix[:,['text_pos','text']].to_records(index=False)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ModelParamsEvaluation (f_union,model,params,comment):\n",
    "    pipeline = Pipeline([\n",
    "    # Extract the text & text_coded\n",
    "    ('textandtextcoded', TextAndTextCodedExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from text and text_coded\n",
    "    ('union', f_union, ),\n",
    "\n",
    "    # Use a  classifier on the combined features\n",
    "    ('clf', model),\n",
    "    ])\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=params, verbose=1, cv=10)\n",
    "    grid_search.fit(twt_train, author_train)\n",
    "    #best score\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        ScoreSummaryByModelParams.append([comment,grid_search.best_score_,\"\\t%s: %r\" % (param_name, best_parameters[param_name])])    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "              # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char')),\n",
    "            ])),               \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    2.9s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   12.5s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:   28.7s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:   53.3s\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.953\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tunion__char__tfidf__max_df: 0.5\n",
      "\tunion__char__tfidf__max_features: None\n",
      "\tunion__char__tfidf__ngram_range: (3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1440 out of 1440 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "p = {\n",
    "    'union__char__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__char__tfidf__ngram_range': ((2, 2), (3, 3)), \n",
    "    'union__char__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "            ('selector', ItemSelector(key='text')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='word')),\n",
    "            ])),              \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 720 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    2.3s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   10.1s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:   23.4s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:   42.3s\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  1.1min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  1.6min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:  2.1min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed:  2.8min\n",
      "[Parallel(n_jobs=1)]: Done 4049 tasks       | elapsed:  3.6min\n",
      "[Parallel(n_jobs=1)]: Done 4999 tasks       | elapsed:  4.5min\n",
      "[Parallel(n_jobs=1)]: Done 6049 tasks       | elapsed:  5.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.934\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tunion__word__tfidf__max_df: 0.5\n",
      "\tunion__word__tfidf__max_features: None\n",
      "\tunion__word__tfidf__ngram_range: (1, 1)\n",
      "\tunion__word__tfidf__stop_words: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 7199 tasks       | elapsed:  6.5min\n",
      "[Parallel(n_jobs=1)]: Done 7200 out of 7200 | elapsed:  6.5min finished\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__word__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__word__tfidf__ngram_range': ((1, 1),(2, 2), (3, 3),(4,4),(5,5)), \n",
    "    'union__word__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'union__word__tfidf__stop_words': (None, 'english'),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('text', Pipeline([\n",
    "            ('selector', ItemSelector(key='text')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process)),\n",
    "            ])),              \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 360 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    7.0s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   28.9s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:  1.1min\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  1.9min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  3.0min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  4.3min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:  5.9min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed:  7.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.944\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n",
      "\tunion__text__tfidf__max_df: 0.5\n",
      "\tunion__text__tfidf__max_features: None\n",
      "\tunion__text__tfidf__ngram_range: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 3600 out of 3600 | elapsed:  8.6min finished\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__text__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__text__tfidf__ngram_range': ((1, 1),(2, 2), (3, 3),(4,4),(5,5)), \n",
    "    'union__text__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, stemmed words, no stop words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling pos tag features  from the text_pos\n",
    "            ('text_pos', Pipeline([\n",
    "            ('selector', ItemSelector(key='text_pos')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='char')),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 360 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    1.9s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:    8.0s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:   18.1s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:   32.4s\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:   51.0s\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  1.2min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:  1.7min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed:  2.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.716\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tunion__text_pos__tfidf__max_df: 0.5\n",
      "\tunion__text_pos__tfidf__max_features: None\n",
      "\tunion__text_pos__tfidf__ngram_range: (3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 3600 out of 3600 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__text_pos__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__text_pos__tfidf__ngram_range': ((3, 3), (4, 4),(5,5),(6,6),(7,7)), \n",
    "    'union__text_pos__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>BestScore</th>\n",
       "      <th>BestParameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.953383</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.953383</td>\n",
       "      <td>\\tunion__char__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.953383</td>\n",
       "      <td>\\tunion__char__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.953383</td>\n",
       "      <td>\\tunion__char__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.944361</td>\n",
       "      <td>\\tunion__text__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.944361</td>\n",
       "      <td>\\tunion__text__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.944361</td>\n",
       "      <td>\\tunion__text__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.944361</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.933835</td>\n",
       "      <td>\\tunion__word__tfidf__stop_words: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.933835</td>\n",
       "      <td>\\tunion__word__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.933835</td>\n",
       "      <td>\\tunion__word__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.933835</td>\n",
       "      <td>\\tunion__word__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.933835</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>\\tunion__text_pos__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Method  BestScore  \\\n",
       "0                         Bernoulli Naive Bayes, char   0.953383   \n",
       "1                         Bernoulli Naive Bayes, char   0.953383   \n",
       "2                         Bernoulli Naive Bayes, char   0.953383   \n",
       "3                         Bernoulli Naive Bayes, char   0.953383   \n",
       "12  Bernoulli Naive Bayes, stemmed words, no stop ...   0.944361   \n",
       "11  Bernoulli Naive Bayes, stemmed words, no stop ...   0.944361   \n",
       "10  Bernoulli Naive Bayes, stemmed words, no stop ...   0.944361   \n",
       "9   Bernoulli Naive Bayes, stemmed words, no stop ...   0.944361   \n",
       "8                         Bernoulli Naive Bayes, word   0.933835   \n",
       "7                         Bernoulli Naive Bayes, word   0.933835   \n",
       "6                         Bernoulli Naive Bayes, word   0.933835   \n",
       "5                         Bernoulli Naive Bayes, word   0.933835   \n",
       "4                         Bernoulli Naive Bayes, word   0.933835   \n",
       "13                    Bernoulli Naive Bayes, POS tags   0.715789   \n",
       "14                    Bernoulli Naive Bayes, POS tags   0.715789   \n",
       "15                    Bernoulli Naive Bayes, POS tags   0.715789   \n",
       "16                    Bernoulli Naive Bayes, POS tags   0.715789   \n",
       "\n",
       "                                    BestParameter  \n",
       "0                               \\tclf__alpha: 0.1  \n",
       "1               \\tunion__char__tfidf__max_df: 0.5  \n",
       "2        \\tunion__char__tfidf__max_features: None  \n",
       "3       \\tunion__char__tfidf__ngram_range: (3, 3)  \n",
       "12      \\tunion__text__tfidf__ngram_range: (1, 1)  \n",
       "11       \\tunion__text__tfidf__max_features: None  \n",
       "10              \\tunion__text__tfidf__max_df: 0.5  \n",
       "9                             \\tclf__alpha: 0.001  \n",
       "8          \\tunion__word__tfidf__stop_words: None  \n",
       "7       \\tunion__word__tfidf__ngram_range: (1, 1)  \n",
       "6        \\tunion__word__tfidf__max_features: None  \n",
       "5               \\tunion__word__tfidf__max_df: 0.5  \n",
       "4                               \\tclf__alpha: 0.1  \n",
       "13                              \\tclf__alpha: 0.1  \n",
       "14          \\tunion__text_pos__tfidf__max_df: 0.5  \n",
       "15   \\tunion__text_pos__tfidf__max_features: None  \n",
       "16  \\tunion__text_pos__tfidf__ngram_range: (3, 3)  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\n",
    "df_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByModelParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    4.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.953\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    5.7s finished\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f2_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling stememd word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    8.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.958\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   10.9s finished\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f2_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),    \n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   10.3s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   12.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.956\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.0001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    9.8s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   12.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.952\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + stemmed word + POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    5.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.949\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    6.9s finished\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f4_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "             # Pipeline for pulling word features after word_processing from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   11.1s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   13.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.953\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f4_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + stemmed word + POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>BestScore</th>\n",
       "      <th>BestParameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bernoulli Naive Bayes, char + stemmed word</td>\n",
       "      <td>0.957895</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + stemmed word</td>\n",
       "      <td>0.956391</td>\n",
       "      <td>\\tclf__alpha: 0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.953383</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.953383</td>\n",
       "      <td>\\tunion__char__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word</td>\n",
       "      <td>0.953383</td>\n",
       "      <td>\\tclf__alpha: 0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + stemmed w...</td>\n",
       "      <td>0.953383</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.953383</td>\n",
       "      <td>\\tunion__char__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.953383</td>\n",
       "      <td>\\tunion__char__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bernoulli Naive Bayes, char + stemmed word + P...</td>\n",
       "      <td>0.951880</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + POS tags</td>\n",
       "      <td>0.948872</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.944361</td>\n",
       "      <td>\\tunion__text__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.944361</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.944361</td>\n",
       "      <td>\\tunion__text__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.944361</td>\n",
       "      <td>\\tunion__text__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.933835</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.933835</td>\n",
       "      <td>\\tunion__word__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.933835</td>\n",
       "      <td>\\tunion__word__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.933835</td>\n",
       "      <td>\\tunion__word__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.933835</td>\n",
       "      <td>\\tunion__word__tfidf__stop_words: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>\\tunion__text_pos__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Method  BestScore  \\\n",
       "18         Bernoulli Naive Bayes, char + stemmed word   0.957895   \n",
       "19  Bernoulli Naive Bayes, char + word + stemmed word   0.956391   \n",
       "0                         Bernoulli Naive Bayes, char   0.953383   \n",
       "1                         Bernoulli Naive Bayes, char   0.953383   \n",
       "17                 Bernoulli Naive Bayes, char + word   0.953383   \n",
       "22  Bernoulli Naive Bayes, char + word + stemmed w...   0.953383   \n",
       "2                         Bernoulli Naive Bayes, char   0.953383   \n",
       "3                         Bernoulli Naive Bayes, char   0.953383   \n",
       "20  Bernoulli Naive Bayes, char + stemmed word + P...   0.951880   \n",
       "21      Bernoulli Naive Bayes, char + word + POS tags   0.948872   \n",
       "11  Bernoulli Naive Bayes, stemmed words, no stop ...   0.944361   \n",
       "9   Bernoulli Naive Bayes, stemmed words, no stop ...   0.944361   \n",
       "10  Bernoulli Naive Bayes, stemmed words, no stop ...   0.944361   \n",
       "12  Bernoulli Naive Bayes, stemmed words, no stop ...   0.944361   \n",
       "4                         Bernoulli Naive Bayes, word   0.933835   \n",
       "6                         Bernoulli Naive Bayes, word   0.933835   \n",
       "7                         Bernoulli Naive Bayes, word   0.933835   \n",
       "5                         Bernoulli Naive Bayes, word   0.933835   \n",
       "8                         Bernoulli Naive Bayes, word   0.933835   \n",
       "16                    Bernoulli Naive Bayes, POS tags   0.715789   \n",
       "15                    Bernoulli Naive Bayes, POS tags   0.715789   \n",
       "13                    Bernoulli Naive Bayes, POS tags   0.715789   \n",
       "14                    Bernoulli Naive Bayes, POS tags   0.715789   \n",
       "\n",
       "                                    BestParameter  \n",
       "18                              \\tclf__alpha: 0.1  \n",
       "19                           \\tclf__alpha: 0.0001  \n",
       "0                               \\tclf__alpha: 0.1  \n",
       "1               \\tunion__char__tfidf__max_df: 0.5  \n",
       "17                           \\tclf__alpha: 0.0001  \n",
       "22                              \\tclf__alpha: 0.1  \n",
       "2        \\tunion__char__tfidf__max_features: None  \n",
       "3       \\tunion__char__tfidf__ngram_range: (3, 3)  \n",
       "20                              \\tclf__alpha: 0.1  \n",
       "21                              \\tclf__alpha: 0.1  \n",
       "11       \\tunion__text__tfidf__max_features: None  \n",
       "9                             \\tclf__alpha: 0.001  \n",
       "10              \\tunion__text__tfidf__max_df: 0.5  \n",
       "12      \\tunion__text__tfidf__ngram_range: (1, 1)  \n",
       "4                               \\tclf__alpha: 0.1  \n",
       "6        \\tunion__word__tfidf__max_features: None  \n",
       "7       \\tunion__word__tfidf__ngram_range: (1, 1)  \n",
       "5               \\tunion__word__tfidf__max_df: 0.5  \n",
       "8          \\tunion__word__tfidf__stop_words: None  \n",
       "16  \\tunion__text_pos__tfidf__ngram_range: (3, 3)  \n",
       "15   \\tunion__text_pos__tfidf__max_features: None  \n",
       "13                              \\tclf__alpha: 0.1  \n",
       "14          \\tunion__text_pos__tfidf__max_df: 0.5  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\n",
    "df_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByModelParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc,precision_score, accuracy_score, recall_score, f1_score\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ScoreSummaryByVector = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PredictionEvaluation(author_test_b,author_predictions_b,comment):\n",
    "    Precision=precision_score(author_test_b,author_predictions_b)\n",
    "    print ('Precision: %0.3f' % (Precision))\n",
    "    Accuracy=accuracy_score(author_test_b,author_predictions_b)\n",
    "    print ('Accuracy: %0.3f' % (Accuracy))\n",
    "    Recall=recall_score(author_test_b,author_predictions_b)\n",
    "    print ('Recall: %0.3f' % (Recall))\n",
    "    F1=f1_score(author_test_b,author_predictions_b)\n",
    "    print ('F1: %0.3f' % (F1))\n",
    "    print ('Confussion matrix:')\n",
    "    print (confusion_matrix(author_test_b,author_predictions_b))\n",
    "    ROC_AUC=roc_auc_score(author_test_b,author_predictions_b)\n",
    "    print ('ROC-AUC: %0.3f' % (ROC_AUC))\n",
    "    ScoreSummaryByVector.append([Precision,Accuracy,Recall,F1,ROC_AUC,comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ModelRun (f_union,model):\n",
    "    pipeline = Pipeline([\n",
    "    # Extract the text & text_coded\n",
    "    ('textandtextcoded', TextAndTextCodedExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from text and text_coded\n",
    "    ('union', f_union, ),\n",
    "\n",
    "    # Use a  classifier on the combined features\n",
    "    ('clf', model),\n",
    "    ])\n",
    "    pipeline.fit(twt_train, author_train)\n",
    "    author_predicted = pipeline.predict(twt_test)\n",
    "    \n",
    "    feature_names=list()\n",
    "    for p in (pipeline.get_params()['union'].transformer_list):\n",
    "        fn=(p[0],pipeline.get_params()['union'].get_params()[p[0]].get_params()['tfidf'].get_feature_names())\n",
    "        feature_names.append(fn)\n",
    "    df_fn=pd.DataFrame()\n",
    "    for fn in feature_names:\n",
    "        df_fn= df_fn.append(pd.DataFrame(\n",
    "        {'FeatureType': fn[0],\n",
    "         'Feature': fn[1]\n",
    "        }),\n",
    "        ignore_index=True)    \n",
    "    \n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    lb = LabelBinarizer()\n",
    "    author_test_b = lb.fit_transform(author_test.values)\n",
    "    author_predicted_b  = lb.fit_transform(author_predicted)\n",
    "    return (df_fn,pipeline.get_params()['clf'],author_predicted,author_predicted_b, author_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_informative_feature_for_binary_classification(feature_names, classifier):\n",
    "    class_labels = classifier.classes_\n",
    "\n",
    "    topnvalues_class0 = sorted(zip(classifier.coef_[0], feature_names['Feature'].values, feature_names['FeatureType'].values))\n",
    "    topnvalues_class1 = sorted(zip(classifier.coef_[0], feature_names['Feature'].values, feature_names['FeatureType'].values), reverse=True)\n",
    "\n",
    "    topn_df_class0=pd.DataFrame(topnvalues_class0, columns=['Coef','Feature','FeatureType'])\n",
    "    topn_df_class0['Author']=class_labels[0]\n",
    "    \n",
    "    topn_df_class1=pd.DataFrame(topnvalues_class1, columns=['Coef','Feature','FeatureType'])\n",
    "    topn_df_class1['Author']=class_labels[1]    \n",
    "    \n",
    "    topn_df=topn_df_class0.append(topn_df_class1)\n",
    "    \n",
    "        \n",
    "    return topn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling stememd word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f2_union,BernoulliNB(alpha=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.964\n",
      "Accuracy: 0.961\n",
      "Recall: 0.957\n",
      "F1: 0.961\n",
      "Confussion matrix:\n",
      "[[140   5]\n",
      " [  6 134]]\n",
      "ROC-AUC: 0.961\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f3_union,BernoulliNB(alpha=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.957\n",
      "Accuracy: 0.954\n",
      "Recall: 0.950\n",
      "F1: 0.953\n",
      "Confussion matrix:\n",
      "[[139   6]\n",
      " [  7 133]]\n",
      "ROC-AUC: 0.954\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+stemmed word+POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f4_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "             # Pipeline for pulling word features after word_processing from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f4_union,BernoulliNB(alpha=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.942\n",
      "Accuracy: 0.954\n",
      "Recall: 0.963\n",
      "F1: 0.953\n",
      "Confussion matrix:\n",
      "[[141   8]\n",
      " [  5 131]]\n",
      "ROC-AUC: 0.955\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+word+stemmed word+POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.964029</td>\n",
       "      <td>0.961404</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.960573</td>\n",
       "      <td>0.961330</td>\n",
       "      <td>char+stemmed word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.956835</td>\n",
       "      <td>0.954386</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.953405</td>\n",
       "      <td>0.954310</td>\n",
       "      <td>char+stemmed word+POS tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.942446</td>\n",
       "      <td>0.954386</td>\n",
       "      <td>0.963235</td>\n",
       "      <td>0.952727</td>\n",
       "      <td>0.954772</td>\n",
       "      <td>char+word+stemmed word+POS tag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Accuracy    Recall        F1   ROC-AUC  \\\n",
       "0   0.964029  0.961404  0.957143  0.960573  0.961330   \n",
       "1   0.956835  0.954386  0.950000  0.953405  0.954310   \n",
       "2   0.942446  0.954386  0.963235  0.952727  0.954772   \n",
       "\n",
       "                           Vector  \n",
       "0               char+stemmed word  \n",
       "1       char+stemmed word+POS tag  \n",
       "2  char+word+stemmed word+POS tag  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByVector=DataFrame(ScoreSummaryByVector,columns=['Precision','Accuracy','Recall','F1','ROC-AUC','Vector'])\n",
    "df_ScoreSummaryByVector.sort_values(['F1'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TopFeatures_df=most_informative_feature_for_binary_classification(feature_names, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>CoefChar</th>\n",
       "      <th>Char</th>\n",
       "      <th>CoefWord</th>\n",
       "      <th>Word</th>\n",
       "      <th>CoefText</th>\n",
       "      <th>Text</th>\n",
       "      <th>CoefTextPOS</th>\n",
       "      <th>TextPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>\\nhi</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>abandon</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>_</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>VB-VB-VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>\\nnu</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>abhorrent</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>abandon</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>VB-VB-WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>\\nto</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>abiding</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>abhorr</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>VB-VB-CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>\\n</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>able</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>abid</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>VB-VB-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>\"d</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>abortion</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>abl</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>VB-VB-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>\"h</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>abortions</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>abort</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>VB-VB-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>\"t</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>aca</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>aca</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>VB-VB-RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>\"w</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>accept</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>accept</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>VB-VBG-DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>#d</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>account</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>account</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>VB-VBG-JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>#e</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>achieve</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>achiev</td>\n",
       "      <td>-14.984893</td>\n",
       "      <td>VB-VBG-JJR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Author   CoefChar  Char   CoefWord       Word   CoefText     Text  \\\n",
       "0  HillaryClinton -14.984893  \\nhi -14.984893    abandon -14.984893        _   \n",
       "1  HillaryClinton -14.984893  \\nnu -14.984893  abhorrent -14.984893  abandon   \n",
       "2  HillaryClinton -14.984893  \\nto -14.984893    abiding -14.984893   abhorr   \n",
       "3  HillaryClinton -14.984893  \\n  -14.984893       able -14.984893     abid   \n",
       "4  HillaryClinton -14.984893    \"d -14.984893   abortion -14.984893      abl   \n",
       "5  HillaryClinton -14.984893    \"h -14.984893  abortions -14.984893    abort   \n",
       "6  HillaryClinton -14.984893    \"t -14.984893        aca -14.984893      aca   \n",
       "7  HillaryClinton -14.984893    \"w -14.984893     accept -14.984893   accept   \n",
       "8  HillaryClinton -14.984893    #d -14.984893    account -14.984893  account   \n",
       "9  HillaryClinton -14.984893    #e -14.984893    achieve -14.984893   achiev   \n",
       "\n",
       "   CoefTextPOS     TextPOS  \n",
       "0   -14.984893    VB-VB-VB  \n",
       "1   -14.984893    VB-VB-WP  \n",
       "2   -14.984893    VB-VB-CD  \n",
       "3   -14.984893    VB-VB-IN  \n",
       "4   -14.984893    VB-VB-NN  \n",
       "5   -14.984893   VB-VB-NNP  \n",
       "6   -14.984893    VB-VB-RB  \n",
       "7   -14.984893   VB-VBG-DT  \n",
       "8   -14.984893   VB-VBG-JJ  \n",
       "9   -14.984893  VB-VBG-JJR  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='char')),['Author','Coef','Feature']].head(10)\n",
    "df1.rename(columns={'Coef':'CoefChar','Feature':'Char'}, inplace=True)\n",
    "df1.reset_index(inplace=True)\n",
    "df2=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='word')),['Coef','Feature']].head(10)\n",
    "df2.rename(columns={'Coef':'CoefWord','Feature':'Word'}, inplace=True)\n",
    "df2.reset_index(inplace=True)\n",
    "df3=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='text')),['Coef','Feature']].head(10)\n",
    "df3.rename(columns={'Coef':'CoefText','Feature':'Text'}, inplace=True)\n",
    "df3.reset_index(inplace=True)\n",
    "df4=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='text_pos')),['Coef','Feature']].head(10)\n",
    "df4.rename(columns={'Coef':'CoefTextPOS','Feature':'TextPOS'}, inplace=True)\n",
    "df4['TextPOS']=df4.apply(lambda x: text_pos_inv_convert(x['TextPOS']), axis=1)\n",
    "df4.reset_index(inplace=True)\n",
    "df_kk_top_features = pd.concat([df1,df2,df3,df4],axis=1)\n",
    "df_kk_top_features.drop('index', axis=1, inplace=True)\n",
    "df_kk_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>CoefChar</th>\n",
       "      <th>Char</th>\n",
       "      <th>CoefWord</th>\n",
       "      <th>Word</th>\n",
       "      <th>CoefText</th>\n",
       "      <th>Text</th>\n",
       "      <th>CoefTextPOS</th>\n",
       "      <th>TextPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-0.914739</td>\n",
       "      <td>url</td>\n",
       "      <td>-0.930364</td>\n",
       "      <td>url</td>\n",
       "      <td>-0.930364</td>\n",
       "      <td>url</td>\n",
       "      <td>-1.297214</td>\n",
       "      <td>NNP-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-0.946238</td>\n",
       "      <td>re</td>\n",
       "      <td>-1.029619</td>\n",
       "      <td>the</td>\n",
       "      <td>-0.930364</td>\n",
       "      <td>i</td>\n",
       "      <td>-1.823307</td>\n",
       "      <td>IN-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-0.962367</td>\n",
       "      <td>he</td>\n",
       "      <td>-1.083203</td>\n",
       "      <td>ref</td>\n",
       "      <td>-1.029619</td>\n",
       "      <td>the</td>\n",
       "      <td>-2.085670</td>\n",
       "      <td>NN-NN-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-0.978761</td>\n",
       "      <td>ur</td>\n",
       "      <td>-1.443818</td>\n",
       "      <td>to</td>\n",
       "      <td>-1.083203</td>\n",
       "      <td>ref</td>\n",
       "      <td>-2.110988</td>\n",
       "      <td>NN-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-1.003867</td>\n",
       "      <td>ing</td>\n",
       "      <td>-1.511871</td>\n",
       "      <td>my</td>\n",
       "      <td>-1.443818</td>\n",
       "      <td>to</td>\n",
       "      <td>-2.110988</td>\n",
       "      <td>NN-IN-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-1.047163</td>\n",
       "      <td>to</td>\n",
       "      <td>-1.842725</td>\n",
       "      <td>in</td>\n",
       "      <td>-1.511871</td>\n",
       "      <td>my</td>\n",
       "      <td>-2.136963</td>\n",
       "      <td>NNP-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-1.074071</td>\n",
       "      <td>ng</td>\n",
       "      <td>-1.862527</td>\n",
       "      <td>so</td>\n",
       "      <td>-1.804258</td>\n",
       "      <td>a</td>\n",
       "      <td>-2.191030</td>\n",
       "      <td>IN-DT-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-1.083203</td>\n",
       "      <td>ref</td>\n",
       "      <td>-1.924402</td>\n",
       "      <td>this</td>\n",
       "      <td>-1.823307</td>\n",
       "      <td>s</td>\n",
       "      <td>-2.219201</td>\n",
       "      <td>NNP-NNP-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-1.111112</td>\n",
       "      <td>ef</td>\n",
       "      <td>-1.924402</td>\n",
       "      <td>is</td>\n",
       "      <td>-1.842725</td>\n",
       "      <td>in</td>\n",
       "      <td>-2.248189</td>\n",
       "      <td>NN-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-1.355710</td>\n",
       "      <td>is</td>\n",
       "      <td>-1.945909</td>\n",
       "      <td>for</td>\n",
       "      <td>-1.862527</td>\n",
       "      <td>so</td>\n",
       "      <td>-2.248189</td>\n",
       "      <td>DT-NN-IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Author  CoefChar Char  CoefWord  Word  CoefText Text  CoefTextPOS  \\\n",
       "0  KimKardashian -0.914739  url -0.930364   url -0.930364  url    -1.297214   \n",
       "1  KimKardashian -0.946238   re -1.029619   the -0.930364    i    -1.823307   \n",
       "2  KimKardashian -0.962367  he  -1.083203   ref -1.029619  the    -2.085670   \n",
       "3  KimKardashian -0.978761   ur -1.443818    to -1.083203  ref    -2.110988   \n",
       "4  KimKardashian -1.003867  ing -1.511871    my -1.443818   to    -2.110988   \n",
       "5  KimKardashian -1.047163   to -1.842725    in -1.511871   my    -2.136963   \n",
       "6  KimKardashian -1.074071  ng  -1.862527    so -1.804258    a    -2.191030   \n",
       "7  KimKardashian -1.083203  ref -1.924402  this -1.823307    s    -2.219201   \n",
       "8  KimKardashian -1.111112  ef  -1.924402    is -1.842725   in    -2.248189   \n",
       "9  KimKardashian -1.355710  is  -1.945909   for -1.862527   so    -2.248189   \n",
       "\n",
       "       TextPOS  \n",
       "0  NNP-NNP-NNP  \n",
       "1   IN-NNP-NNP  \n",
       "2    NN-NN-NNP  \n",
       "3   NN-NNP-NNP  \n",
       "4    NN-IN-NNP  \n",
       "5    NNP-NN-NN  \n",
       "6     IN-DT-NN  \n",
       "7   NNP-NNP-NN  \n",
       "8     NN-NN-NN  \n",
       "9     DT-NN-IN  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='char')),['Author','Coef','Feature']].head(10)\n",
    "df1.rename(columns={'Coef':'CoefChar','Feature':'Char'}, inplace=True)\n",
    "df1.reset_index(inplace=True)\n",
    "df2=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='word')),['Coef','Feature']].head(10)\n",
    "df2.rename(columns={'Coef':'CoefWord','Feature':'Word'}, inplace=True)\n",
    "df2.reset_index(inplace=True)\n",
    "df3=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='text')),['Coef','Feature']].head(10)\n",
    "df3.rename(columns={'Coef':'CoefText','Feature':'Text'}, inplace=True)\n",
    "df3.reset_index(inplace=True)\n",
    "df4=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='text_pos')),['Coef','Feature']].head(10)\n",
    "df4.rename(columns={'Coef':'CoefTextPOS','Feature':'TextPOS'}, inplace=True)\n",
    "df4['TextPOS']=df4.apply(lambda x: text_pos_inv_convert(x['TextPOS']), axis=1)\n",
    "df4.reset_index(inplace=True)\n",
    "df_kk_top_features = pd.concat([df1,df2,df3,df4],axis=1)\n",
    "df_kk_top_features.drop('index', axis=1, inplace=True)\n",
    "df_kk_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York State of mind URL</td>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>HillaryClinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A wise woman once told me this...her name migh...</td>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>HillaryClinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Im #BreakingUpWith my trainer because I only ...</td>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>HillaryClinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Every time u tweet #BeatCancer EBay &amp; Miller/C...</td>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>HillaryClinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>RT REF Live without pretending. Love without d...</td>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>HillaryClinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Thank you, Texas! #SuperTuesdaypic.twitter.com...</td>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>KimKardashian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>A very Happy Birthday to my very wonderful hus...</td>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>KimKardashian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>So pleased &amp; proud to see Mayor REF sworn in t...</td>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>KimKardashian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>I don't remember posting these Jerusalem pix.T...</td>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>HillaryClinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>\"You can go vote early right now. Early voting...</td>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>KimKardashian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text          author  \\\n",
       "0                           New York State of mind URL   KimKardashian   \n",
       "17   A wise woman once told me this...her name migh...   KimKardashian   \n",
       "124  Im #BreakingUpWith my trainer because I only ...   KimKardashian   \n",
       "134  Every time u tweet #BeatCancer EBay & Miller/C...   KimKardashian   \n",
       "146  RT REF Live without pretending. Love without d...   KimKardashian   \n",
       "150  Thank you, Texas! #SuperTuesdaypic.twitter.com...  HillaryClinton   \n",
       "180  A very Happy Birthday to my very wonderful hus...  HillaryClinton   \n",
       "185  So pleased & proud to see Mayor REF sworn in t...  HillaryClinton   \n",
       "194  I don't remember posting these Jerusalem pix.T...   KimKardashian   \n",
       "207  \"You can go vote early right now. Early voting...  HillaryClinton   \n",
       "\n",
       "          predicted  \n",
       "0    HillaryClinton  \n",
       "17   HillaryClinton  \n",
       "124  HillaryClinton  \n",
       "134  HillaryClinton  \n",
       "146  HillaryClinton  \n",
       "150   KimKardashian  \n",
       "180   KimKardashian  \n",
       "185   KimKardashian  \n",
       "194  HillaryClinton  \n",
       "207   KimKardashian  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_predicted=pd.DataFrame(author_predicted,columns=['predicted'])\n",
    "df_wrong_result = pd.concat([twt_test.reset_index(),author_test.reset_index(),author_predicted], axis=1)\n",
    "df_wrong_result.drop('index', axis=1, inplace=True)\n",
    "df_wrong_result.drop('text_pos', axis=1, inplace=True)\n",
    "df_wrong_result=df_wrong_result[df_wrong_result['author']<>df_wrong_result['predicted']]\n",
    "df_wrong_result.head(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
