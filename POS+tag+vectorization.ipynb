{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an advanced example of model evaluation with the addition of POS tag vectorization (based on an other column in the data set), using FeatureUnion in a pipeline and GridSearchCV with cross-validation splitting strategy (10 folds in a (Stratified)KFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use Bernoulli Naive Bayes classifier only because on these data all classifiers provides more or less the same results.\n",
    "See https://github.com/KaterynaD/TweetsAutorshipAttributionModelsEvaluation/blob/master/Autorship%2Battribution%2Bmodels%2Bevaluation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the achieved results point of view: char and stemmed word vectorizeres provides good results. The other combinations (with or without POS tag vectorizer) may or may not provide slightly better or worse results.\n",
    "For most pairs of authors I get 90 - 96% accuracy. I experimented with 500 - 2000 rows rows data sets per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sparsity reasons I pre-process data before analysis:\n",
    "removing re-tweets\n",
    "removing short messages (less then 4 words)\n",
    "replacing @ with REF\n",
    "replacing any url with URL\n",
    "replacing any date with DATE\n",
    "replacing any time with TIME\n",
    "replace digits with NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14044"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data\n",
    "df=pd.read_csv('C:\\Kate\\Python\\Authorship Attribution\\data\\AllTweets.csv')\n",
    "author1='KimKardashian'\n",
    "author2='HillaryClinton'\n",
    "df_kk=df.loc[(df['author'] == author1)]\n",
    "df_hc=df.loc[(df['author'] == author2)]\n",
    "df=df_kk.append(df_hc,ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "#2000 random sample rows for KK\n",
    "rows = random.sample(df_kk.index, 2000)\n",
    "df_kk = df_kk.ix[rows]\n",
    "#2000 random sample rows for HC\n",
    "rows = random.sample(df_hc.index, 2000)\n",
    "df_hc = df_hc.ix[rows]\n",
    "#join back together\n",
    "df=df_kk.append(df_hc,ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3835"
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data pre-processing\n",
    "df.drop(df[df.retweet==True].index, inplace=True)\n",
    "df['num_of_words'] = df[\"text\"].str.split().apply(len)\n",
    "df.drop(df[df.num_of_words<4].index, inplace=True)\n",
    "df[\"text\"].replace(r\"http\\S+\", \"URL\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"@\\S+\", \"REF\", regex=True ,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/.-](\\d{1,2})[/.-](\\d{2,4})+\", \"DATE\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"\\d+\", \"NUM\", regex=True,inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag vectorizer\n",
    "\n",
    "I am going to convert the text column in the data set to the string of POS tag and replace the usual POS tag 2-3 chars abbreviations to 1 char abbrevations (e.g. 'NNP' -> 'N', 'NNPS' -> 'O') in order to use CountVectorizer with 'char' analyzer to get the most informative POS tag combinations per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#POS tag 2-3 chars abbrivation mapping to 1 char abbrevations\n",
    "#http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "pos_code_map={'CC':'A','CD':'B','DT':'C','EX':'D','FW':'E','IN':'F','JJ':'G','JJR':'H','JJS':'I','LS':'J','MD':'K','NN':'L','NNS':'M',\n",
    "'NNP':'N','NNPS':'O','PDT':'P','POS':'Q','PRP':'R','PRP$':'S','RB':'T','RBR':'U','RBS':'V','RP':'W','SYM':'X','TO':'Y','UH':'Z',\n",
    "'VB':'1','VBD':'2','VBG':'3','VBN':'4','VBP':'5','VBZ':'6','WDT':'7','WP':'8','WP$':'9','WRB':'@'}\n",
    "code_pos_map={v: k for k, v in pos_code_map.iteritems()}\n",
    "#Python 3 inv_map = {v: k for k, v in my_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#abbrivation converters\n",
    "def convert(tag):\n",
    "    try:\n",
    "        code=pos_code_map[tag]\n",
    "    except:\n",
    "        code='?'\n",
    "    return code\n",
    "def inv_convert(code):\n",
    "    try:\n",
    "        tag=code_pos_map[code]\n",
    "    except:\n",
    "        tag='?'\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#POS tag converting\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "def pos_tags(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed=tokenizer.tokenize(text)\n",
    "    return \"\".join(convert(tag) for (word, tag) in nltk.pos_tag(text_processed))\n",
    "def text_pos_inv_convert(text):\n",
    "    return \"-\".join(inv_convert(c.upper()) for c in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a new column for pos tags\n",
    "df['text_pos']=df.apply(lambda x: pos_tags(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how a sequence of pos tags looks like to be used in CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>text_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>Watching \"I'm pregnant and I sniff toxic fumes...</td>\n",
       "      <td>3R5GAR5GMFC6GNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>West Coast...Keeping Up With The Kardashians i...</td>\n",
       "      <td>NNNWFCO6FN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>RT REF Another day... Another photo shoot! But...</td>\n",
       "      <td>NNCLCLLACLSMAR53CLFSLF63W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>RT REF Thx love thisREF Cutest moment Between...</td>\n",
       "      <td>NNN1LNLNNANLNLLN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>Excited to share with you an all-new #KUWTK! O...</td>\n",
       "      <td>4YLFRCCGNTBHLY1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                                               text  \\\n",
       "0  KimKardashian  Watching \"I'm pregnant and I sniff toxic fumes...   \n",
       "1  KimKardashian  West Coast...Keeping Up With The Kardashians i...   \n",
       "2  KimKardashian  RT REF Another day... Another photo shoot! But...   \n",
       "3  KimKardashian   RT REF Thx love thisREF Cutest moment Between...   \n",
       "4  KimKardashian  Excited to share with you an all-new #KUWTK! O...   \n",
       "\n",
       "                    text_pos  \n",
       "0            3R5GAR5GMFC6GNG  \n",
       "1                 NNNWFCO6FN  \n",
       "2  NNCLCLLACLSMAR53CLFSLF63W  \n",
       "3           NNN1LNLNNANLNLLN  \n",
       "4            4YLFRCCGNTBHLY1  "
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix[:,['author','text','text_pos']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look if there are unique combinations of POS tags per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "for a in df.author.unique():\n",
    "    v = CountVectorizer(analyzer='char',ngram_range=(3, 3))\n",
    "    ngrams = v.fit_transform(df[df['author'] == a]['text_pos'])\n",
    "    df_t=pd.DataFrame(\n",
    "    {'Feature': v.get_feature_names(),\n",
    "     'Count': list(ngrams.sum(axis=0).flat),\n",
    "     'Author': a\n",
    "    })\n",
    "    #\n",
    "    df_features=df_features.append(df_t,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the 1 char abbrivated pos tag sequence back to the common known 2-3 chars abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features['Feature_POS']=df_features.apply(lambda x: text_pos_inv_convert(x['Feature']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are indeed a lot of unique POS tags combinations per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Count</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4538</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>14</td>\n",
       "      <td>6ga</td>\n",
       "      <td>VBZ-JJ-CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4101</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>13</td>\n",
       "      <td>4f3</td>\n",
       "      <td>VBN-IN-VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5693</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>12</td>\n",
       "      <td>gm7</td>\n",
       "      <td>JJ-NNS-WDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5076</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>12</td>\n",
       "      <td>bmf</td>\n",
       "      <td>CD-NNS-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>12</td>\n",
       "      <td>lgf</td>\n",
       "      <td>NN-JJ-IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author  Count Feature Feature_POS\n",
       "4538  HillaryClinton     14     6ga   VBZ-JJ-CC\n",
       "4101  HillaryClinton     13     4f3  VBN-IN-VBG\n",
       "5693  HillaryClinton     12     gm7  JJ-NNS-WDT\n",
       "5076  HillaryClinton     12     bmf   CD-NNS-IN\n",
       "6087  HillaryClinton     12     lgf    NN-JJ-IN"
      ]
     },
     "execution_count": 844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[~df_features.Feature.isin(df_features[df_features['Author'] != author2].Feature)].sort_values('Count', ascending=False).ix[:,['Author','Count','Feature','Feature_POS']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Count</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>13</td>\n",
       "      <td>ln5</td>\n",
       "      <td>NN-NNP-VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>11</td>\n",
       "      <td>5r2</td>\n",
       "      <td>VBP-PRP-VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>8</td>\n",
       "      <td>nn@</td>\n",
       "      <td>NNP-NNP-WRB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>8</td>\n",
       "      <td>co6</td>\n",
       "      <td>DT-NNPS-VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2710</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>7</td>\n",
       "      <td>o6f</td>\n",
       "      <td>NNPS-VBZ-IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Author  Count Feature  Feature_POS\n",
       "2144  KimKardashian     13     ln5   NN-NNP-VBP\n",
       "779   KimKardashian     11     5r2  VBP-PRP-VBD\n",
       "2624  KimKardashian      8     nn@  NNP-NNP-WRB\n",
       "1412  KimKardashian      8     co6  DT-NNPS-VBZ\n",
       "2710  KimKardashian      7     o6f  NNPS-VBZ-IN"
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[~df_features.Feature.isin(df_features[df_features['Author'] != author1].Feature)].sort_values('Count', ascending=False).ix[:,['Author','Count','Feature','Feature_POS']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting let's hold out a part of the available data as a test set twt_test (X), author_test (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "twt_train, twt_test, author_train, author_test = train_test_split(df.ix[:,['text','text_pos']], df['author'], test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will be used as tokenizer in the evaluation. As I discovered using stop words does not improve the model so I removed item 2 (removing stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def text_process(text):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Tokenizes and removes punctuation\n",
    "    3. Stems\n",
    "    4. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenizing\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed=tokenizer.tokenize(text)\n",
    "    \n",
    "    \n",
    "    # steming\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    \n",
    "    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
    "    \n",
    "\n",
    "    return text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ScoreSummaryByModelParams = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ItemSelector and TextAndTextCodedExtractor classes is used in a pipeline to get a proper column (text or text_pos) from a data set to be used in a vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextAndTextCodedExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the text & text_pos from a tweet in a single pass.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, tweets):\n",
    "        features=tweets.ix[:,['text_pos','text']].to_records(index=False)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModelParamsEvaluation function receives as parameters 2 parts of its pipeline: f_union, which is a pipeline itself with different combinations of vectorizers and a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ModelParamsEvaluation (f_union,model,params,comment):\n",
    "    pipeline = Pipeline([\n",
    "    # Extract the text & text_coded\n",
    "    ('textandtextcoded', TextAndTextCodedExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from text and text_coded\n",
    "    ('union', f_union, ),\n",
    "\n",
    "    # Use a  classifier on the combined features\n",
    "    ('clf', model),\n",
    "    ])\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=params, verbose=1, cv=10)\n",
    "    grid_search.fit(twt_train, author_train)\n",
    "    #best score\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        ScoreSummaryByModelParams.append([comment,grid_search.best_score_,\"\\t%s: %r\" % (param_name, best_parameters[param_name])])    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I examine the model with only 1 vectorizer We do not need FeatureUnion in this case but I use it just to keep the pattern of all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "              # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char')),\n",
    "            ])),               \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'char' analyzer provides a perfect result by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    8.7s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   37.5s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:  1.4min\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  2.5min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  3.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.965\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n",
      "\tunion__char__tfidf__max_df: 0.5\n",
      "\tunion__char__tfidf__max_features: 5000\n",
      "\tunion__char__tfidf__ngram_range: (3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1440 out of 1440 | elapsed:  4.4min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "p = {\n",
    "    'union__char__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__char__tfidf__ngram_range': ((2, 2), (3, 3)), \n",
    "    'union__char__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "            ('selector', ItemSelector(key='text')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='word')),\n",
    "            ])),              \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'word' analyzer is worse for these 2 authors but for other pairs (AdamSavage - ScottKelly) it provides better results the the char analyzer. As you can see it is not recommended using stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 720 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    6.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   26.7s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:  1.0min\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  2.8min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  4.1min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:  5.5min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed:  7.2min\n",
      "[Parallel(n_jobs=1)]: Done 4049 tasks       | elapsed:  9.1min\n",
      "[Parallel(n_jobs=1)]: Done 4999 tasks       | elapsed: 11.2min\n",
      "[Parallel(n_jobs=1)]: Done 6049 tasks       | elapsed: 13.5min\n",
      "[Parallel(n_jobs=1)]: Done 7199 tasks       | elapsed: 16.1min\n",
      "[Parallel(n_jobs=1)]: Done 7200 out of 7200 | elapsed: 16.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.948\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tunion__word__tfidf__max_df: 0.5\n",
      "\tunion__word__tfidf__max_features: None\n",
      "\tunion__word__tfidf__ngram_range: (1, 1)\n",
      "\tunion__word__tfidf__stop_words: None\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__word__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__word__tfidf__ngram_range': ((1, 1),(2, 2), (3, 3),(4,4),(5,5)), \n",
    "    'union__word__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'union__word__tfidf__stop_words': (None, 'english'),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('text', Pipeline([\n",
    "            ('selector', ItemSelector(key='text')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process)),\n",
    "            ])),              \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'stemmed word' analyzer is better then the just 'word' analyzer but still worse then the 'char' for these 2 authors\n",
    "But for other pairs (AdamSavage - ScottKelly) it provides better results the the char analyzer and worse then the just 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 360 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   22.6s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:  1.5min\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:  3.5min\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  6.2min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  9.7min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed: 14.0min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed: 19.0min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed: 24.8min\n",
      "[Parallel(n_jobs=1)]: Done 3600 out of 3600 | elapsed: 27.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.952\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tunion__text__tfidf__max_df: 0.5\n",
      "\tunion__text__tfidf__max_features: None\n",
      "\tunion__text__tfidf__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__text__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__text__tfidf__ngram_range': ((1, 1),(2, 2), (3, 3),(4,4),(5,5)), \n",
    "    'union__text__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, stemmed words, no stop words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling pos tag features  from the text_pos\n",
    "            ('text_pos', Pipeline([\n",
    "            ('selector', ItemSelector(key='text_pos')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='char')),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag vectorizer is not very selective. Let's see how it words in the combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 360 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    5.0s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   20.5s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:   46.2s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  1.4min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  2.1min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  3.1min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:  4.2min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed:  5.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.764\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tunion__text_pos__tfidf__max_df: 0.5\n",
      "\tunion__text_pos__tfidf__max_features: None\n",
      "\tunion__text_pos__tfidf__ngram_range: (3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 3600 out of 3600 | elapsed:  6.2min finished\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__text_pos__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__text_pos__tfidf__ngram_range': ((3, 3), (4, 4),(5,5),(6,6),(7,7)), \n",
    "    'union__text_pos__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>BestScore</th>\n",
       "      <th>BestParameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.965233</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.965233</td>\n",
       "      <td>\\tunion__char__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.965233</td>\n",
       "      <td>\\tunion__char__tfidf__max_features: 5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.965233</td>\n",
       "      <td>\\tunion__char__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.952195</td>\n",
       "      <td>\\tunion__text__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.952195</td>\n",
       "      <td>\\tunion__text__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.952195</td>\n",
       "      <td>\\tunion__text__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.952195</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.948283</td>\n",
       "      <td>\\tunion__word__tfidf__stop_words: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.948283</td>\n",
       "      <td>\\tunion__word__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.948283</td>\n",
       "      <td>\\tunion__word__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.948283</td>\n",
       "      <td>\\tunion__word__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.948283</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.763581</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.763581</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.763581</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.763581</td>\n",
       "      <td>\\tunion__text_pos__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Method  BestScore  \\\n",
       "0                         Bernoulli Naive Bayes, char   0.965233   \n",
       "1                         Bernoulli Naive Bayes, char   0.965233   \n",
       "2                         Bernoulli Naive Bayes, char   0.965233   \n",
       "3                         Bernoulli Naive Bayes, char   0.965233   \n",
       "12  Bernoulli Naive Bayes, stemmed words, no stop ...   0.952195   \n",
       "11  Bernoulli Naive Bayes, stemmed words, no stop ...   0.952195   \n",
       "10  Bernoulli Naive Bayes, stemmed words, no stop ...   0.952195   \n",
       "9   Bernoulli Naive Bayes, stemmed words, no stop ...   0.952195   \n",
       "8                         Bernoulli Naive Bayes, word   0.948283   \n",
       "7                         Bernoulli Naive Bayes, word   0.948283   \n",
       "6                         Bernoulli Naive Bayes, word   0.948283   \n",
       "5                         Bernoulli Naive Bayes, word   0.948283   \n",
       "4                         Bernoulli Naive Bayes, word   0.948283   \n",
       "13                    Bernoulli Naive Bayes, POS tags   0.763581   \n",
       "14                    Bernoulli Naive Bayes, POS tags   0.763581   \n",
       "15                    Bernoulli Naive Bayes, POS tags   0.763581   \n",
       "16                    Bernoulli Naive Bayes, POS tags   0.763581   \n",
       "\n",
       "                                    BestParameter  \n",
       "0                             \\tclf__alpha: 0.001  \n",
       "1               \\tunion__char__tfidf__max_df: 0.5  \n",
       "2        \\tunion__char__tfidf__max_features: 5000  \n",
       "3       \\tunion__char__tfidf__ngram_range: (3, 3)  \n",
       "12      \\tunion__text__tfidf__ngram_range: (1, 1)  \n",
       "11       \\tunion__text__tfidf__max_features: None  \n",
       "10              \\tunion__text__tfidf__max_df: 0.5  \n",
       "9                               \\tclf__alpha: 0.1  \n",
       "8          \\tunion__word__tfidf__stop_words: None  \n",
       "7       \\tunion__word__tfidf__ngram_range: (1, 1)  \n",
       "6        \\tunion__word__tfidf__max_features: None  \n",
       "5               \\tunion__word__tfidf__max_df: 0.5  \n",
       "4                               \\tclf__alpha: 0.1  \n",
       "13                              \\tclf__alpha: 0.1  \n",
       "14          \\tunion__text_pos__tfidf__max_df: 0.5  \n",
       "15   \\tunion__text_pos__tfidf__max_features: None  \n",
       "16  \\tunion__text_pos__tfidf__ngram_range: (3, 3)  "
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\n",
    "df_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByModelParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   13.7s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   16.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.969\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f2_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With small variations 'char and stemmed word' combination provides the best result for most of analyzed authors pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling stememd word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   28.3s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   34.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.973\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f2_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),    \n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   31.5s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   38.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.970\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.0001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + stemmed word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using POS tag vectorizer does not improve the score dramatically. Its impact is more visible only for AdamSavage - ScottKelly pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   30.7s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   37.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.966\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.0001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + stemmed word + POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   16.0s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   19.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.966\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f4_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "             # Pipeline for pulling word features after word_processing from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   35.9s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   43.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.968\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f4_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + stemmed word + POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>BestScore</th>\n",
       "      <th>BestParameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bernoulli Naive Bayes, char + stemmed word</td>\n",
       "      <td>0.972621</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + stemmed word</td>\n",
       "      <td>0.970448</td>\n",
       "      <td>\\tclf__alpha: 0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word</td>\n",
       "      <td>0.968709</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + stemmed w...</td>\n",
       "      <td>0.967840</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + POS tags</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bernoulli Naive Bayes, char + stemmed word + P...</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>\\tclf__alpha: 0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.965233</td>\n",
       "      <td>\\tunion__char__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.965233</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.965233</td>\n",
       "      <td>\\tunion__char__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.965233</td>\n",
       "      <td>\\tunion__char__tfidf__max_features: 5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.952195</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.952195</td>\n",
       "      <td>\\tunion__text__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.952195</td>\n",
       "      <td>\\tunion__text__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.952195</td>\n",
       "      <td>\\tunion__text__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.948283</td>\n",
       "      <td>\\tunion__word__tfidf__stop_words: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.948283</td>\n",
       "      <td>\\tunion__word__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.948283</td>\n",
       "      <td>\\tunion__word__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.948283</td>\n",
       "      <td>\\tunion__word__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.948283</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.763581</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.763581</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.763581</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.763581</td>\n",
       "      <td>\\tunion__text_pos__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Method  BestScore  \\\n",
       "18         Bernoulli Naive Bayes, char + stemmed word   0.972621   \n",
       "19  Bernoulli Naive Bayes, char + word + stemmed word   0.970448   \n",
       "17                 Bernoulli Naive Bayes, char + word   0.968709   \n",
       "22  Bernoulli Naive Bayes, char + word + stemmed w...   0.967840   \n",
       "21      Bernoulli Naive Bayes, char + word + POS tags   0.966102   \n",
       "20  Bernoulli Naive Bayes, char + stemmed word + P...   0.966102   \n",
       "1                         Bernoulli Naive Bayes, char   0.965233   \n",
       "0                         Bernoulli Naive Bayes, char   0.965233   \n",
       "3                         Bernoulli Naive Bayes, char   0.965233   \n",
       "2                         Bernoulli Naive Bayes, char   0.965233   \n",
       "9   Bernoulli Naive Bayes, stemmed words, no stop ...   0.952195   \n",
       "10  Bernoulli Naive Bayes, stemmed words, no stop ...   0.952195   \n",
       "12  Bernoulli Naive Bayes, stemmed words, no stop ...   0.952195   \n",
       "11  Bernoulli Naive Bayes, stemmed words, no stop ...   0.952195   \n",
       "8                         Bernoulli Naive Bayes, word   0.948283   \n",
       "7                         Bernoulli Naive Bayes, word   0.948283   \n",
       "6                         Bernoulli Naive Bayes, word   0.948283   \n",
       "5                         Bernoulli Naive Bayes, word   0.948283   \n",
       "4                         Bernoulli Naive Bayes, word   0.948283   \n",
       "13                    Bernoulli Naive Bayes, POS tags   0.763581   \n",
       "14                    Bernoulli Naive Bayes, POS tags   0.763581   \n",
       "15                    Bernoulli Naive Bayes, POS tags   0.763581   \n",
       "16                    Bernoulli Naive Bayes, POS tags   0.763581   \n",
       "\n",
       "                                    BestParameter  \n",
       "18                            \\tclf__alpha: 0.001  \n",
       "19                           \\tclf__alpha: 0.0001  \n",
       "17                            \\tclf__alpha: 0.001  \n",
       "22                            \\tclf__alpha: 0.001  \n",
       "21                            \\tclf__alpha: 0.001  \n",
       "20                           \\tclf__alpha: 0.0001  \n",
       "1               \\tunion__char__tfidf__max_df: 0.5  \n",
       "0                             \\tclf__alpha: 0.001  \n",
       "3       \\tunion__char__tfidf__ngram_range: (3, 3)  \n",
       "2        \\tunion__char__tfidf__max_features: 5000  \n",
       "9                               \\tclf__alpha: 0.1  \n",
       "10              \\tunion__text__tfidf__max_df: 0.5  \n",
       "12      \\tunion__text__tfidf__ngram_range: (1, 1)  \n",
       "11       \\tunion__text__tfidf__max_features: None  \n",
       "8          \\tunion__word__tfidf__stop_words: None  \n",
       "7       \\tunion__word__tfidf__ngram_range: (1, 1)  \n",
       "6        \\tunion__word__tfidf__max_features: None  \n",
       "5               \\tunion__word__tfidf__max_df: 0.5  \n",
       "4                               \\tclf__alpha: 0.1  \n",
       "13                              \\tclf__alpha: 0.1  \n",
       "14          \\tunion__text_pos__tfidf__max_df: 0.5  \n",
       "15   \\tunion__text_pos__tfidf__max_features: None  \n",
       "16  \\tunion__text_pos__tfidf__ngram_range: (3, 3)  "
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\n",
    "df_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByModelParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run prediction and review the results\n",
    "PredictionEvaluation function combines the scores from several methods for comparizon, ModelRun function runs the prediction for different models and most_informative_feature_for_binary_classification is used to get most informative features from a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc,precision_score, accuracy_score, recall_score, f1_score\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ScoreSummaryByVector = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PredictionEvaluation(author_test_b,author_predictions_b,comment):\n",
    "    Precision=precision_score(author_test_b,author_predictions_b)\n",
    "    print ('Precision: %0.3f' % (Precision))\n",
    "    Accuracy=accuracy_score(author_test_b,author_predictions_b)\n",
    "    print ('Accuracy: %0.3f' % (Accuracy))\n",
    "    Recall=recall_score(author_test_b,author_predictions_b)\n",
    "    print ('Recall: %0.3f' % (Recall))\n",
    "    F1=f1_score(author_test_b,author_predictions_b)\n",
    "    print ('F1: %0.3f' % (F1))\n",
    "    print ('Confussion matrix:')\n",
    "    print (confusion_matrix(author_test_b,author_predictions_b))\n",
    "    ROC_AUC=roc_auc_score(author_test_b,author_predictions_b)\n",
    "    print ('ROC-AUC: %0.3f' % (ROC_AUC))\n",
    "    ScoreSummaryByVector.append([Precision,Accuracy,Recall,F1,ROC_AUC,comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ModelRun (f_union,model):\n",
    "    pipeline = Pipeline([\n",
    "    # Extract the text & text_coded\n",
    "    ('textandtextcoded', TextAndTextCodedExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from text and text_coded\n",
    "    ('union', f_union, ),\n",
    "\n",
    "    # Use a  classifier on the combined features\n",
    "    ('clf', model),\n",
    "    ])\n",
    "    pipeline.fit(twt_train, author_train)\n",
    "    author_predicted = pipeline.predict(twt_test)\n",
    "    \n",
    "    feature_names=list()\n",
    "    for p in (pipeline.get_params()['union'].transformer_list):\n",
    "        fn=(p[0],pipeline.get_params()['union'].get_params()[p[0]].get_params()['tfidf'].get_feature_names())\n",
    "        feature_names.append(fn)\n",
    "    df_fn=pd.DataFrame()\n",
    "    for fn in feature_names:\n",
    "        df_fn= df_fn.append(pd.DataFrame(\n",
    "        {'FeatureType': fn[0],\n",
    "         'Feature': fn[1]\n",
    "        }),\n",
    "        ignore_index=True)    \n",
    "    \n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    lb = LabelBinarizer()\n",
    "    author_test_b = lb.fit_transform(author_test.values)\n",
    "    author_predicted_b  = lb.fit_transform(author_predicted)\n",
    "    return (df_fn,pipeline.get_params()['clf'],author_predicted,author_predicted_b, author_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_informative_feature_for_binary_classification(feature_names, classifier):\n",
    "    class_labels = classifier.classes_\n",
    "\n",
    "    topnvalues_class0 = sorted(zip(classifier.coef_[0], feature_names['Feature'].values, feature_names['FeatureType'].values))\n",
    "    topnvalues_class1 = sorted(zip(classifier.coef_[0], feature_names['Feature'].values, feature_names['FeatureType'].values), reverse=True)\n",
    "\n",
    "    topn_df_class0=pd.DataFrame(topnvalues_class0, columns=['Coef','Feature','FeatureType'])\n",
    "    topn_df_class0['Author']=class_labels[0]\n",
    "    \n",
    "    topn_df_class1=pd.DataFrame(topnvalues_class1, columns=['Coef','Feature','FeatureType'])\n",
    "    topn_df_class1['Author']=class_labels[1]    \n",
    "    \n",
    "    topn_df=topn_df_class0.append(topn_df_class1)\n",
    "    \n",
    "        \n",
    "    return topn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling stememd word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f2_union,BernoulliNB(alpha=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.953\n",
      "Accuracy: 0.960\n",
      "Recall: 0.967\n",
      "F1: 0.960\n",
      "Confussion matrix:\n",
      "[[736  36]\n",
      " [ 25 737]]\n",
      "ROC-AUC: 0.960\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f3_union,BernoulliNB(alpha=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.951\n",
      "Accuracy: 0.958\n",
      "Recall: 0.966\n",
      "F1: 0.958\n",
      "Confussion matrix:\n",
      "[[735  38]\n",
      " [ 26 735]]\n",
      "ROC-AUC: 0.958\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+stemmed word+POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f4_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "             # Pipeline for pulling word features after word_processing from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f4_union,BernoulliNB(alpha=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.952\n",
      "Accuracy: 0.959\n",
      "Recall: 0.966\n",
      "F1: 0.959\n",
      "Confussion matrix:\n",
      "[[735  37]\n",
      " [ 26 736]]\n",
      "ROC-AUC: 0.959\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+word+stemmed word+POS tag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the summary per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.953428</td>\n",
       "      <td>0.960235</td>\n",
       "      <td>0.967192</td>\n",
       "      <td>0.960261</td>\n",
       "      <td>0.960280</td>\n",
       "      <td>char+stemmed word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.952135</td>\n",
       "      <td>0.958931</td>\n",
       "      <td>0.965879</td>\n",
       "      <td>0.958958</td>\n",
       "      <td>0.958976</td>\n",
       "      <td>char+word+stemmed word+POS tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.950841</td>\n",
       "      <td>0.958279</td>\n",
       "      <td>0.965834</td>\n",
       "      <td>0.958279</td>\n",
       "      <td>0.958338</td>\n",
       "      <td>char+stemmed word+POS tag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Accuracy    Recall        F1   ROC-AUC  \\\n",
       "0   0.953428  0.960235  0.967192  0.960261  0.960280   \n",
       "2   0.952135  0.958931  0.965879  0.958958  0.958976   \n",
       "1   0.950841  0.958279  0.965834  0.958279  0.958338   \n",
       "\n",
       "                           Vector  \n",
       "0               char+stemmed word  \n",
       "2  char+word+stemmed word+POS tag  \n",
       "1       char+stemmed word+POS tag  "
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByVector=DataFrame(ScoreSummaryByVector,columns=['Precision','Accuracy','Recall','F1','ROC-AUC','Vector'])\n",
    "df_ScoreSummaryByVector.sort_values(['F1'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's review teh most informative features for the last prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TopFeatures_df=most_informative_feature_for_binary_classification(feature_names, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>CoefChar</th>\n",
       "      <th>Char</th>\n",
       "      <th>CoefWord</th>\n",
       "      <th>Word</th>\n",
       "      <th>CoefText</th>\n",
       "      <th>Text</th>\n",
       "      <th>CoefTextPOS</th>\n",
       "      <th>TextPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>\\nno</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>________</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>________</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>VB-VB-VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>\"c</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>aaron</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>aaron</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>VB-VB-VBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>\"i</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>abandoned</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>abandon</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>VB-VB-WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>\"n</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>abbey</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>abbey</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>VB-VB-WRB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>(v</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>abhorrent</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>abhorr</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>VB-VB-JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>-h</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>abiding</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>abid</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>VB-VB-NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>ah</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>ability</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>abil</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>VB-VB-PDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>ec</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>able</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>abl</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>VB-VB-TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>ef</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>abortion</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>abort</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>VB-VBG-WRB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>io</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>above</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>abov</td>\n",
       "      <td>-16.213406</td>\n",
       "      <td>VB-VBG-JJR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Author   CoefChar  Char   CoefWord       Word   CoefText      Text  \\\n",
       "0  HillaryClinton -16.213406  \\nno -16.213406   ________ -16.213406  ________   \n",
       "1  HillaryClinton -16.213406    \"c -16.213406      aaron -16.213406     aaron   \n",
       "2  HillaryClinton -16.213406    \"i -16.213406  abandoned -16.213406   abandon   \n",
       "3  HillaryClinton -16.213406    \"n -16.213406      abbey -16.213406     abbey   \n",
       "4  HillaryClinton -16.213406    (v -16.213406  abhorrent -16.213406    abhorr   \n",
       "5  HillaryClinton -16.213406    -h -16.213406    abiding -16.213406      abid   \n",
       "6  HillaryClinton -16.213406    ah -16.213406    ability -16.213406      abil   \n",
       "7  HillaryClinton -16.213406    ec -16.213406       able -16.213406       abl   \n",
       "8  HillaryClinton -16.213406    ef -16.213406   abortion -16.213406     abort   \n",
       "9  HillaryClinton -16.213406    io -16.213406      above -16.213406      abov   \n",
       "\n",
       "   CoefTextPOS     TextPOS  \n",
       "0   -16.213406    VB-VB-VB  \n",
       "1   -16.213406   VB-VB-VBN  \n",
       "2   -16.213406    VB-VB-WP  \n",
       "3   -16.213406   VB-VB-WRB  \n",
       "4   -16.213406    VB-VB-JJ  \n",
       "5   -16.213406   VB-VB-NNS  \n",
       "6   -16.213406   VB-VB-PDT  \n",
       "7   -16.213406    VB-VB-TO  \n",
       "8   -16.213406  VB-VBG-WRB  \n",
       "9   -16.213406  VB-VBG-JJR  "
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='char')),['Author','Coef','Feature']].head(10)\n",
    "df1.rename(columns={'Coef':'CoefChar','Feature':'Char'}, inplace=True)\n",
    "df1.reset_index(inplace=True)\n",
    "df2=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='word')),['Coef','Feature']].head(10)\n",
    "df2.rename(columns={'Coef':'CoefWord','Feature':'Word'}, inplace=True)\n",
    "df2.reset_index(inplace=True)\n",
    "df3=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='text')),['Coef','Feature']].head(10)\n",
    "df3.rename(columns={'Coef':'CoefText','Feature':'Text'}, inplace=True)\n",
    "df3.reset_index(inplace=True)\n",
    "df4=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='text_pos')),['Coef','Feature']].head(10)\n",
    "df4.rename(columns={'Coef':'CoefTextPOS','Feature':'TextPOS'}, inplace=True)\n",
    "df4['TextPOS']=df4.apply(lambda x: text_pos_inv_convert(x['TextPOS']), axis=1)\n",
    "df4.reset_index(inplace=True)\n",
    "df_kk_top_features = pd.concat([df1,df2,df3,df4],axis=1)\n",
    "df_kk_top_features.drop('index', axis=1, inplace=True)\n",
    "df_kk_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>CoefChar</th>\n",
       "      <th>Char</th>\n",
       "      <th>CoefWord</th>\n",
       "      <th>Word</th>\n",
       "      <th>CoefText</th>\n",
       "      <th>Text</th>\n",
       "      <th>CoefTextPOS</th>\n",
       "      <th>TextPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-0.902746</td>\n",
       "      <td>ing</td>\n",
       "      <td>-0.984472</td>\n",
       "      <td>url</td>\n",
       "      <td>-0.984472</td>\n",
       "      <td>url</td>\n",
       "      <td>-1.282754</td>\n",
       "      <td>NNP-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-0.972380</td>\n",
       "      <td>url</td>\n",
       "      <td>-1.089562</td>\n",
       "      <td>ref</td>\n",
       "      <td>-1.039486</td>\n",
       "      <td>i</td>\n",
       "      <td>-2.047238</td>\n",
       "      <td>IN-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-0.972380</td>\n",
       "      <td>re</td>\n",
       "      <td>-1.200947</td>\n",
       "      <td>the</td>\n",
       "      <td>-1.089562</td>\n",
       "      <td>ref</td>\n",
       "      <td>-2.120263</td>\n",
       "      <td>NNP-NNP-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-0.989350</td>\n",
       "      <td>the</td>\n",
       "      <td>-1.375444</td>\n",
       "      <td>to</td>\n",
       "      <td>-1.200947</td>\n",
       "      <td>the</td>\n",
       "      <td>-2.120263</td>\n",
       "      <td>NN-NN-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-1.031804</td>\n",
       "      <td>to</td>\n",
       "      <td>-1.510004</td>\n",
       "      <td>my</td>\n",
       "      <td>-1.375444</td>\n",
       "      <td>to</td>\n",
       "      <td>-2.166783</td>\n",
       "      <td>IN-DT-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-1.036919</td>\n",
       "      <td>ng</td>\n",
       "      <td>-1.844010</td>\n",
       "      <td>in</td>\n",
       "      <td>-1.510004</td>\n",
       "      <td>my</td>\n",
       "      <td>-2.174751</td>\n",
       "      <td>NNP-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-1.057645</td>\n",
       "      <td>ur</td>\n",
       "      <td>-1.927891</td>\n",
       "      <td>for</td>\n",
       "      <td>-1.777318</td>\n",
       "      <td>a</td>\n",
       "      <td>-2.207274</td>\n",
       "      <td>NN-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-1.081487</td>\n",
       "      <td>ref</td>\n",
       "      <td>-1.934161</td>\n",
       "      <td>you</td>\n",
       "      <td>-1.844010</td>\n",
       "      <td>in</td>\n",
       "      <td>-2.232380</td>\n",
       "      <td>NN-IN-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-1.125330</td>\n",
       "      <td>ef</td>\n",
       "      <td>-1.966112</td>\n",
       "      <td>is</td>\n",
       "      <td>-1.927891</td>\n",
       "      <td>for</td>\n",
       "      <td>-2.249475</td>\n",
       "      <td>DT-JJ-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>-1.159521</td>\n",
       "      <td>he</td>\n",
       "      <td>-2.005853</td>\n",
       "      <td>on</td>\n",
       "      <td>-1.934161</td>\n",
       "      <td>you</td>\n",
       "      <td>-2.407945</td>\n",
       "      <td>NN-NN-NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Author  CoefChar Char  CoefWord Word  CoefText Text  CoefTextPOS  \\\n",
       "0  KimKardashian -0.902746  ing -0.984472  url -0.984472  url    -1.282754   \n",
       "1  KimKardashian -0.972380  url -1.089562  ref -1.039486    i    -2.047238   \n",
       "2  KimKardashian -0.972380   re -1.200947  the -1.089562  ref    -2.120263   \n",
       "3  KimKardashian -0.989350  the -1.375444   to -1.200947  the    -2.120263   \n",
       "4  KimKardashian -1.031804   to -1.510004   my -1.375444   to    -2.166783   \n",
       "5  KimKardashian -1.036919  ng  -1.844010   in -1.510004   my    -2.174751   \n",
       "6  KimKardashian -1.057645   ur -1.927891  for -1.777318    a    -2.207274   \n",
       "7  KimKardashian -1.081487  ref -1.934161  you -1.844010   in    -2.232380   \n",
       "8  KimKardashian -1.125330  ef  -1.966112   is -1.927891  for    -2.249475   \n",
       "9  KimKardashian -1.159521  he  -2.005853   on -1.934161  you    -2.407945   \n",
       "\n",
       "       TextPOS  \n",
       "0  NNP-NNP-NNP  \n",
       "1   IN-NNP-NNP  \n",
       "2   NNP-NNP-NN  \n",
       "3    NN-NN-NNP  \n",
       "4     IN-DT-NN  \n",
       "5    NNP-NN-NN  \n",
       "6   NN-NNP-NNP  \n",
       "7    NN-IN-NNP  \n",
       "8     DT-JJ-NN  \n",
       "9     NN-NN-NN  "
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='char')),['Author','Coef','Feature']].head(10)\n",
    "df1.rename(columns={'Coef':'CoefChar','Feature':'Char'}, inplace=True)\n",
    "df1.reset_index(inplace=True)\n",
    "df2=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='word')),['Coef','Feature']].head(10)\n",
    "df2.rename(columns={'Coef':'CoefWord','Feature':'Word'}, inplace=True)\n",
    "df2.reset_index(inplace=True)\n",
    "df3=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='text')),['Coef','Feature']].head(10)\n",
    "df3.rename(columns={'Coef':'CoefText','Feature':'Text'}, inplace=True)\n",
    "df3.reset_index(inplace=True)\n",
    "df4=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='text_pos')),['Coef','Feature']].head(10)\n",
    "df4.rename(columns={'Coef':'CoefTextPOS','Feature':'TextPOS'}, inplace=True)\n",
    "df4['TextPOS']=df4.apply(lambda x: text_pos_inv_convert(x['TextPOS']), axis=1)\n",
    "df4.reset_index(inplace=True)\n",
    "df_kk_top_features = pd.concat([df1,df2,df3,df4],axis=1)\n",
    "df_kk_top_features.drop('index', axis=1, inplace=True)\n",
    "df_kk_top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's take a look what was predicted wrongly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't understand why its always easier to gi...</td>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>HillaryClinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Happy #WomensEqualityDay from REF</td>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>KimKardashian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"I trusted her when my life was on the line, a...</td>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>KimKardashian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Late night set vibespic.twitter.com/EtWQUMYPXN</td>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>HillaryClinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Life isn't always about yourself...helping oth...</td>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>HillaryClinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Patiently waiting #Paris pic.twitter.com/bNUMw...</td>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>HillaryClinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Teamed up w/ REF for a AW x REF capsule all sa...</td>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>HillaryClinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>The best Mother's Day gift has been seeing my ...</td>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>KimKardashian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>These pictures of the devastation are just sho...</td>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>HillaryClinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>I’ll be REF in Midtown Crossing on August NUM!...</td>\n",
       "      <td>KimKardashian</td>\n",
       "      <td>HillaryClinton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text          author  \\\n",
       "4    I don't understand why its always easier to gi...   KimKardashian   \n",
       "12                   Happy #WomensEqualityDay from REF  HillaryClinton   \n",
       "20   \"I trusted her when my life was on the line, a...  HillaryClinton   \n",
       "26      Late night set vibespic.twitter.com/EtWQUMYPXN   KimKardashian   \n",
       "31   Life isn't always about yourself...helping oth...   KimKardashian   \n",
       "41   Patiently waiting #Paris pic.twitter.com/bNUMw...   KimKardashian   \n",
       "75   Teamed up w/ REF for a AW x REF capsule all sa...   KimKardashian   \n",
       "98   The best Mother's Day gift has been seeing my ...  HillaryClinton   \n",
       "101  These pictures of the devastation are just sho...   KimKardashian   \n",
       "140  I’ll be REF in Midtown Crossing on August NUM!...   KimKardashian   \n",
       "\n",
       "          predicted  \n",
       "4    HillaryClinton  \n",
       "12    KimKardashian  \n",
       "20    KimKardashian  \n",
       "26   HillaryClinton  \n",
       "31   HillaryClinton  \n",
       "41   HillaryClinton  \n",
       "75   HillaryClinton  \n",
       "98    KimKardashian  \n",
       "101  HillaryClinton  \n",
       "140  HillaryClinton  "
      ]
     },
     "execution_count": 840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_predicted=pd.DataFrame(author_predicted,columns=['predicted'])\n",
    "df_wrong_result = pd.concat([twt_test.reset_index(),author_test.reset_index(),author_predicted], axis=1)\n",
    "df_wrong_result.drop('index', axis=1, inplace=True)\n",
    "df_wrong_result.drop('text_pos', axis=1, inplace=True)\n",
    "df_wrong_result=df_wrong_result[df_wrong_result['author']<>df_wrong_result['predicted']]\n",
    "df_wrong_result.head(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
