{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24112"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data\n",
    "df=pd.read_csv('C:\\Kate\\Python\\Authorship Attribution\\data\\AllTweets.csv')\n",
    "author1='DonaldTrump'\n",
    "author2='BarackObama'\n",
    "df_kk=df.loc[(df['author'] == author1)]\n",
    "df_hc=df.loc[(df['author'] == author2)]\n",
    "df=df_kk.append(df_hc,ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "#2000 random sample rows for KK\n",
    "rows = random.sample(df_kk.index, 2000)\n",
    "df_kk = df_kk.ix[rows]\n",
    "#2000 random sample rows for HC\n",
    "rows = random.sample(df_hc.index, 2000)\n",
    "df_hc = df_hc.ix[rows]\n",
    "#join back together\n",
    "df=df_kk.append(df_hc,ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3506"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data pre-processing\n",
    "df.drop(df[df.retweet==True].index, inplace=True)\n",
    "df['num_of_words'] = df[\"text\"].str.split().apply(len)\n",
    "df.drop(df[df.num_of_words<4].index, inplace=True)\n",
    "df[\"text\"].replace(r\"http\\S+\", \"URL\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"@\\S+\", \"REF\", regex=True ,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/.-](\\d{1,2})[/.-](\\d{2,4})+\", \"DATE\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"\\d+\", \"NUM\", regex=True,inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "pos_code_map={'CC':'A','CD':'B','DT':'C','EX':'D','FW':'E','IN':'F','JJ':'G','JJR':'H','JJS':'I','LS':'J','MD':'K','NN':'L','NNS':'M',\n",
    "'NNP':'N','NNPS':'O','PDT':'P','POS':'Q','PRP':'R','PRP$':'S','RB':'T','RBR':'U','RBS':'V','RP':'W','SYM':'X','TO':'Y','UH':'Z',\n",
    "'VB':'1','VBD':'2','VBG':'3','VBN':'4','VBP':'5','VBZ':'6','WDT':'7','WP':'8','WP$':'9','WRB':'@'}\n",
    "code_pos_map={v: k for k, v in pos_code_map.iteritems()}\n",
    "#Python 3 inv_map = {v: k for k, v in my_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert(tag):\n",
    "    try:\n",
    "        code=pos_code_map[tag]\n",
    "    except:\n",
    "        code='?'\n",
    "    return code\n",
    "def inv_convert(code):\n",
    "    try:\n",
    "        tag=code_pos_map[code]\n",
    "    except:\n",
    "        tag='?'\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "def pos_tags(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed=tokenizer.tokenize(text)\n",
    "    return \"\".join(convert(tag) for (word, tag) in nltk.pos_tag(text_processed))\n",
    "def text_pos_inv_convert(text):\n",
    "    return \"-\".join(inv_convert(c.upper()) for c in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['text_pos']=df.apply(lambda x: pos_tags(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>text_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>Small crowds at REF today in Atlanta. People w...</td>\n",
       "      <td>GLFNLFNN2TGFNCGLAL864RLAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>“Donald Trump dedicates second Scottish golf c...</td>\n",
       "      <td>LNN6GGLLY1LNNFN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>Looking forward to tonight's Ayrshire Chamber ...</td>\n",
       "      <td>3TY1GNNFNNNNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>Failing REF lost all credibility. Not only was...</td>\n",
       "      <td>3N2CLTT2R2NRK14FNFNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>“In every battle there comes a time when both ...</td>\n",
       "      <td>LFCLT6CL@CM5R1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author                                               text  \\\n",
       "2  DonaldTrump  Small crowds at REF today in Atlanta. People w...   \n",
       "3  DonaldTrump  “Donald Trump dedicates second Scottish golf c...   \n",
       "5  DonaldTrump  Looking forward to tonight's Ayrshire Chamber ...   \n",
       "6  DonaldTrump  Failing REF lost all credibility. Not only was...   \n",
       "7  DonaldTrump  “In every battle there comes a time when both ...   \n",
       "\n",
       "                    text_pos  \n",
       "2  GLFNLFNN2TGFNCGLAL864RLAL  \n",
       "3            LNN6GGLLY1LNNFN  \n",
       "5              3TY1GNNFNNNNN  \n",
       "6       3N2CLTT2R2NRK14FNFNN  \n",
       "7             LFCLT6CL@CM5R1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix[:,['author','text','text_pos']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "for a in df.author.unique():\n",
    "    v = CountVectorizer(analyzer='char',ngram_range=(7, 7))\n",
    "    ngrams = v.fit_transform(df[df['author'] == a]['text_pos'])\n",
    "    df_t=pd.DataFrame(\n",
    "    {'Feature': v.get_feature_names(),\n",
    "     'Count': list(ngrams.sum(axis=0).flat),\n",
    "     'Author': a\n",
    "    })\n",
    "    #\n",
    "    df_features=df_features.append(df_t,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features['Feature_POS']=df_features.apply(lambda x: text_pos_inv_convert(x['Feature']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Count</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39305</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>8</td>\n",
       "      <td>r2lfny1</td>\n",
       "      <td>PRP-VBD-NN-IN-NNP-TO-VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34503</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>8</td>\n",
       "      <td>lynllll</td>\n",
       "      <td>NN-TO-NNP-NN-NN-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27116</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>7</td>\n",
       "      <td>fcgllll</td>\n",
       "      <td>IN-DT-JJ-NN-NN-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27364</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>7</td>\n",
       "      <td>fclllll</td>\n",
       "      <td>IN-DT-NN-NN-NN-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25667</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>6</td>\n",
       "      <td>clfclnn</td>\n",
       "      <td>DT-NN-IN-DT-NN-NNP-NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Author  Count  Feature              Feature_POS\n",
       "39305  BarackObama      8  r2lfny1  PRP-VBD-NN-IN-NNP-TO-VB\n",
       "34503  BarackObama      8  lynllll    NN-TO-NNP-NN-NN-NN-NN\n",
       "27116  BarackObama      7  fcgllll     IN-DT-JJ-NN-NN-NN-NN\n",
       "27364  BarackObama      7  fclllll     IN-DT-NN-NN-NN-NN-NN\n",
       "25667  BarackObama      6  clfclnn   DT-NN-IN-DT-NN-NNP-NNP"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[~df_features.Feature.isin(df_features[df_features['Author'] != author2].Feature)].sort_values('Count', ascending=False).ix[:,['Author','Count','Feature','Feature_POS']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Count</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13933</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>7</td>\n",
       "      <td>nfnnfnn</td>\n",
       "      <td>NNP-IN-NNP-NNP-IN-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15526</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>7</td>\n",
       "      <td>nnnnnnf</td>\n",
       "      <td>NNP-NNP-NNP-NNP-NNP-NNP-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15041</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>6</td>\n",
       "      <td>nnfnnfn</td>\n",
       "      <td>NNP-NNP-IN-NNP-NNP-IN-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15370</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>6</td>\n",
       "      <td>nnnfnnf</td>\n",
       "      <td>NNP-NNP-NNP-IN-NNP-NNP-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15411</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>5</td>\n",
       "      <td>nnnlfnn</td>\n",
       "      <td>NNP-NNP-NNP-NN-IN-NNP-NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Author  Count  Feature                 Feature_POS\n",
       "13933  DonaldTrump      7  nfnnfnn   NNP-IN-NNP-NNP-IN-NNP-NNP\n",
       "15526  DonaldTrump      7  nnnnnnf  NNP-NNP-NNP-NNP-NNP-NNP-IN\n",
       "15041  DonaldTrump      6  nnfnnfn   NNP-NNP-IN-NNP-NNP-IN-NNP\n",
       "15370  DonaldTrump      6  nnnfnnf   NNP-NNP-NNP-IN-NNP-NNP-IN\n",
       "15411  DonaldTrump      5  nnnlfnn   NNP-NNP-NNP-NN-IN-NNP-NNP"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[~df_features.Feature.isin(df_features[df_features['Author'] != author1].Feature)].sort_values('Count', ascending=False).ix[:,['Author','Count','Feature','Feature_POS']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "twt_train, twt_test, author_train, author_test = train_test_split(df.ix[:,['text','text_pos']], df['author'], test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def text_process(text):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Tokenizes and removes punctuation\n",
    "    3. Stems\n",
    "    4. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenizing\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed=tokenizer.tokenize(text)\n",
    "    \n",
    "    \n",
    "    # steming\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    \n",
    "    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
    "    \n",
    "\n",
    "    return text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ScoreSummaryByModelParams = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextAndTextCodedExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the text & text_pos from a tweet in a single pass.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, tweets):\n",
    "        features=tweets.ix[:,['text_pos','text']].to_records(index=False)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ModelParamsEvaluation (f_union,model,params,comment):\n",
    "    pipeline = Pipeline([\n",
    "    # Extract the text & text_coded\n",
    "    ('textandtextcoded', TextAndTextCodedExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from text and text_coded\n",
    "    ('union', f_union, ),\n",
    "\n",
    "    # Use a  classifier on the combined features\n",
    "    ('clf', model),\n",
    "    ])\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=params, verbose=1, cv=10)\n",
    "    grid_search.fit(twt_train, author_train)\n",
    "    #best score\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        ScoreSummaryByModelParams.append([comment,grid_search.best_score_,\"\\t%s: %r\" % (param_name, best_parameters[param_name])])    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "              # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char')),\n",
    "            ])),               \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    8.6s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   38.7s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:  1.5min\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  2.5min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  3.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.941\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.01\n",
      "\tunion__char__tfidf__max_df: 0.5\n",
      "\tunion__char__tfidf__max_features: 5000\n",
      "\tunion__char__tfidf__ngram_range: (3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1440 out of 1440 | elapsed:  4.3min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "p = {\n",
    "    'union__char__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__char__tfidf__ngram_range': ((2, 2), (3, 3)), \n",
    "    'union__char__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "            ('selector', ItemSelector(key='text')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='word')),\n",
    "            ])),              \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 720 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    5.8s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   25.7s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:   58.4s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  1.7min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  2.7min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  3.9min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:  5.4min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed:  7.0min\n",
      "[Parallel(n_jobs=1)]: Done 4049 tasks       | elapsed:  8.9min\n",
      "[Parallel(n_jobs=1)]: Done 4999 tasks       | elapsed: 11.0min\n",
      "[Parallel(n_jobs=1)]: Done 6049 tasks       | elapsed: 13.3min\n",
      "[Parallel(n_jobs=1)]: Done 7199 tasks       | elapsed: 15.9min\n",
      "[Parallel(n_jobs=1)]: Done 7200 out of 7200 | elapsed: 15.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.924\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tunion__word__tfidf__max_df: 0.75\n",
      "\tunion__word__tfidf__max_features: None\n",
      "\tunion__word__tfidf__ngram_range: (1, 1)\n",
      "\tunion__word__tfidf__stop_words: None\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__word__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__word__tfidf__ngram_range': ((1, 1),(2, 2), (3, 3),(4,4),(5,5)), \n",
    "    'union__word__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'union__word__tfidf__stop_words': (None, 'english'),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('text', Pipeline([\n",
    "            ('selector', ItemSelector(key='text')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process)),\n",
    "            ])),              \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 360 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   22.7s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:  1.5min\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:  3.5min\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  6.2min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  9.6min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed: 13.9min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed: 19.0min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed: 24.8min\n",
      "[Parallel(n_jobs=1)]: Done 3600 out of 3600 | elapsed: 27.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.924\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1\n",
      "\tunion__text__tfidf__max_df: 0.75\n",
      "\tunion__text__tfidf__max_features: None\n",
      "\tunion__text__tfidf__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__text__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__text__tfidf__ngram_range': ((1, 1),(2, 2), (3, 3),(4,4),(5,5)), \n",
    "    'union__text__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, stemmed words, no stop words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling pos tag features  from the text_pos\n",
    "            ('text_pos', Pipeline([\n",
    "            ('selector', ItemSelector(key='text_pos')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='char')),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 360 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    4.9s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   20.0s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:   44.8s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  1.3min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  2.1min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  3.0min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:  4.1min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed:  5.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.689\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1\n",
      "\tunion__text_pos__tfidf__max_df: 0.5\n",
      "\tunion__text_pos__tfidf__max_features: None\n",
      "\tunion__text_pos__tfidf__ngram_range: (3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 3600 out of 3600 | elapsed:  6.0min finished\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__text_pos__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__text_pos__tfidf__ngram_range': ((3, 3), (4, 4),(5,5),(6,6),(7,7)), \n",
    "    'union__text_pos__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>BestScore</th>\n",
       "      <th>BestParameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.940561</td>\n",
       "      <td>\\tclf__alpha: 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.940561</td>\n",
       "      <td>\\tunion__char__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.940561</td>\n",
       "      <td>\\tunion__char__tfidf__max_features: 5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.940561</td>\n",
       "      <td>\\tunion__char__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.924394</td>\n",
       "      <td>\\tunion__text__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.924394</td>\n",
       "      <td>\\tunion__text__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.924394</td>\n",
       "      <td>\\tunion__text__tfidf__max_df: 0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.924394</td>\n",
       "      <td>\\tclf__alpha: 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.923918</td>\n",
       "      <td>\\tunion__word__tfidf__stop_words: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.923918</td>\n",
       "      <td>\\tunion__word__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.923918</td>\n",
       "      <td>\\tunion__word__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.923918</td>\n",
       "      <td>\\tunion__word__tfidf__max_df: 0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.923918</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.688540</td>\n",
       "      <td>\\tclf__alpha: 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.688540</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.688540</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.688540</td>\n",
       "      <td>\\tunion__text_pos__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Method  BestScore  \\\n",
       "0                         Bernoulli Naive Bayes, char   0.940561   \n",
       "1                         Bernoulli Naive Bayes, char   0.940561   \n",
       "2                         Bernoulli Naive Bayes, char   0.940561   \n",
       "3                         Bernoulli Naive Bayes, char   0.940561   \n",
       "12  Bernoulli Naive Bayes, stemmed words, no stop ...   0.924394   \n",
       "11  Bernoulli Naive Bayes, stemmed words, no stop ...   0.924394   \n",
       "10  Bernoulli Naive Bayes, stemmed words, no stop ...   0.924394   \n",
       "9   Bernoulli Naive Bayes, stemmed words, no stop ...   0.924394   \n",
       "8                         Bernoulli Naive Bayes, word   0.923918   \n",
       "7                         Bernoulli Naive Bayes, word   0.923918   \n",
       "6                         Bernoulli Naive Bayes, word   0.923918   \n",
       "5                         Bernoulli Naive Bayes, word   0.923918   \n",
       "4                         Bernoulli Naive Bayes, word   0.923918   \n",
       "13                    Bernoulli Naive Bayes, POS tags   0.688540   \n",
       "14                    Bernoulli Naive Bayes, POS tags   0.688540   \n",
       "15                    Bernoulli Naive Bayes, POS tags   0.688540   \n",
       "16                    Bernoulli Naive Bayes, POS tags   0.688540   \n",
       "\n",
       "                                    BestParameter  \n",
       "0                              \\tclf__alpha: 0.01  \n",
       "1               \\tunion__char__tfidf__max_df: 0.5  \n",
       "2        \\tunion__char__tfidf__max_features: 5000  \n",
       "3       \\tunion__char__tfidf__ngram_range: (3, 3)  \n",
       "12      \\tunion__text__tfidf__ngram_range: (1, 1)  \n",
       "11       \\tunion__text__tfidf__max_features: None  \n",
       "10             \\tunion__text__tfidf__max_df: 0.75  \n",
       "9                                 \\tclf__alpha: 1  \n",
       "8          \\tunion__word__tfidf__stop_words: None  \n",
       "7       \\tunion__word__tfidf__ngram_range: (1, 1)  \n",
       "6        \\tunion__word__tfidf__max_features: None  \n",
       "5              \\tunion__word__tfidf__max_df: 0.75  \n",
       "4                               \\tclf__alpha: 0.1  \n",
       "13                                \\tclf__alpha: 1  \n",
       "14          \\tunion__text_pos__tfidf__max_df: 0.5  \n",
       "15   \\tunion__text_pos__tfidf__max_features: None  \n",
       "16  \\tunion__text_pos__tfidf__ngram_range: (3, 3)  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\n",
    "df_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByModelParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   13.3s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   16.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.947\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.01\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f2_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling stememd word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   28.3s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   34.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.951\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f2_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),    \n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   31.4s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   38.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.950\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   30.8s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   37.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.943\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + stemmed word + POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   16.0s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   19.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.944\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f4_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "             # Pipeline for pulling word features after word_processing from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   34.0s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   41.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.947\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.01\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f4_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + stemmed word + POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>BestScore</th>\n",
       "      <th>BestParameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bernoulli Naive Bayes, char + stemmed word</td>\n",
       "      <td>0.951022</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + stemmed word</td>\n",
       "      <td>0.949596</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + stemmed w...</td>\n",
       "      <td>0.946743</td>\n",
       "      <td>\\tclf__alpha: 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word</td>\n",
       "      <td>0.946743</td>\n",
       "      <td>\\tclf__alpha: 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + POS tags</td>\n",
       "      <td>0.943890</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bernoulli Naive Bayes, char + stemmed word + P...</td>\n",
       "      <td>0.943414</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.940561</td>\n",
       "      <td>\\tunion__char__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.940561</td>\n",
       "      <td>\\tclf__alpha: 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.940561</td>\n",
       "      <td>\\tunion__char__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.940561</td>\n",
       "      <td>\\tunion__char__tfidf__max_features: 5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.924394</td>\n",
       "      <td>\\tclf__alpha: 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.924394</td>\n",
       "      <td>\\tunion__text__tfidf__max_df: 0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.924394</td>\n",
       "      <td>\\tunion__text__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.924394</td>\n",
       "      <td>\\tunion__text__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.923918</td>\n",
       "      <td>\\tunion__word__tfidf__stop_words: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.923918</td>\n",
       "      <td>\\tunion__word__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.923918</td>\n",
       "      <td>\\tunion__word__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.923918</td>\n",
       "      <td>\\tunion__word__tfidf__max_df: 0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.923918</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.688540</td>\n",
       "      <td>\\tclf__alpha: 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.688540</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.688540</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.688540</td>\n",
       "      <td>\\tunion__text_pos__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Method  BestScore  \\\n",
       "18         Bernoulli Naive Bayes, char + stemmed word   0.951022   \n",
       "19  Bernoulli Naive Bayes, char + word + stemmed word   0.949596   \n",
       "22  Bernoulli Naive Bayes, char + word + stemmed w...   0.946743   \n",
       "17                 Bernoulli Naive Bayes, char + word   0.946743   \n",
       "21      Bernoulli Naive Bayes, char + word + POS tags   0.943890   \n",
       "20  Bernoulli Naive Bayes, char + stemmed word + P...   0.943414   \n",
       "1                         Bernoulli Naive Bayes, char   0.940561   \n",
       "0                         Bernoulli Naive Bayes, char   0.940561   \n",
       "3                         Bernoulli Naive Bayes, char   0.940561   \n",
       "2                         Bernoulli Naive Bayes, char   0.940561   \n",
       "9   Bernoulli Naive Bayes, stemmed words, no stop ...   0.924394   \n",
       "10  Bernoulli Naive Bayes, stemmed words, no stop ...   0.924394   \n",
       "12  Bernoulli Naive Bayes, stemmed words, no stop ...   0.924394   \n",
       "11  Bernoulli Naive Bayes, stemmed words, no stop ...   0.924394   \n",
       "8                         Bernoulli Naive Bayes, word   0.923918   \n",
       "7                         Bernoulli Naive Bayes, word   0.923918   \n",
       "6                         Bernoulli Naive Bayes, word   0.923918   \n",
       "5                         Bernoulli Naive Bayes, word   0.923918   \n",
       "4                         Bernoulli Naive Bayes, word   0.923918   \n",
       "13                    Bernoulli Naive Bayes, POS tags   0.688540   \n",
       "14                    Bernoulli Naive Bayes, POS tags   0.688540   \n",
       "15                    Bernoulli Naive Bayes, POS tags   0.688540   \n",
       "16                    Bernoulli Naive Bayes, POS tags   0.688540   \n",
       "\n",
       "                                    BestParameter  \n",
       "18                            \\tclf__alpha: 0.001  \n",
       "19                            \\tclf__alpha: 0.001  \n",
       "22                             \\tclf__alpha: 0.01  \n",
       "17                             \\tclf__alpha: 0.01  \n",
       "21                              \\tclf__alpha: 0.1  \n",
       "20                              \\tclf__alpha: 0.1  \n",
       "1               \\tunion__char__tfidf__max_df: 0.5  \n",
       "0                              \\tclf__alpha: 0.01  \n",
       "3       \\tunion__char__tfidf__ngram_range: (3, 3)  \n",
       "2        \\tunion__char__tfidf__max_features: 5000  \n",
       "9                                 \\tclf__alpha: 1  \n",
       "10             \\tunion__text__tfidf__max_df: 0.75  \n",
       "12      \\tunion__text__tfidf__ngram_range: (1, 1)  \n",
       "11       \\tunion__text__tfidf__max_features: None  \n",
       "8          \\tunion__word__tfidf__stop_words: None  \n",
       "7       \\tunion__word__tfidf__ngram_range: (1, 1)  \n",
       "6        \\tunion__word__tfidf__max_features: None  \n",
       "5              \\tunion__word__tfidf__max_df: 0.75  \n",
       "4                               \\tclf__alpha: 0.1  \n",
       "13                                \\tclf__alpha: 1  \n",
       "14          \\tunion__text_pos__tfidf__max_df: 0.5  \n",
       "15   \\tunion__text_pos__tfidf__max_features: None  \n",
       "16  \\tunion__text_pos__tfidf__ngram_range: (3, 3)  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\n",
    "df_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByModelParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc,precision_score, accuracy_score, recall_score, f1_score\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ScoreSummaryByVector = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PredictionEvaluation(author_test_b,author_predictions_b,comment):\n",
    "    Precision=precision_score(author_test_b,author_predictions_b)\n",
    "    print ('Precision: %0.3f' % (Precision))\n",
    "    Accuracy=accuracy_score(author_test_b,author_predictions_b)\n",
    "    print ('Accuracy: %0.3f' % (Accuracy))\n",
    "    Recall=recall_score(author_test_b,author_predictions_b)\n",
    "    print ('Recall: %0.3f' % (Recall))\n",
    "    F1=f1_score(author_test_b,author_predictions_b)\n",
    "    print ('F1: %0.3f' % (F1))\n",
    "    print ('Confussion matrix:')\n",
    "    print (confusion_matrix(author_test_b,author_predictions_b))\n",
    "    ROC_AUC=roc_auc_score(author_test_b,author_predictions_b)\n",
    "    print ('ROC-AUC: %0.3f' % (ROC_AUC))\n",
    "    ScoreSummaryByVector.append([Precision,Accuracy,Recall,F1,ROC_AUC,comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ModelRun (f_union,model):\n",
    "    pipeline = Pipeline([\n",
    "    # Extract the text & text_coded\n",
    "    ('textandtextcoded', TextAndTextCodedExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from text and text_coded\n",
    "    ('union', f_union, ),\n",
    "\n",
    "    # Use a  classifier on the combined features\n",
    "    ('clf', model),\n",
    "    ])\n",
    "    pipeline.fit(twt_train, author_train)\n",
    "    author_predicted = pipeline.predict(twt_test)\n",
    "    \n",
    "    feature_names=list()\n",
    "    for p in (pipeline.get_params()['union'].transformer_list):\n",
    "        fn=(p[0],pipeline.get_params()['union'].get_params()[p[0]].get_params()['tfidf'].get_feature_names())\n",
    "        feature_names.append(fn)\n",
    "    df_fn=pd.DataFrame()\n",
    "    for fn in feature_names:\n",
    "        df_fn= df_fn.append(pd.DataFrame(\n",
    "        {'FeatureType': fn[0],\n",
    "         'Feature': fn[1]\n",
    "        }),\n",
    "        ignore_index=True)    \n",
    "    \n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    lb = LabelBinarizer()\n",
    "    author_test_b = lb.fit_transform(author_test.values)\n",
    "    author_predicted_b  = lb.fit_transform(author_predicted)\n",
    "    return (df_fn,pipeline.get_params()['clf'],author_predicted,author_predicted_b, author_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_informative_feature_for_binary_classification(feature_names, classifier):\n",
    "    class_labels = classifier.classes_\n",
    "\n",
    "    topnvalues_class0 = sorted(zip(classifier.coef_[0], feature_names['Feature'].values, feature_names['FeatureType'].values))\n",
    "    topnvalues_class1 = sorted(zip(classifier.coef_[0], feature_names['Feature'].values, feature_names['FeatureType'].values), reverse=True)\n",
    "\n",
    "    topn_df_class0=pd.DataFrame(topnvalues_class0, columns=['Coef','Feature','FeatureType'])\n",
    "    topn_df_class0['Author']=class_labels[0]\n",
    "    \n",
    "    topn_df_class1=pd.DataFrame(topnvalues_class1, columns=['Coef','Feature','FeatureType'])\n",
    "    topn_df_class1['Author']=class_labels[1]    \n",
    "    \n",
    "    topn_df=topn_df_class0.append(topn_df_class1)\n",
    "    \n",
    "        \n",
    "    return topn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "            # Pipeline for pulling stememd word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.75,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f2_union,BernoulliNB(alpha=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.943\n",
      "Accuracy: 0.931\n",
      "Recall: 0.907\n",
      "F1: 0.925\n",
      "Confussion matrix:\n",
      "[[710  36]\n",
      " [ 61 596]]\n",
      "ROC-AUC: 0.929\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.75,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f3_union,BernoulliNB(alpha=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.922\n",
      "Accuracy: 0.923\n",
      "Recall: 0.908\n",
      "F1: 0.915\n",
      "Confussion matrix:\n",
      "[[712  49]\n",
      " [ 59 583]]\n",
      "ROC-AUC: 0.922\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+stemmed word+POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f4_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "             # Pipeline for pulling word features after word_processing from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f4_union,BernoulliNB(alpha=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.915\n",
      "Accuracy: 0.924\n",
      "Recall: 0.916\n",
      "F1: 0.915\n",
      "Confussion matrix:\n",
      "[[718  54]\n",
      " [ 53 578]]\n",
      "ROC-AUC: 0.923\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+word+stemmed word+POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.943038</td>\n",
       "      <td>0.930862</td>\n",
       "      <td>0.907154</td>\n",
       "      <td>0.924748</td>\n",
       "      <td>0.929448</td>\n",
       "      <td>char+stemmed word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.914557</td>\n",
       "      <td>0.923735</td>\n",
       "      <td>0.916006</td>\n",
       "      <td>0.915281</td>\n",
       "      <td>0.923029</td>\n",
       "      <td>char+word+stemmed word+POS tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.922468</td>\n",
       "      <td>0.923022</td>\n",
       "      <td>0.908100</td>\n",
       "      <td>0.915228</td>\n",
       "      <td>0.921855</td>\n",
       "      <td>char+stemmed word+POS tag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Accuracy    Recall        F1   ROC-AUC  \\\n",
       "0   0.943038  0.930862  0.907154  0.924748  0.929448   \n",
       "2   0.914557  0.923735  0.916006  0.915281  0.923029   \n",
       "1   0.922468  0.923022  0.908100  0.915228  0.921855   \n",
       "\n",
       "                           Vector  \n",
       "0               char+stemmed word  \n",
       "2  char+word+stemmed word+POS tag  \n",
       "1       char+stemmed word+POS tag  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByVector=DataFrame(ScoreSummaryByVector,columns=['Precision','Accuracy','Recall','F1','ROC-AUC','Vector'])\n",
    "df_ScoreSummaryByVector.sort_values(['F1'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TopFeatures_df=most_informative_feature_for_binary_classification(feature_names, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>CoefChar</th>\n",
       "      <th>Char</th>\n",
       "      <th>CoefWord</th>\n",
       "      <th>Word</th>\n",
       "      <th>CoefText</th>\n",
       "      <th>Text</th>\n",
       "      <th>CoefTextPOS</th>\n",
       "      <th>TextPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>\"p</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>aanumqtlnumonum</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>aanumqtlnumonum</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>VB-VB-VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>#b</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>abetterbargain</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>abetterbargain</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>VB-VB-WDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>#e</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>abetterbargainurl</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>abetterbargainurl</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>VB-VB-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>#h</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>ability</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>abil</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>VB-VB-RP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>#j</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>abroad</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>abroad</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>VB-VBD-DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>#p</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>absence</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>absenc</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>VB-VBG-VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>#u</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>abuse</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>abynumwkxj</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>VB-VBG-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>(v</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>abynumwkxj</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>aca</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>VB-VBG-NNPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>-b</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>aca</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>academi</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>VB-VBG-PRP$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BarackObama</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>ct</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>academy</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>acawork</td>\n",
       "      <td>-16.023785</td>\n",
       "      <td>VB-VBG-RB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Author   CoefChar Char   CoefWord               Word   CoefText  \\\n",
       "0  BarackObama -16.023785   \"p -16.023785    aanumqtlnumonum -16.023785   \n",
       "1  BarackObama -16.023785   #b -16.023785     abetterbargain -16.023785   \n",
       "2  BarackObama -16.023785   #e -16.023785  abetterbargainurl -16.023785   \n",
       "3  BarackObama -16.023785   #h -16.023785            ability -16.023785   \n",
       "4  BarackObama -16.023785   #j -16.023785             abroad -16.023785   \n",
       "5  BarackObama -16.023785   #p -16.023785            absence -16.023785   \n",
       "6  BarackObama -16.023785   #u -16.023785              abuse -16.023785   \n",
       "7  BarackObama -16.023785   (v -16.023785         abynumwkxj -16.023785   \n",
       "8  BarackObama -16.023785   -b -16.023785                aca -16.023785   \n",
       "9  BarackObama -16.023785   ct -16.023785            academy -16.023785   \n",
       "\n",
       "                Text  CoefTextPOS      TextPOS  \n",
       "0    aanumqtlnumonum   -16.023785     VB-VB-VB  \n",
       "1     abetterbargain   -16.023785    VB-VB-WDT  \n",
       "2  abetterbargainurl   -16.023785     VB-VB-IN  \n",
       "3               abil   -16.023785     VB-VB-RP  \n",
       "4             abroad   -16.023785    VB-VBD-DT  \n",
       "5             absenc   -16.023785   VB-VBG-VBZ  \n",
       "6         abynumwkxj   -16.023785    VB-VBG-NN  \n",
       "7                aca   -16.023785  VB-VBG-NNPS  \n",
       "8            academi   -16.023785  VB-VBG-PRP$  \n",
       "9            acawork   -16.023785    VB-VBG-RB  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='char')),['Author','Coef','Feature']].head(10)\n",
    "df1.rename(columns={'Coef':'CoefChar','Feature':'Char'}, inplace=True)\n",
    "df1.reset_index(inplace=True)\n",
    "df2=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='word')),['Coef','Feature']].head(10)\n",
    "df2.rename(columns={'Coef':'CoefWord','Feature':'Word'}, inplace=True)\n",
    "df2.reset_index(inplace=True)\n",
    "df3=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='text')),['Coef','Feature']].head(10)\n",
    "df3.rename(columns={'Coef':'CoefText','Feature':'Text'}, inplace=True)\n",
    "df3.reset_index(inplace=True)\n",
    "df4=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='text_pos')),['Coef','Feature']].head(10)\n",
    "df4.rename(columns={'Coef':'CoefTextPOS','Feature':'TextPOS'}, inplace=True)\n",
    "df4['TextPOS']=df4.apply(lambda x: text_pos_inv_convert(x['TextPOS']), axis=1)\n",
    "df4.reset_index(inplace=True)\n",
    "df_kk_top_features = pd.concat([df1,df2,df3,df4],axis=1)\n",
    "df_kk_top_features.drop('index', axis=1, inplace=True)\n",
    "df_kk_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>CoefChar</th>\n",
       "      <th>Char</th>\n",
       "      <th>CoefWord</th>\n",
       "      <th>Word</th>\n",
       "      <th>CoefText</th>\n",
       "      <th>Text</th>\n",
       "      <th>CoefTextPOS</th>\n",
       "      <th>TextPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>-0.684394</td>\n",
       "      <td>ing</td>\n",
       "      <td>-0.987444</td>\n",
       "      <td>to</td>\n",
       "      <td>-0.987444</td>\n",
       "      <td>to</td>\n",
       "      <td>-1.295992</td>\n",
       "      <td>NNP-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>-0.754321</td>\n",
       "      <td>ng</td>\n",
       "      <td>-0.993362</td>\n",
       "      <td>ref</td>\n",
       "      <td>-0.993362</td>\n",
       "      <td>ref</td>\n",
       "      <td>-1.582336</td>\n",
       "      <td>IN-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>-0.785166</td>\n",
       "      <td>re</td>\n",
       "      <td>-1.189427</td>\n",
       "      <td>is</td>\n",
       "      <td>-1.167998</td>\n",
       "      <td>a</td>\n",
       "      <td>-1.671781</td>\n",
       "      <td>DT-JJ-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>-0.955511</td>\n",
       "      <td>to</td>\n",
       "      <td>-1.384099</td>\n",
       "      <td>and</td>\n",
       "      <td>-1.189427</td>\n",
       "      <td>is</td>\n",
       "      <td>-1.908169</td>\n",
       "      <td>NN-IN-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>-0.955511</td>\n",
       "      <td>ref</td>\n",
       "      <td>-1.419817</td>\n",
       "      <td>of</td>\n",
       "      <td>-1.384099</td>\n",
       "      <td>and</td>\n",
       "      <td>-1.923095</td>\n",
       "      <td>IN-DT-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>-0.996333</td>\n",
       "      <td>is</td>\n",
       "      <td>-1.555949</td>\n",
       "      <td>in</td>\n",
       "      <td>-1.419817</td>\n",
       "      <td>of</td>\n",
       "      <td>-2.068512</td>\n",
       "      <td>DT-NN-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>-1.076872</td>\n",
       "      <td>ef</td>\n",
       "      <td>-1.648658</td>\n",
       "      <td>url</td>\n",
       "      <td>-1.555949</td>\n",
       "      <td>in</td>\n",
       "      <td>-2.131313</td>\n",
       "      <td>NNP-NNP-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>-1.080103</td>\n",
       "      <td>an</td>\n",
       "      <td>-1.719694</td>\n",
       "      <td>num</td>\n",
       "      <td>-1.582336</td>\n",
       "      <td>i</td>\n",
       "      <td>-2.188471</td>\n",
       "      <td>PRP-MD-VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>-1.113001</td>\n",
       "      <td>at</td>\n",
       "      <td>-1.744540</td>\n",
       "      <td>on</td>\n",
       "      <td>-1.648658</td>\n",
       "      <td>url</td>\n",
       "      <td>-2.208274</td>\n",
       "      <td>NNP-IN-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>-1.129865</td>\n",
       "      <td>be</td>\n",
       "      <td>-1.829838</td>\n",
       "      <td>for</td>\n",
       "      <td>-1.683546</td>\n",
       "      <td>be</td>\n",
       "      <td>-2.280844</td>\n",
       "      <td>JJ-NN-IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Author  CoefChar Char  CoefWord Word  CoefText Text  CoefTextPOS  \\\n",
       "0  DonaldTrump -0.684394  ing -0.987444   to -0.987444   to    -1.295992   \n",
       "1  DonaldTrump -0.754321  ng  -0.993362  ref -0.993362  ref    -1.582336   \n",
       "2  DonaldTrump -0.785166   re -1.189427   is -1.167998    a    -1.671781   \n",
       "3  DonaldTrump -0.955511  to  -1.384099  and -1.189427   is    -1.908169   \n",
       "4  DonaldTrump -0.955511  ref -1.419817   of -1.384099  and    -1.923095   \n",
       "5  DonaldTrump -0.996333  is  -1.555949   in -1.419817   of    -2.068512   \n",
       "6  DonaldTrump -1.076872  ef  -1.648658  url -1.555949   in    -2.131313   \n",
       "7  DonaldTrump -1.080103   an -1.719694  num -1.582336    i    -2.188471   \n",
       "8  DonaldTrump -1.113001  at  -1.744540   on -1.648658  url    -2.208274   \n",
       "9  DonaldTrump -1.129865   be -1.829838  for -1.683546   be    -2.280844   \n",
       "\n",
       "       TextPOS  \n",
       "0  NNP-NNP-NNP  \n",
       "1   IN-NNP-NNP  \n",
       "2     DT-JJ-NN  \n",
       "3    NN-IN-NNP  \n",
       "4     IN-DT-NN  \n",
       "5     DT-NN-IN  \n",
       "6   NNP-NNP-IN  \n",
       "7    PRP-MD-VB  \n",
       "8   NNP-IN-NNP  \n",
       "9     JJ-NN-IN  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='char')),['Author','Coef','Feature']].head(10)\n",
    "df1.rename(columns={'Coef':'CoefChar','Feature':'Char'}, inplace=True)\n",
    "df1.reset_index(inplace=True)\n",
    "df2=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='word')),['Coef','Feature']].head(10)\n",
    "df2.rename(columns={'Coef':'CoefWord','Feature':'Word'}, inplace=True)\n",
    "df2.reset_index(inplace=True)\n",
    "df3=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='text')),['Coef','Feature']].head(10)\n",
    "df3.rename(columns={'Coef':'CoefText','Feature':'Text'}, inplace=True)\n",
    "df3.reset_index(inplace=True)\n",
    "df4=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='text_pos')),['Coef','Feature']].head(10)\n",
    "df4.rename(columns={'Coef':'CoefTextPOS','Feature':'TextPOS'}, inplace=True)\n",
    "df4['TextPOS']=df4.apply(lambda x: text_pos_inv_convert(x['TextPOS']), axis=1)\n",
    "df4.reset_index(inplace=True)\n",
    "df_kk_top_features = pd.concat([df1,df2,df3,df4],axis=1)\n",
    "df_kk_top_features.drop('index', axis=1, inplace=True)\n",
    "df_kk_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>California Republican reps on Twitter: REF (CA...</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>DonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Our trade deficit just jumped in May to “the s...</td>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Thank you for your support at this mornings To...</td>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Alison Grimes supports harsh restrictions to k...</td>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>In any business venture, remember that brandin...</td>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>House GOP wants to cut Medicare, Obama took $N...</td>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>We should expand oil production in America eve...</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>DonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>I believe in #AmericaFirst and that means FAMI...</td>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Ron Paul is right that we are wasting trillion...</td>\n",
       "      <td>DonaldTrump</td>\n",
       "      <td>BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Preparing for our big \"Walk for Change\" nation...</td>\n",
       "      <td>BarackObama</td>\n",
       "      <td>DonaldTrump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text       author  \\\n",
       "15   California Republican reps on Twitter: REF (CA...  BarackObama   \n",
       "17   Our trade deficit just jumped in May to “the s...  DonaldTrump   \n",
       "31   Thank you for your support at this mornings To...  DonaldTrump   \n",
       "42   Alison Grimes supports harsh restrictions to k...  DonaldTrump   \n",
       "97   In any business venture, remember that brandin...  DonaldTrump   \n",
       "116  House GOP wants to cut Medicare, Obama took $N...  DonaldTrump   \n",
       "127  We should expand oil production in America eve...  BarackObama   \n",
       "139  I believe in #AmericaFirst and that means FAMI...  DonaldTrump   \n",
       "140  Ron Paul is right that we are wasting trillion...  DonaldTrump   \n",
       "144  Preparing for our big \"Walk for Change\" nation...  BarackObama   \n",
       "\n",
       "       predicted  \n",
       "15   DonaldTrump  \n",
       "17   BarackObama  \n",
       "31   BarackObama  \n",
       "42   BarackObama  \n",
       "97   BarackObama  \n",
       "116  BarackObama  \n",
       "127  DonaldTrump  \n",
       "139  BarackObama  \n",
       "140  BarackObama  \n",
       "144  DonaldTrump  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_predicted=pd.DataFrame(author_predicted,columns=['predicted'])\n",
    "df_wrong_result = pd.concat([twt_test.reset_index(),author_test.reset_index(),author_predicted], axis=1)\n",
    "df_wrong_result.drop('index', axis=1, inplace=True)\n",
    "df_wrong_result.drop('text_pos', axis=1, inplace=True)\n",
    "df_wrong_result=df_wrong_result[df_wrong_result['author']<>df_wrong_result['predicted']]\n",
    "df_wrong_result.head(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
