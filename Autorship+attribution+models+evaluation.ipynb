{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I work on the question  whether the author of a tweet (very short text) can be successfully identified.\n",
    "I try to choose the best classification method its parameters set and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose binary classification problem for the start and use Kim Kardashian and Hillary Clinton tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data\n",
    "df=pd.read_csv('/data/AllTweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10688"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kk=df.loc[(df['author'] == 'KimKardashian')]\n",
    "len(df_kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3356"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hc=df.loc[(df['author'] == 'HillaryClinton')]\n",
    "len(df_hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's choose the same random number of tweets from both authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "#2000 random sample rows for KK\n",
    "rows = random.sample(df_kk.index, 2000)\n",
    "df_kk = df_kk.ix[rows]\n",
    "#2000 random sample rows for HC\n",
    "rows = random.sample(df_hc.index, 2000)\n",
    "df_hc = df_hc.ix[rows]\n",
    "#join back together\n",
    "df=df_kk.append(df_hc,ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sparsity reasons I pre-process data before analysis:\n",
    "1. removing re-tweets\n",
    "2. removing short messages (less then 4 words)\n",
    "3. replacing @ with REF\n",
    "4. replacing any url with URL\n",
    "5. replacing any date with DATE\n",
    "6. replacing any time with TIME\n",
    "7. replace digits with NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3817"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data pre-processing\n",
    "df.drop(df[df.retweet==True].index, inplace=True)\n",
    "df['num_of_words'] = df[\"text\"].str.split().apply(len)\n",
    "df.drop(df[df.num_of_words<4].index, inplace=True)\n",
    "df[\"text\"].replace(r\"http\\S+\", \"URL\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"@\\S+\", \"REF\", regex=True ,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/.-](\\d{1,2})[/.-](\\d{2,4})+\", \"DATE\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"\\d+\", \"NUM\", regex=True,inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting let's hold out a part of the available data as a test set twt_test (X), author_test (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "twt_train, twt_test, author_train, author_test = train_test_split(df['text'], df['author'], test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train set length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2290"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twt_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1527"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The place to start is to get better results from known classification method that perform well on the problem.\n",
    "I test several algorithms using its default parameters as well as CountVectorizer() and TfidfTransformer() with default parameters\n",
    "I also apply ten-fold cross validation on the training set to select the best method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ScoreSummaryByModel = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def ModelEvaluation (model,comment):\n",
    "    pipeline = Pipeline([('vect', CountVectorizer())\n",
    "                  , ('tfidf', TfidfTransformer())\n",
    "                  , ('model', model)])\n",
    "    scores = cross_val_score(pipeline, df['text'], df['author'], cv=10)\t\n",
    "    mean = scores.mean()\t\n",
    "    std = scores.std()\t\n",
    "    #The mean score and the 95% confidence interval of the score estimate (accuracy)\n",
    "    ScoreSummaryByModel.append([comment,mean, std, \"%0.3f (+/- %0.3f)\" % (mean, std * 2)])\n",
    "    print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.934 (+/- 0.023)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "ModelEvaluation (MultinomialNB(),'Naive Bayes classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifier results are not very impressive. It's just a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.944 (+/- 0.024)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "ModelEvaluation (BernoulliNB(binarize=0.0),'Bernoulli Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes reasults looks better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.943 (+/- 0.013)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "ModelEvaluation (LinearSVC(),'LinearSVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC looks Ok Maybe parameters tuning of the method improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.515 (+/- 0.001)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "ModelEvaluation (SVC(),'SVC, default rbf kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ups :( Default rbf kernel definetly does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.943 (+/- 0.014)\n"
     ]
    }
   ],
   "source": [
    "ModelEvaluation (SVC(kernel='linear'),'SVC, linear kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, SVC with linear kernel should be not worse then LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.942 (+/- 0.018)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "ModelEvaluation (SGDClassifier(),'SGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear classifiers (SVM, logistic regression, a.o.) with SGD training should also be good for the problem. Let's try to optimize its parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the summary. Bernoulli Naive Bayes with default parameters returned teh best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli Naive Bayes</td>\n",
       "      <td>0.943685</td>\n",
       "      <td>0.011849</td>\n",
       "      <td>0.944 (+/- 0.024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC, linear kernel</td>\n",
       "      <td>0.942895</td>\n",
       "      <td>0.006942</td>\n",
       "      <td>0.943 (+/- 0.014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.942894</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.943 (+/- 0.013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.942370</td>\n",
       "      <td>0.009249</td>\n",
       "      <td>0.942 (+/- 0.018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes classifier</td>\n",
       "      <td>0.933994</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.934 (+/- 0.023)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC, default rbf kernel</td>\n",
       "      <td>0.515064</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.515 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Method      Mean       Std           Accuracy\n",
       "1    Bernoulli Naive Bayes  0.943685  0.011849  0.944 (+/- 0.024)\n",
       "4       SVC, linear kernel  0.942895  0.006942  0.943 (+/- 0.014)\n",
       "2                LinearSVC  0.942894  0.006748  0.943 (+/- 0.013)\n",
       "5                      SGD  0.942370  0.009249  0.942 (+/- 0.018)\n",
       "0   Naive Bayes classifier  0.933994  0.011551  0.934 (+/- 0.023)\n",
       "3  SVC, default rbf kernel  0.515064  0.000643  0.515 (+/- 0.001)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByModel=DataFrame(ScoreSummaryByModel,columns=['Method','Mean','Std','Accuracy'])\n",
    "df_ScoreSummaryByModel.sort_values(['Mean'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better features extractions let's try tokenize the text, remove stopwords (including the placeholders we added at the stage of data pre-processing) and stem the words\n",
    "I will use the function in farther analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def text_process(text):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Tokenizes and removes punctuation\n",
    "    2. Removes  stopwords\n",
    "    3. Stems\n",
    "    4. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenizing\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed=tokenizer.tokenize(text)\n",
    "    \n",
    "    # removing any stopwords\n",
    "    stoplist = stopwords.words('english')\n",
    "    stoplist.append('twitter')\n",
    "    stoplist.append('pic')\n",
    "    stoplist.append('com')\n",
    "    stoplist.append('net')\n",
    "    stoplist.append('gov')\n",
    "    stoplist.append('tv')\n",
    "    stoplist.append('www')\n",
    "    stoplist.append('twitter')\n",
    "    stoplist.append('num')\n",
    "    stoplist.append('date')\n",
    "    stoplist.append('time')\n",
    "    stoplist.append('url')\n",
    "    stoplist.append('ref')\n",
    "\n",
    "    text_processed = [word.lower() for word in text_processed if word.lower() not in stoplist]\n",
    "    \n",
    "    # steming\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    \n",
    "    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
    "    \n",
    "\n",
    "    return text_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the grid search approach for parameter tuning. It will methodically build and evaluate a model for each combination of \n",
    "algorithm parameters and feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ScoreSummaryByModelParams = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "def ModelParamsEvaluation (vectorizer,model,params,comment):\n",
    "    pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', model),\n",
    "    ])\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=params, verbose=1)\n",
    "    grid_search.fit(df['text'], df['author'])\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        ScoreSummaryByModelParams.append([comment,grid_search.best_score_,\"\\t%s: %r\" % (param_name, best_parameters[param_name])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   13.7s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   56.5s\n",
      "[Parallel(n_jobs=1)]: Done 216 out of 216 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.965\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n",
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "p = {'vect__analyzer':('char', 'char_wb'),\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((2, 2), (3, 3)), \n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(CountVectorizer(),BernoulliNB(),p,'Bernoulli Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense. 3 chars is almost a word (in actual use, the average is 4.79 letters per word, and 80% are between 2 and 7 letters long according to http://norvig.com/mayzner.html). Other classification methods tests provided the same result - using 3 chars is the best approach. I do not include the results for other methods in the notebook. I tested them on my local computer. Interesting, the score is higher then for word analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   18.6s\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (3, 3), (5,5),(2,5)), \n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(CountVectorizer(analyzer='word'),BernoulliNB(),p,'Bernoulli Naive Bayes, analyzer=word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I, actually, expected better results for longer ngrams. Maybe becasue tweets are short messages, unigrams make more sense. Other classification methods tests provided the same result - using unigrams is the best approach. I do not include the results for other methods in the notebook. I tested them on my local computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (3, 3), (5,5),(2,5)), \n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(CountVectorizer(analyzer='word',tokenizer=text_process),BernoulliNB(),p,'Bernoulli Naive Bayes, analyzer=word, tokenizer=text_process')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a big difference if I use tokenizer. The result even slightly worse. Other classification methods tests provided the same result. I do not include the results for other methods in the notebook. I tested them on my local computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the combination of all vectors with discovered parameters. FeatureUnion concatenates results of multiple transformer objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "word_vector =  CountVectorizer(analyzer='word',stop_words='english',ngram_range=(1, 1),max_df=0.5)\n",
    "char_vector = CountVectorizer(analyzer='char_wb',ngram_range=(3, 3),max_df=0.75)\n",
    "vectorizer = FeatureUnion([(\"chars\", char_vector),(\"words\", word_vector)])\n",
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(vectorizer,BernoulliNB(),p,'Bernoulli Naive Bayes, vectorizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is becomes better on the combination.\n",
    "Let's try to add tokenized vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vector =  CountVectorizer(analyzer='word',stop_words='english',ngram_range=(1, 5),max_df=0.5)\n",
    "char_vector = CountVectorizer(analyzer='char_wb',ngram_range=(3, 3),max_df=0.75)\n",
    "text_vector = CountVectorizer(analyzer='word',tokenizer=text_process,ngram_range=(3, 3),max_df=0.75)\n",
    "vectorizer = FeatureUnion([(\"chars\", char_vector),(\"words\", word_vector),(\"text\", text_vector)])\n",
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(vectorizer,BernoulliNB(),p,'Bernoulli Naive Bayes, vectorizer+text_vector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result improves a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LinearSVC\n",
    "p = {'vect__analyzer':('char', 'char_wb'),\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((2, 2), (3, 3)),\n",
    "    'clf__C': (1,0.1,0.01,0.001,0.0001)\n",
    "    }\n",
    "ModelParamsEvaluation(CountVectorizer(),LinearSVC(),p,'LinearSVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LinearSVC\n",
    "p = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (3, 3), (5,5),(2,5)),\n",
    "    'clf__C': (1,0.1,0.01,0.001,0.0001)\n",
    "    }\n",
    "ModelParamsEvaluation(CountVectorizer(analyzer='word'),LinearSVC(),p,'LinearSVC analyzer=word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LinearSVC\n",
    "word_vector =  CountVectorizer(analyzer='word',stop_words='english',ngram_range=(1, 5),max_df=0.5)\n",
    "char_vector = CountVectorizer(analyzer='char_wb',ngram_range=(3, 3),max_df=0.75)\n",
    "vectorizer = FeatureUnion([(\"chars\", char_vector),(\"words\", word_vector)])\n",
    "p = {'clf__C': (1,0.1,0.01,0.001,0.0001)}\n",
    "ModelParamsEvaluation(vectorizer,LinearSVC(),p,'LinearSVC vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LinearSVC\n",
    "word_vector =  CountVectorizer(analyzer='word',stop_words='english',ngram_range=(1, 5),max_df=0.5)\n",
    "char_vector = CountVectorizer(analyzer='char_wb',ngram_range=(3, 3),max_df=0.75)\n",
    "text_vector = CountVectorizer(analyzer='word',tokenizer=text_process,ngram_range=(3, 3),max_df=0.75)\n",
    "vectorizer = FeatureUnion([(\"chars\", char_vector),(\"words\", word_vector),(\"text\", text_vector)])\n",
    "p = {'clf__C': (1,0.1,0.01,0.001,0.0001)}\n",
    "ModelParamsEvaluation(vectorizer,LinearSVC(),p,'LinearSVC vectorizer+text_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVC, linear kernel\n",
    "p = {'vect__analyzer':('char', 'char_wb'),\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((2, 2), (3, 3)),\n",
    "     'clf__C': (1,0.1,0.01,0.001,0.0001)}\n",
    "ModelParamsEvaluation (CountVectorizer(),SVC(kernel='linear'),p,'SVC, linear kernel, char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVC, linear kernel\n",
    "p = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (3, 3), (5,5),(2,5)),\n",
    "    'clf__C': (1,0.1,0.01,0.001,0.0001)}\n",
    "ModelParamsEvaluation (CountVectorizer(analyzer='word'),SVC(kernel='linear'),p,'SVC, linear kernel, analyzer=word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVC, linear kernel\n",
    "p = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (3, 3), (5,5),(2,5)),\n",
    "     'clf__C': (1,0.1,0.01,0.001,0.0001)}\n",
    "ModelParamsEvaluation (CountVectorizer(analyzer='word',tokenizer=text_process),SVC(kernel='linear'),p,'SVC, linear kernel, analyzer=word, tokenizer=text_process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vector =  CountVectorizer(analyzer='word',stop_words='english',ngram_range=(1, 5),max_df=0.5)\n",
    "char_vector = CountVectorizer(analyzer='char_wb',ngram_range=(3, 3),max_df=0.75)\n",
    "vectorizer = FeatureUnion([(\"chars\", char_vector),(\"words\", word_vector)])\n",
    "p = {'clf__C': (1,0.1,0.01,0.001,0.0001)}\n",
    "ModelParamsEvaluation(vectorizer,SVC(kernel='linear'),p,'SVC, linear  vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vector =  CountVectorizer(analyzer='word',stop_words='english',ngram_range=(1, 5),max_df=0.5)\n",
    "char_vector = CountVectorizer(analyzer='char_wb',ngram_range=(3, 3),max_df=0.75)\n",
    "text_vector = CountVectorizer(analyzer='word',tokenizer=text_process,ngram_range=(3, 3),max_df=0.75)\n",
    "vectorizer = FeatureUnion([(\"chars\", char_vector),(\"words\", word_vector),(\"text\", text_vector)])\n",
    "p = {'clf__C': (1,0.1,0.01,0.001,0.0001)}\n",
    "ModelParamsEvaluation(vectorizer,SVC(kernel='linear'),p,'SVC, linear  vectorizer+text_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SGDClassifier\n",
    "p = {'vect__analyzer':('char', 'char_wb'),\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((2, 2), (3, 3)),\n",
    "    'clf__alpha': (0.01,0.001,0.0001,0.00001, 0.000001),\n",
    "    'clf__penalty': ('l1','l2', 'elasticnet')}\n",
    "ModelParamsEvaluation (CountVectorizer(),SGDClassifier(),p,'SGD Classifier, analyzer=char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SGDClassifier\n",
    "p = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (3, 3), (5,5),(2,5)),\n",
    "    'clf__alpha': (0.01,0.001,0.0001,0.00001, 0.000001),\n",
    "    'clf__penalty': ('l1','l2', 'elasticnet')}\n",
    "ModelParamsEvaluation (CountVectorizer(analyzer='word'),SGDClassifier(),p,'SGD Classifier, analyzer=word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SGDClassifier\n",
    "p = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (3, 3), (5,5),(2,5)),\n",
    "    'clf__alpha': (0.01,0.001,0.0001,0.00001, 0.000001),\n",
    "    'clf__penalty': ('l1','l2', 'elasticnet')}\n",
    "ModelParamsEvaluation (CountVectorizer(analyzer='word',tokenizer=text_process),SGDClassifier(),p,'SGD Classifier, analyzer=word,tokenizer=text_process')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SGDClassifier\n",
    "word_vector =  CountVectorizer(analyzer='word',stop_words='english',ngram_range=(1, 5),max_df=0.5)\n",
    "char_vector = CountVectorizer(analyzer='char_wb',ngram_range=(3, 3),max_df=0.75)\n",
    "vectorizer = FeatureUnion([(\"chars\", char_vector),(\"words\", word_vector)])\n",
    "p = {'clf__alpha': (0.01,0.001,0.0001,0.00001, 0.000001),\n",
    "    'clf__penalty': ('l1','l2', 'elasticnet')}\n",
    "ModelParamsEvaluation (vectorizer,SGDClassifier(),p,'SGD Classifier, vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SGDClassifier\n",
    "word_vector =  CountVectorizer(analyzer='word',stop_words='english',ngram_range=(1, 5),max_df=0.5)\n",
    "char_vector = CountVectorizer(analyzer='char_wb',ngram_range=(3, 3),max_df=0.75)\n",
    "text_vector = CountVectorizer(analyzer='word',tokenizer=text_process,ngram_range=(3, 3),max_df=0.75)\n",
    "vectorizer = FeatureUnion([(\"chars\", char_vector),(\"words\", word_vector),(\"text\", text_vector)])\n",
    "p = {'clf__alpha': (0.01,0.001,0.0001,0.00001, 0.000001),\n",
    "    'clf__penalty': ('l1','l2', 'elasticnet')}\n",
    "ModelParamsEvaluation (vectorizer,SGDClassifier(),p,'SGD Classifier, vectorizer+text_vector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the summary. And the winner is Bernoulli Naive Bayes with alpha = 0.0001 based on combination of character 3-ngrams, word unigrams and tokenized unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\n",
    "df_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByModelParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the discovered best approach to test data set\n",
    "I use Bernoulli Naive Bayes and SGDClassifier for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need score metrics and few functions now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc,precision_score, accuracy_score, recall_score, f1_score\n",
    "from scipy import interp\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ROCCurves (Actual, Predicted):\n",
    "    '''\n",
    "    Plot ROC curves for the multiclass problem\n",
    "    based on http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "    '''\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    n_classes=2\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(Actual, Predicted)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Actual.ravel(), Predicted.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    ##############################################################################\n",
    "    # Plot ROC curves for the multiclass problem\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         linewidth=2)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         linewidth=2)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                   ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConfusionMatrix(author_test_b,author_predictions_b):\n",
    "    cm=confusion_matrix(author_test_b,author_predictions_b)\n",
    "    plt.matshow(cm)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PredictionEvaluation(author_test_b,author_predictions_b):\n",
    "    print ('Precision: %0.3f' % (precision_score(author_test_b,author_predictions_b)))\n",
    "    print ('Accuracy: %0.3f' % (accuracy_score(author_test_b,author_predictions_b)))\n",
    "    print ('Recall: %0.3f' % (recall_score(author_test_b,author_predictions_b)))\n",
    "    print ('F1: %0.3f' % (f1_score(author_test_b,author_predictions_b)))\n",
    "    print ('Confussion matrix:')\n",
    "    print (confusion_matrix(author_test_b,author_predictions_b))\n",
    "    print ('ROC-AUC: %0.3f' % (roc_auc_score(author_test_b,author_predictions_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vector = CountVectorizer(analyzer='word',stop_words='english',ngram_range=(1, 5),max_df=0.5)\n",
    "char_vector = CountVectorizer(analyzer='char_wb',ngram_range=(3, 3),max_df=0.75)\n",
    "text_vector = CountVectorizer(analyzer='word',tokenizer=text_process,ngram_range=(3, 3),max_df=0.75)\n",
    "vectorizer  = FeatureUnion([(\"chars\", char_vector),(\"words\", word_vector),(\"text\", text_vector)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', BernoulliNB(alpha=0.0001)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline.fit(twt_train,author_train)\n",
    "author_predictions = pipeline.predict(twt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the results we need to binarize labels(authors) - 'HillaryClinton' is 0 and 'KimKardashian' is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "author_test_b = lb.fit_transform(author_test.values)\n",
    "author_predictions_b  = lb.fit_transform(author_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PredictionEvaluation(author_test_b,author_predictions_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ROCCurves (author_test_b,author_predictions_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ConfusionMatrix(author_test,author_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDClassifier pipeline on the same test data with slightly higher precision but less accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(alpha=0.0001)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline.fit(twt_train,author_train)\n",
    "author_predictions = pipeline.predict(twt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_test_b = lb.fit_transform(author_test.values)\n",
    "author_predictions_b  = lb.fit_transform(author_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PredictionEvaluation(author_test_b,author_predictions_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ROCCurves (author_test_b,author_predictions_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ConfusionMatrix(author_test_b,author_predictions_b)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
