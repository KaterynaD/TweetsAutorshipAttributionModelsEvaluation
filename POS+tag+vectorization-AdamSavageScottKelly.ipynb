{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6091"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data\n",
    "df=pd.read_csv('C:\\Kate\\Python\\Authorship Attribution\\data\\AllTweets.csv')\n",
    "author1='ScottKelly'\n",
    "author2='AdamSavage'\n",
    "df_kk=df.loc[(df['author'] == author1)]\n",
    "df_hc=df.loc[(df['author'] == author2)]\n",
    "df=df_kk.append(df_hc,ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "#1000 random sample rows for KK\n",
    "rows = random.sample(df_kk.index, 1000)\n",
    "df_kk = df_kk.ix[rows]\n",
    "#2000 random sample rows for HC\n",
    "rows = random.sample(df_hc.index, 1000)\n",
    "df_hc = df_hc.ix[rows]\n",
    "#join back together\n",
    "df=df_kk.append(df_hc,ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1964"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data pre-processing\n",
    "df.drop(df[df.retweet==True].index, inplace=True)\n",
    "df['num_of_words'] = df[\"text\"].str.split().apply(len)\n",
    "df.drop(df[df.num_of_words<4].index, inplace=True)\n",
    "df[\"text\"].replace(r\"http\\S+\", \"URL\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"@\\S+\", \"REF\", regex=True ,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/.-](\\d{1,2})[/.-](\\d{2,4})+\", \"DATE\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "df[\"text\"].replace(r\"\\d+\", \"NUM\", regex=True,inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "pos_code_map={'CC':'A','CD':'B','DT':'C','EX':'D','FW':'E','IN':'F','JJ':'G','JJR':'H','JJS':'I','LS':'J','MD':'K','NN':'L','NNS':'M',\n",
    "'NNP':'N','NNPS':'O','PDT':'P','POS':'Q','PRP':'R','PRP$':'S','RB':'T','RBR':'U','RBS':'V','RP':'W','SYM':'X','TO':'Y','UH':'Z',\n",
    "'VB':'1','VBD':'2','VBG':'3','VBN':'4','VBP':'5','VBZ':'6','WDT':'7','WP':'8','WP$':'9','WRB':'@'}\n",
    "code_pos_map={v: k for k, v in pos_code_map.iteritems()}\n",
    "#Python 3 inv_map = {v: k for k, v in my_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert(tag):\n",
    "    try:\n",
    "        code=pos_code_map[tag]\n",
    "    except:\n",
    "        code='?'\n",
    "    return code\n",
    "def inv_convert(code):\n",
    "    try:\n",
    "        tag=code_pos_map[code]\n",
    "    except:\n",
    "        tag='?'\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "def pos_tags(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed=tokenizer.tokenize(text)\n",
    "    return \"\".join(convert(tag) for (word, tag) in nltk.pos_tag(text_processed))\n",
    "def text_pos_inv_convert(text):\n",
    "    return \"-\".join(inv_convert(c.upper()) for c in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['text_pos']=df.apply(lambda x: pos_tags(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>text_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>Day NUM. An interesting band of clouds over So...</td>\n",
       "      <td>NNCGLFMFNN2NFNNLLL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>Soyuz final exam tomorrow. NUM hours in the So...</td>\n",
       "      <td>NGLLNMFCNLL2CLL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>Music is a huge part of life in space. Here's ...</td>\n",
       "      <td>N6CGLFLFLTGSNLFCNNNLLLN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>Post-tropical Cyclone Linda sits off the South...</td>\n",
       "      <td>NGNN6FCNNLNLLN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>#EarthArt A swirl of green runs through it. #Y...</td>\n",
       "      <td>1NLFGMFRNLLN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                               text  \\\n",
       "0  ScottKelly  Day NUM. An interesting band of clouds over So...   \n",
       "1  ScottKelly  Soyuz final exam tomorrow. NUM hours in the So...   \n",
       "2  ScottKelly  Music is a huge part of life in space. Here's ...   \n",
       "3  ScottKelly  Post-tropical Cyclone Linda sits off the South...   \n",
       "4  ScottKelly  #EarthArt A swirl of green runs through it. #Y...   \n",
       "\n",
       "                  text_pos  \n",
       "0       NNCGLFMFNN2NFNNLLL  \n",
       "1          NGLLNMFCNLL2CLL  \n",
       "2  N6CGLFLFLTGSNLFCNNNLLLN  \n",
       "3           NGNN6FCNNLNLLN  \n",
       "4             1NLFGMFRNLLN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix[:,['author','text','text_pos']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "for a in df.author.unique():\n",
    "    v = CountVectorizer(analyzer='char',ngram_range=(7, 7))\n",
    "    ngrams = v.fit_transform(df[df['author'] == a]['text_pos'])\n",
    "    df_t=pd.DataFrame(\n",
    "    {'Feature': v.get_feature_names(),\n",
    "     'Count': list(ngrams.sum(axis=0).flat),\n",
    "     'Author': a\n",
    "    })\n",
    "    #\n",
    "    df_features=df_features.append(df_t,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features['Feature_POS']=df_features.apply(lambda x: text_pos_inv_convert(x['Feature']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Count</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13610</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>5</td>\n",
       "      <td>fclllll</td>\n",
       "      <td>IN-DT-NN-NN-NN-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14710</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>5</td>\n",
       "      <td>glfnnnn</td>\n",
       "      <td>JJ-NN-IN-NNP-NNP-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17278</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>4</td>\n",
       "      <td>ncnnnnn</td>\n",
       "      <td>NNP-DT-NNP-NNP-NNP-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16016</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>4</td>\n",
       "      <td>llfnfnn</td>\n",
       "      <td>NN-NN-IN-NNP-IN-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13512</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>4</td>\n",
       "      <td>fcgllnn</td>\n",
       "      <td>IN-DT-JJ-NN-NN-NNP-NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Author  Count  Feature                 Feature_POS\n",
       "13610  AdamSavage      5  fclllll        IN-DT-NN-NN-NN-NN-NN\n",
       "14710  AdamSavage      5  glfnnnn    JJ-NN-IN-NNP-NNP-NNP-NNP\n",
       "17278  AdamSavage      4  ncnnnnn  NNP-DT-NNP-NNP-NNP-NNP-NNP\n",
       "16016  AdamSavage      4  llfnfnn     NN-NN-IN-NNP-IN-NNP-NNP\n",
       "13512  AdamSavage      4  fcgllnn      IN-DT-JJ-NN-NN-NNP-NNP"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[~df_features.Feature.isin(df_features[df_features['Author'] != author2].Feature)].sort_values('Count', ascending=False).ix[:,['Author','Count','Feature','Feature_POS']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Count</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5392</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>38</td>\n",
       "      <td>lfnnlln</td>\n",
       "      <td>NN-IN-NNP-NNP-NN-NN-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7231</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>32</td>\n",
       "      <td>nfnnlll</td>\n",
       "      <td>NNP-IN-NNP-NNP-NN-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7477</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>27</td>\n",
       "      <td>nlfnnll</td>\n",
       "      <td>NNP-NN-IN-NNP-NNP-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5975</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>19</td>\n",
       "      <td>lnlfnnl</td>\n",
       "      <td>NN-NNP-NN-IN-NNP-NNP-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>17</td>\n",
       "      <td>lnfnnll</td>\n",
       "      <td>NN-NNP-IN-NNP-NNP-NN-NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Author  Count  Feature              Feature_POS\n",
       "5392  ScottKelly     38  lfnnlln  NN-IN-NNP-NNP-NN-NN-NNP\n",
       "7231  ScottKelly     32  nfnnlll  NNP-IN-NNP-NNP-NN-NN-NN\n",
       "7477  ScottKelly     27  nlfnnll  NNP-NN-IN-NNP-NNP-NN-NN\n",
       "5975  ScottKelly     19  lnlfnnl  NN-NNP-NN-IN-NNP-NNP-NN\n",
       "5935  ScottKelly     17  lnfnnll  NN-NNP-IN-NNP-NNP-NN-NN"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[~df_features.Feature.isin(df_features[df_features['Author'] != author1].Feature)].sort_values('Count', ascending=False).ix[:,['Author','Count','Feature','Feature_POS']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "twt_train, twt_test, author_train, author_test = train_test_split(df.ix[:,['text','text_pos']], df['author'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def text_process(text):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Tokenizes and removes punctuation\n",
    "    3. Stems\n",
    "    4. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenizing\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed=tokenizer.tokenize(text)\n",
    "    \n",
    "    \n",
    "    # steming\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    \n",
    "    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
    "    \n",
    "\n",
    "    return text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ScoreSummaryByModelParams = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextAndTextCodedExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the text & text_pos from a tweet in a single pass.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, tweets):\n",
    "        features=tweets.ix[:,['text_pos','text']].to_records(index=False)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ModelParamsEvaluation (f_union,model,params,comment):\n",
    "    pipeline = Pipeline([\n",
    "    # Extract the text & text_coded\n",
    "    ('textandtextcoded', TextAndTextCodedExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from text and text_coded\n",
    "    ('union', f_union, ),\n",
    "\n",
    "    # Use a  classifier on the combined features\n",
    "    ('clf', model),\n",
    "    ])\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=params, verbose=1, cv=10)\n",
    "    grid_search.fit(twt_train, author_train)\n",
    "    #best score\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        ScoreSummaryByModelParams.append([comment,grid_search.best_score_,\"\\t%s: %r\" % (param_name, best_parameters[param_name])])    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "              # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char')),\n",
    "            ])),               \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 144 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    7.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   25.0s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:   55.5s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  1.7min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  2.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.846\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.0001\n",
      "\tunion__char__tfidf__max_df: 0.5\n",
      "\tunion__char__tfidf__max_features: None\n",
      "\tunion__char__tfidf__ngram_range: (3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1440 out of 1440 | elapsed:  2.9min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "p = {\n",
    "    'union__char__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__char__tfidf__ngram_range': ((2, 2), (3, 3)), \n",
    "    'union__char__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "            ('selector', ItemSelector(key='text')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='word')),\n",
    "            ])),              \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 720 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    4.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   17.4s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:   40.4s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  1.2min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  2.7min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:  3.6min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed:  4.6min\n",
      "[Parallel(n_jobs=1)]: Done 4049 tasks       | elapsed:  5.8min\n",
      "[Parallel(n_jobs=1)]: Done 4999 tasks       | elapsed:  7.2min\n",
      "[Parallel(n_jobs=1)]: Done 6049 tasks       | elapsed:  8.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.889\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tunion__word__tfidf__max_df: 0.5\n",
      "\tunion__word__tfidf__max_features: None\n",
      "\tunion__word__tfidf__ngram_range: (1, 1)\n",
      "\tunion__word__tfidf__stop_words: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 7199 tasks       | elapsed: 10.4min\n",
      "[Parallel(n_jobs=1)]: Done 7200 out of 7200 | elapsed: 10.4min finished\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__word__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__word__tfidf__ngram_range': ((1, 1),(2, 2), (3, 3),(4,4),(5,5)), \n",
    "    'union__word__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'union__word__tfidf__stop_words': (None, 'english'),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('text', Pipeline([\n",
    "            ('selector', ItemSelector(key='text')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process)),\n",
    "            ])),              \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 360 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   13.9s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   58.7s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:  2.1min\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  3.9min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  6.1min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  8.9min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed: 12.2min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed: 15.9min\n",
      "[Parallel(n_jobs=1)]: Done 3600 out of 3600 | elapsed: 17.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.885\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tunion__text__tfidf__max_df: 0.5\n",
      "\tunion__text__tfidf__max_features: None\n",
      "\tunion__text__tfidf__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__text__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__text__tfidf__ngram_range': ((1, 1),(2, 2), (3, 3),(4,4),(5,5)), \n",
    "    'union__text__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, stemmed words, no stop words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling pos tag features  from the text_pos\n",
    "            ('text_pos', Pipeline([\n",
    "            ('selector', ItemSelector(key='text_pos')),\n",
    "            ('tfidf',    TfidfVectorizer(analyzer='char')),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 360 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    2.9s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   12.2s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:   27.7s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:   49.0s\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  1.3min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  1.9min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:  2.5min\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed:  3.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.692\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1\n",
      "\tunion__text_pos__tfidf__max_df: 0.5\n",
      "\tunion__text_pos__tfidf__max_features: None\n",
      "\tunion__text_pos__tfidf__ngram_range: (3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 3600 out of 3600 | elapsed:  3.7min finished\n"
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'union__text_pos__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'union__text_pos__tfidf__ngram_range': ((3, 3), (4, 4),(5,5),(6,6),(7,7)), \n",
    "    'union__text_pos__tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>BestScore</th>\n",
       "      <th>BestParameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.888646</td>\n",
       "      <td>\\tunion__word__tfidf__stop_words: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.888646</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.888646</td>\n",
       "      <td>\\tunion__word__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.888646</td>\n",
       "      <td>\\tunion__word__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.888646</td>\n",
       "      <td>\\tunion__word__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.885007</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.885007</td>\n",
       "      <td>\\tunion__text__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.885007</td>\n",
       "      <td>\\tunion__text__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.885007</td>\n",
       "      <td>\\tunion__text__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.846434</td>\n",
       "      <td>\\tclf__alpha: 0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.846434</td>\n",
       "      <td>\\tunion__char__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.846434</td>\n",
       "      <td>\\tunion__char__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.846434</td>\n",
       "      <td>\\tunion__char__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.692140</td>\n",
       "      <td>\\tclf__alpha: 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.692140</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.692140</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.692140</td>\n",
       "      <td>\\tunion__text_pos__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Method  BestScore  \\\n",
       "8                         Bernoulli Naive Bayes, word   0.888646   \n",
       "4                         Bernoulli Naive Bayes, word   0.888646   \n",
       "5                         Bernoulli Naive Bayes, word   0.888646   \n",
       "6                         Bernoulli Naive Bayes, word   0.888646   \n",
       "7                         Bernoulli Naive Bayes, word   0.888646   \n",
       "9   Bernoulli Naive Bayes, stemmed words, no stop ...   0.885007   \n",
       "12  Bernoulli Naive Bayes, stemmed words, no stop ...   0.885007   \n",
       "11  Bernoulli Naive Bayes, stemmed words, no stop ...   0.885007   \n",
       "10  Bernoulli Naive Bayes, stemmed words, no stop ...   0.885007   \n",
       "0                         Bernoulli Naive Bayes, char   0.846434   \n",
       "1                         Bernoulli Naive Bayes, char   0.846434   \n",
       "3                         Bernoulli Naive Bayes, char   0.846434   \n",
       "2                         Bernoulli Naive Bayes, char   0.846434   \n",
       "13                    Bernoulli Naive Bayes, POS tags   0.692140   \n",
       "14                    Bernoulli Naive Bayes, POS tags   0.692140   \n",
       "15                    Bernoulli Naive Bayes, POS tags   0.692140   \n",
       "16                    Bernoulli Naive Bayes, POS tags   0.692140   \n",
       "\n",
       "                                    BestParameter  \n",
       "8          \\tunion__word__tfidf__stop_words: None  \n",
       "4                               \\tclf__alpha: 0.1  \n",
       "5               \\tunion__word__tfidf__max_df: 0.5  \n",
       "6        \\tunion__word__tfidf__max_features: None  \n",
       "7       \\tunion__word__tfidf__ngram_range: (1, 1)  \n",
       "9                               \\tclf__alpha: 0.1  \n",
       "12      \\tunion__text__tfidf__ngram_range: (1, 1)  \n",
       "11       \\tunion__text__tfidf__max_features: None  \n",
       "10              \\tunion__text__tfidf__max_df: 0.5  \n",
       "0                            \\tclf__alpha: 0.0001  \n",
       "1               \\tunion__char__tfidf__max_df: 0.5  \n",
       "3       \\tunion__char__tfidf__ngram_range: (3, 3)  \n",
       "2        \\tunion__char__tfidf__max_features: None  \n",
       "13                                \\tclf__alpha: 1  \n",
       "14          \\tunion__text_pos__tfidf__max_df: 0.5  \n",
       "15   \\tunion__text_pos__tfidf__max_features: None  \n",
       "16  \\tunion__text_pos__tfidf__ngram_range: (3, 3)  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\n",
    "df_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByModelParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    8.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.881\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   11.0s finished\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f2_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "            # Pipeline for pulling stememd word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   17.6s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   21.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.876\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.0001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f2_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),    \n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   19.7s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   24.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.885\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   19.2s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   23.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.878\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.0001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + stemmed word + POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   10.8s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   13.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.879\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "ModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + POS tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f4_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "             # Pipeline for pulling word features after word_processing from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   21.2s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   26.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.886\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n",
    "\n",
    "ModelParamsEvaluation(f4_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + word + stemmed word + POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>BestScore</th>\n",
       "      <th>BestParameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.888646</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.888646</td>\n",
       "      <td>\\tunion__word__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.888646</td>\n",
       "      <td>\\tunion__word__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.888646</td>\n",
       "      <td>\\tunion__word__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bernoulli Naive Bayes, word</td>\n",
       "      <td>0.888646</td>\n",
       "      <td>\\tunion__word__tfidf__stop_words: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + stemmed w...</td>\n",
       "      <td>0.886463</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.885007</td>\n",
       "      <td>\\tunion__text__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + stemmed word</td>\n",
       "      <td>0.885007</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.885007</td>\n",
       "      <td>\\tunion__text__tfidf__ngram_range: (1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.885007</td>\n",
       "      <td>\\tunion__text__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bernoulli Naive Bayes, stemmed words, no stop ...</td>\n",
       "      <td>0.885007</td>\n",
       "      <td>\\tclf__alpha: 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word</td>\n",
       "      <td>0.880640</td>\n",
       "      <td>\\tclf__alpha: 0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Bernoulli Naive Bayes, char + word + POS tags</td>\n",
       "      <td>0.879185</td>\n",
       "      <td>\\tclf__alpha: 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bernoulli Naive Bayes, char + stemmed word + P...</td>\n",
       "      <td>0.878457</td>\n",
       "      <td>\\tclf__alpha: 0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bernoulli Naive Bayes, char + stemmed word</td>\n",
       "      <td>0.875546</td>\n",
       "      <td>\\tclf__alpha: 0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.846434</td>\n",
       "      <td>\\tunion__char__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.846434</td>\n",
       "      <td>\\tunion__char__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.846434</td>\n",
       "      <td>\\tunion__char__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli Naive Bayes, char</td>\n",
       "      <td>0.846434</td>\n",
       "      <td>\\tclf__alpha: 0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.692140</td>\n",
       "      <td>\\tclf__alpha: 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.692140</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_df: 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.692140</td>\n",
       "      <td>\\tunion__text_pos__tfidf__max_features: None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bernoulli Naive Bayes, POS tags</td>\n",
       "      <td>0.692140</td>\n",
       "      <td>\\tunion__text_pos__tfidf__ngram_range: (3, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Method  BestScore  \\\n",
       "4                         Bernoulli Naive Bayes, word   0.888646   \n",
       "5                         Bernoulli Naive Bayes, word   0.888646   \n",
       "6                         Bernoulli Naive Bayes, word   0.888646   \n",
       "7                         Bernoulli Naive Bayes, word   0.888646   \n",
       "8                         Bernoulli Naive Bayes, word   0.888646   \n",
       "22  Bernoulli Naive Bayes, char + word + stemmed w...   0.886463   \n",
       "10  Bernoulli Naive Bayes, stemmed words, no stop ...   0.885007   \n",
       "19  Bernoulli Naive Bayes, char + word + stemmed word   0.885007   \n",
       "12  Bernoulli Naive Bayes, stemmed words, no stop ...   0.885007   \n",
       "11  Bernoulli Naive Bayes, stemmed words, no stop ...   0.885007   \n",
       "9   Bernoulli Naive Bayes, stemmed words, no stop ...   0.885007   \n",
       "17                 Bernoulli Naive Bayes, char + word   0.880640   \n",
       "21      Bernoulli Naive Bayes, char + word + POS tags   0.879185   \n",
       "20  Bernoulli Naive Bayes, char + stemmed word + P...   0.878457   \n",
       "18         Bernoulli Naive Bayes, char + stemmed word   0.875546   \n",
       "1                         Bernoulli Naive Bayes, char   0.846434   \n",
       "3                         Bernoulli Naive Bayes, char   0.846434   \n",
       "2                         Bernoulli Naive Bayes, char   0.846434   \n",
       "0                         Bernoulli Naive Bayes, char   0.846434   \n",
       "13                    Bernoulli Naive Bayes, POS tags   0.692140   \n",
       "14                    Bernoulli Naive Bayes, POS tags   0.692140   \n",
       "15                    Bernoulli Naive Bayes, POS tags   0.692140   \n",
       "16                    Bernoulli Naive Bayes, POS tags   0.692140   \n",
       "\n",
       "                                    BestParameter  \n",
       "4                               \\tclf__alpha: 0.1  \n",
       "5               \\tunion__word__tfidf__max_df: 0.5  \n",
       "6        \\tunion__word__tfidf__max_features: None  \n",
       "7       \\tunion__word__tfidf__ngram_range: (1, 1)  \n",
       "8          \\tunion__word__tfidf__stop_words: None  \n",
       "22                            \\tclf__alpha: 0.001  \n",
       "10              \\tunion__text__tfidf__max_df: 0.5  \n",
       "19                            \\tclf__alpha: 0.001  \n",
       "12      \\tunion__text__tfidf__ngram_range: (1, 1)  \n",
       "11       \\tunion__text__tfidf__max_features: None  \n",
       "9                               \\tclf__alpha: 0.1  \n",
       "17                           \\tclf__alpha: 0.0001  \n",
       "21                            \\tclf__alpha: 0.001  \n",
       "20                           \\tclf__alpha: 0.0001  \n",
       "18                           \\tclf__alpha: 0.0001  \n",
       "1               \\tunion__char__tfidf__max_df: 0.5  \n",
       "3       \\tunion__char__tfidf__ngram_range: (3, 3)  \n",
       "2        \\tunion__char__tfidf__max_features: None  \n",
       "0                            \\tclf__alpha: 0.0001  \n",
       "13                                \\tclf__alpha: 1  \n",
       "14          \\tunion__text_pos__tfidf__max_df: 0.5  \n",
       "15   \\tunion__text_pos__tfidf__max_features: None  \n",
       "16  \\tunion__text_pos__tfidf__ngram_range: (3, 3)  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\n",
    "df_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByModelParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc,precision_score, accuracy_score, recall_score, f1_score\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ScoreSummaryByVector = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PredictionEvaluation(author_test_b,author_predictions_b,comment):\n",
    "    Precision=precision_score(author_test_b,author_predictions_b)\n",
    "    print ('Precision: %0.3f' % (Precision))\n",
    "    Accuracy=accuracy_score(author_test_b,author_predictions_b)\n",
    "    print ('Accuracy: %0.3f' % (Accuracy))\n",
    "    Recall=recall_score(author_test_b,author_predictions_b)\n",
    "    print ('Recall: %0.3f' % (Recall))\n",
    "    F1=f1_score(author_test_b,author_predictions_b)\n",
    "    print ('F1: %0.3f' % (F1))\n",
    "    print ('Confussion matrix:')\n",
    "    print (confusion_matrix(author_test_b,author_predictions_b))\n",
    "    ROC_AUC=roc_auc_score(author_test_b,author_predictions_b)\n",
    "    print ('ROC-AUC: %0.3f' % (ROC_AUC))\n",
    "    ScoreSummaryByVector.append([Precision,Accuracy,Recall,F1,ROC_AUC,comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ModelRun (f_union,model):\n",
    "    pipeline = Pipeline([\n",
    "    # Extract the text & text_coded\n",
    "    ('textandtextcoded', TextAndTextCodedExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from text and text_coded\n",
    "    ('union', f_union, ),\n",
    "\n",
    "    # Use a  classifier on the combined features\n",
    "    ('clf', model),\n",
    "    ])\n",
    "    pipeline.fit(twt_train, author_train)\n",
    "    author_predicted = pipeline.predict(twt_test)\n",
    "    \n",
    "    feature_names=list()\n",
    "    for p in (pipeline.get_params()['union'].transformer_list):\n",
    "        fn=(p[0],pipeline.get_params()['union'].get_params()[p[0]].get_params()['tfidf'].get_feature_names())\n",
    "        feature_names.append(fn)\n",
    "    df_fn=pd.DataFrame()\n",
    "    for fn in feature_names:\n",
    "        df_fn= df_fn.append(pd.DataFrame(\n",
    "        {'FeatureType': fn[0],\n",
    "         'Feature': fn[1]\n",
    "        }),\n",
    "        ignore_index=True)    \n",
    "    \n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    lb = LabelBinarizer()\n",
    "    author_test_b = lb.fit_transform(author_test.values)\n",
    "    author_predicted_b  = lb.fit_transform(author_predicted)\n",
    "    return (df_fn,pipeline.get_params()['clf'],author_predicted,author_predicted_b, author_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_informative_feature_for_binary_classification(feature_names, classifier):\n",
    "    class_labels = classifier.classes_\n",
    "\n",
    "    topnvalues_class0 = sorted(zip(classifier.coef_[0], feature_names['Feature'].values, feature_names['FeatureType'].values))\n",
    "    topnvalues_class1 = sorted(zip(classifier.coef_[0], feature_names['Feature'].values, feature_names['FeatureType'].values), reverse=True)\n",
    "\n",
    "    topn_df_class0=pd.DataFrame(topnvalues_class0, columns=['Coef','Feature','FeatureType'])\n",
    "    topn_df_class0['Author']=class_labels[0]\n",
    "    \n",
    "    topn_df_class1=pd.DataFrame(topnvalues_class1, columns=['Coef','Feature','FeatureType'])\n",
    "    topn_df_class1['Author']=class_labels[1]    \n",
    "    \n",
    "    topn_df=topn_df_class0.append(topn_df_class1)\n",
    "    \n",
    "        \n",
    "    return topn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "            # Pipeline for pulling stememd word features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),        \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f2_union,BernoulliNB(alpha=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.875\n",
      "Accuracy: 0.890\n",
      "Recall: 0.908\n",
      "F1: 0.891\n",
      "Confussion matrix:\n",
      "[[259  38]\n",
      " [ 27 266]]\n",
      "ROC-AUC: 0.890\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+stemmed word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "             # Pipeline for pulling word stemmed features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f3_union,BernoulliNB(alpha=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.875\n",
      "Accuracy: 0.893\n",
      "Recall: 0.914\n",
      "F1: 0.894\n",
      "Confussion matrix:\n",
      "[[261  38]\n",
      " [ 25 266]]\n",
      "ROC-AUC: 0.893\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+stemmed word+POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f4_union=FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling word features from the text\n",
    "            ('word', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "             # Pipeline for pulling word features after word_processing from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling char features  from the text\n",
    "            ('char', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for pulling flexible pattern features  from the text_coded\n",
    "            ('text_pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='text_pos')),\n",
    "                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n",
    "            ])),                  \n",
    "\n",
    "        ],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f4_union,BernoulliNB(alpha=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.895\n",
      "Accuracy: 0.907\n",
      "Recall: 0.922\n",
      "F1: 0.908\n",
      "Confussion matrix:\n",
      "[[263  32]\n",
      " [ 23 272]]\n",
      "ROC-AUC: 0.907\n"
     ]
    }
   ],
   "source": [
    "PredictionEvaluation(author_predicted_b, author_test_b,'char+word+stemmed word+POS tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.906780</td>\n",
       "      <td>0.922034</td>\n",
       "      <td>0.908180</td>\n",
       "      <td>0.906780</td>\n",
       "      <td>char+word+stemmed word+POS tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.893220</td>\n",
       "      <td>0.914089</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.893500</td>\n",
       "      <td>char+stemmed word+POS tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.889831</td>\n",
       "      <td>0.907850</td>\n",
       "      <td>0.891122</td>\n",
       "      <td>0.889952</td>\n",
       "      <td>char+stemmed word</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision  Accuracy    Recall        F1   ROC-AUC  \\\n",
       "2   0.894737  0.906780  0.922034  0.908180  0.906780   \n",
       "1   0.875000  0.893220  0.914089  0.894118  0.893500   \n",
       "0   0.875000  0.889831  0.907850  0.891122  0.889952   \n",
       "\n",
       "                           Vector  \n",
       "2  char+word+stemmed word+POS tag  \n",
       "1       char+stemmed word+POS tag  \n",
       "0               char+stemmed word  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ScoreSummaryByVector=DataFrame(ScoreSummaryByVector,columns=['Precision','Accuracy','Recall','F1','ROC-AUC','Vector'])\n",
    "df_ScoreSummaryByVector.sort_values(['F1'],ascending=False,inplace=True)\n",
    "df_ScoreSummaryByVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TopFeatures_df=most_informative_feature_for_binary_classification(feature_names, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>CoefChar</th>\n",
       "      <th>Char</th>\n",
       "      <th>CoefWord</th>\n",
       "      <th>Word</th>\n",
       "      <th>CoefText</th>\n",
       "      <th>Text</th>\n",
       "      <th>CoefTextPOS</th>\n",
       "      <th>TextPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>\\n(t</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>______________________________________________...</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>______________________________________________...</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>VB-VB-VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>\\n(v</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>aaron</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>aaron</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>VB-VB-WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>\\nur</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>abandoned</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>abandon</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>VB-VB-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>\"c</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>abbotsford</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>abbotsford</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>VB-VB-PDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>\"e</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>above</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>abov</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>VB-VB-PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>\"f</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>absolutely</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>abus</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>VB-VBG-DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>\"g</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>abuse</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>acceler</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>VB-VBG-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>\"h</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>accelerant</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>action</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>VB-VBG-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>\"l</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>accelerated</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>actor</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>VB-VBG-PRP$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>\"m</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>accomplished</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>ad</td>\n",
       "      <td>-15.741218</td>\n",
       "      <td>VB-VBN-VBN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Author   CoefChar  Char   CoefWord  \\\n",
       "0  AdamSavage -15.741218  \\n(t -15.741218   \n",
       "1  AdamSavage -15.741218  \\n(v -15.741218   \n",
       "2  AdamSavage -15.741218  \\nur -15.741218   \n",
       "3  AdamSavage -15.741218    \"c -15.741218   \n",
       "4  AdamSavage -15.741218    \"e -15.741218   \n",
       "5  AdamSavage -15.741218    \"f -15.741218   \n",
       "6  AdamSavage -15.741218    \"g -15.741218   \n",
       "7  AdamSavage -15.741218    \"h -15.741218   \n",
       "8  AdamSavage -15.741218    \"l -15.741218   \n",
       "9  AdamSavage -15.741218    \"m -15.741218   \n",
       "\n",
       "                                                Word   CoefText  \\\n",
       "0  ______________________________________________... -15.741218   \n",
       "1                                              aaron -15.741218   \n",
       "2                                          abandoned -15.741218   \n",
       "3                                         abbotsford -15.741218   \n",
       "4                                              above -15.741218   \n",
       "5                                         absolutely -15.741218   \n",
       "6                                              abuse -15.741218   \n",
       "7                                         accelerant -15.741218   \n",
       "8                                        accelerated -15.741218   \n",
       "9                                       accomplished -15.741218   \n",
       "\n",
       "                                                Text  CoefTextPOS      TextPOS  \n",
       "0  ______________________________________________...   -15.741218    VB-VB-VBG  \n",
       "1                                              aaron   -15.741218     VB-VB-WP  \n",
       "2                                            abandon   -15.741218     VB-VB-IN  \n",
       "3                                         abbotsford   -15.741218    VB-VB-PDT  \n",
       "4                                               abov   -15.741218    VB-VB-PRP  \n",
       "5                                               abus   -15.741218    VB-VBG-DT  \n",
       "6                                            acceler   -15.741218    VB-VBG-IN  \n",
       "7                                             action   -15.741218   VB-VBG-NNP  \n",
       "8                                              actor   -15.741218  VB-VBG-PRP$  \n",
       "9                                                 ad   -15.741218   VB-VBN-VBN  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='char')),['Author','Coef','Feature']].head(10)\n",
    "df1.rename(columns={'Coef':'CoefChar','Feature':'Char'}, inplace=True)\n",
    "df1.reset_index(inplace=True)\n",
    "df2=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='word')),['Coef','Feature']].head(10)\n",
    "df2.rename(columns={'Coef':'CoefWord','Feature':'Word'}, inplace=True)\n",
    "df2.reset_index(inplace=True)\n",
    "df3=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='text')),['Coef','Feature']].head(10)\n",
    "df3.rename(columns={'Coef':'CoefText','Feature':'Text'}, inplace=True)\n",
    "df3.reset_index(inplace=True)\n",
    "df4=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='text_pos')),['Coef','Feature']].head(10)\n",
    "df4.rename(columns={'Coef':'CoefTextPOS','Feature':'TextPOS'}, inplace=True)\n",
    "df4['TextPOS']=df4.apply(lambda x: text_pos_inv_convert(x['TextPOS']), axis=1)\n",
    "df4.reset_index(inplace=True)\n",
    "df_kk_top_features = pd.concat([df1,df2,df3,df4],axis=1)\n",
    "df_kk_top_features.drop('index', axis=1, inplace=True)\n",
    "df_kk_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>CoefChar</th>\n",
       "      <th>Char</th>\n",
       "      <th>CoefWord</th>\n",
       "      <th>Word</th>\n",
       "      <th>CoefText</th>\n",
       "      <th>Text</th>\n",
       "      <th>CoefTextPOS</th>\n",
       "      <th>TextPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>-0.426084</td>\n",
       "      <td>num</td>\n",
       "      <td>-0.534426</td>\n",
       "      <td>twitter</td>\n",
       "      <td>-0.531941</td>\n",
       "      <td>twitter</td>\n",
       "      <td>-0.650345</td>\n",
       "      <td>NNP-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>-0.476438</td>\n",
       "      <td>wit</td>\n",
       "      <td>-0.544426</td>\n",
       "      <td>com</td>\n",
       "      <td>-0.544426</td>\n",
       "      <td>com</td>\n",
       "      <td>-0.903256</td>\n",
       "      <td>NN-NN-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>-0.493007</td>\n",
       "      <td>com</td>\n",
       "      <td>-0.837145</td>\n",
       "      <td>ref</td>\n",
       "      <td>-0.837145</td>\n",
       "      <td>ref</td>\n",
       "      <td>-1.067046</td>\n",
       "      <td>NNP-NNP-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>-0.500192</td>\n",
       "      <td>tte</td>\n",
       "      <td>-0.962533</td>\n",
       "      <td>the</td>\n",
       "      <td>-0.962533</td>\n",
       "      <td>the</td>\n",
       "      <td>-1.088460</td>\n",
       "      <td>NN-NN-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>-0.500192</td>\n",
       "      <td>pic</td>\n",
       "      <td>-0.997488</td>\n",
       "      <td>yearinspacepic</td>\n",
       "      <td>-0.997488</td>\n",
       "      <td>yearinspacep</td>\n",
       "      <td>-1.252763</td>\n",
       "      <td>IN-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>-0.512284</td>\n",
       "      <td>itt</td>\n",
       "      <td>-1.283853</td>\n",
       "      <td>to</td>\n",
       "      <td>-1.283853</td>\n",
       "      <td>to</td>\n",
       "      <td>-1.315942</td>\n",
       "      <td>NNP-NNP-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>-0.512284</td>\n",
       "      <td>er.</td>\n",
       "      <td>-1.294435</td>\n",
       "      <td>num</td>\n",
       "      <td>-1.294435</td>\n",
       "      <td>num</td>\n",
       "      <td>-1.547271</td>\n",
       "      <td>NN-IN-NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>-0.526991</td>\n",
       "      <td>twi</td>\n",
       "      <td>-1.349094</td>\n",
       "      <td>from</td>\n",
       "      <td>-1.349094</td>\n",
       "      <td>from</td>\n",
       "      <td>-1.618222</td>\n",
       "      <td>IN-DT-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>-0.539413</td>\n",
       "      <td>ic.</td>\n",
       "      <td>-1.383383</td>\n",
       "      <td>of</td>\n",
       "      <td>-1.383383</td>\n",
       "      <td>of</td>\n",
       "      <td>-1.830397</td>\n",
       "      <td>NN-NNP-NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>-0.544426</td>\n",
       "      <td>r.c</td>\n",
       "      <td>-1.468282</td>\n",
       "      <td>in</td>\n",
       "      <td>-1.468282</td>\n",
       "      <td>in</td>\n",
       "      <td>-1.886486</td>\n",
       "      <td>JJ-NN-IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Author  CoefChar Char  CoefWord            Word  CoefText  \\\n",
       "0  ScottKelly -0.426084  num -0.534426         twitter -0.531941   \n",
       "1  ScottKelly -0.476438  wit -0.544426             com -0.544426   \n",
       "2  ScottKelly -0.493007  com -0.837145             ref -0.837145   \n",
       "3  ScottKelly -0.500192  tte -0.962533             the -0.962533   \n",
       "4  ScottKelly -0.500192  pic -0.997488  yearinspacepic -0.997488   \n",
       "5  ScottKelly -0.512284  itt -1.283853              to -1.283853   \n",
       "6  ScottKelly -0.512284  er. -1.294435             num -1.294435   \n",
       "7  ScottKelly -0.526991  twi -1.349094            from -1.349094   \n",
       "8  ScottKelly -0.539413  ic. -1.383383              of -1.383383   \n",
       "9  ScottKelly -0.544426  r.c -1.468282              in -1.468282   \n",
       "\n",
       "           Text  CoefTextPOS      TextPOS  \n",
       "0       twitter    -0.650345    NNP-NN-NN  \n",
       "1           com    -0.903256    NN-NN-NNP  \n",
       "2           ref    -1.067046   NNP-NNP-NN  \n",
       "3           the    -1.088460     NN-NN-NN  \n",
       "4  yearinspacep    -1.252763   IN-NNP-NNP  \n",
       "5            to    -1.315942  NNP-NNP-NNP  \n",
       "6           num    -1.547271    NN-IN-NNP  \n",
       "7          from    -1.618222     IN-DT-NN  \n",
       "8            of    -1.830397    NN-NNP-NN  \n",
       "9            in    -1.886486     JJ-NN-IN  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='char')),['Author','Coef','Feature']].head(10)\n",
    "df1.rename(columns={'Coef':'CoefChar','Feature':'Char'}, inplace=True)\n",
    "df1.reset_index(inplace=True)\n",
    "df2=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='word')),['Coef','Feature']].head(10)\n",
    "df2.rename(columns={'Coef':'CoefWord','Feature':'Word'}, inplace=True)\n",
    "df2.reset_index(inplace=True)\n",
    "df3=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='text')),['Coef','Feature']].head(10)\n",
    "df3.rename(columns={'Coef':'CoefText','Feature':'Text'}, inplace=True)\n",
    "df3.reset_index(inplace=True)\n",
    "df4=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='text_pos')),['Coef','Feature']].head(10)\n",
    "df4.rename(columns={'Coef':'CoefTextPOS','Feature':'TextPOS'}, inplace=True)\n",
    "df4['TextPOS']=df4.apply(lambda x: text_pos_inv_convert(x['TextPOS']), axis=1)\n",
    "df4.reset_index(inplace=True)\n",
    "df_kk_top_features = pd.concat([df1,df2,df3,df4],axis=1)\n",
    "df_kk_top_features.drop('index', axis=1, inplace=True)\n",
    "df_kk_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We've had to move the REF to avoid debris. One...</td>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>AdamSavage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Very cool video on the smart first-down line. ...</td>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>ScottKelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Incredible rescue mission in twin de Havilland...</td>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>ScottKelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>REF you are correct!</td>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>AdamSavage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>I need a haircut. Pretty sure.pic.twitter.com/...</td>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>ScottKelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Loved this street art in Denver, by David Choe...</td>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>ScottKelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Brian Lam has another beautiful article on und...</td>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>ScottKelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Another pic of the same location. Seriously ca...</td>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>ScottKelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Yes! I have taken pictures of it.URL</td>\n",
       "      <td>ScottKelly</td>\n",
       "      <td>AdamSavage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Here are a dozen views of the Sydney Opera Hou...</td>\n",
       "      <td>AdamSavage</td>\n",
       "      <td>ScottKelly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text      author   predicted\n",
       "0    We've had to move the REF to avoid debris. One...  ScottKelly  AdamSavage\n",
       "29   Very cool video on the smart first-down line. ...  AdamSavage  ScottKelly\n",
       "67   Incredible rescue mission in twin de Havilland...  AdamSavage  ScottKelly\n",
       "77                                REF you are correct!  ScottKelly  AdamSavage\n",
       "78   I need a haircut. Pretty sure.pic.twitter.com/...  AdamSavage  ScottKelly\n",
       "84   Loved this street art in Denver, by David Choe...  AdamSavage  ScottKelly\n",
       "88   Brian Lam has another beautiful article on und...  AdamSavage  ScottKelly\n",
       "91   Another pic of the same location. Seriously ca...  AdamSavage  ScottKelly\n",
       "110               Yes! I have taken pictures of it.URL  ScottKelly  AdamSavage\n",
       "121  Here are a dozen views of the Sydney Opera Hou...  AdamSavage  ScottKelly"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_predicted=pd.DataFrame(author_predicted,columns=['predicted'])\n",
    "df_wrong_result = pd.concat([twt_test.reset_index(),author_test.reset_index(),author_predicted], axis=1)\n",
    "df_wrong_result.drop('index', axis=1, inplace=True)\n",
    "df_wrong_result.drop('text_pos', axis=1, inplace=True)\n",
    "df_wrong_result=df_wrong_result[df_wrong_result['author']<>df_wrong_result['predicted']]\n",
    "df_wrong_result.head(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
